{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Tue Jul 18 02:51:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "|100%   26C    P8    32W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\n",
      "|100%   25C    P8    35W / 420W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:22:00.0 Off |                  N/A |\n",
      "|100%   26C    P8    21W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\n",
      "|100%   23C    P8    20W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  On   | 00000000:43:00.0 Off |                  N/A |\n",
      "|100%   24C    P8    15W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A |\n",
      "|100%   25C    P8    14W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "============Memory================\n",
      "MemTotal:       263845160 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 4949, done.\u001b[K\n",
      "remote: Counting objects: 100% (1746/1746), done.\u001b[K\n",
      "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
      "remote: Total 4949 (delta 1660), reused 1627 (delta 1597), pack-reused 3203\u001b[K\n",
      "Receiving objects: 100% (4949/4949), 4.08 MiB | 30.04 MiB/s, done.\n",
      "Resolving deltas: 100% (3384/3384), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa7f97-4a2e-47ab-8c66-ca34489ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4126f1-7c42-4d66-8a7d-8e5c29c322da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eaea4f0-2d91-41ef-9f32-4103f2858796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0  12313      0 --:--:-- --:--:-- --:--:-- 12294\n",
      "Downloading tokenizer\n",
      "--2023-07-18 02:52:03--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-07-18 02:52:04 (28.9 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-07-18 02:52:04--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 02:52:04 (70.1 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-07-18 02:52:04--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated.  37%[======>             ]   4.73G  50.7MB/s    in 2m 15s  \n",
      "\n",
      "2023-07-18 02:54:20 (35.9 MB/s) - Connection closed at byte 5075107840. Retrying.\n",
      "\n",
      "--2023-07-18 02:54:21--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 13476939516 (13G), 8401831676 (7.8G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[+++++++============>]  12.55G  50.8MB/s    in 3m 11s  \n",
      "\n",
      "2023-07-18 02:57:32 (41.9 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-07-18 02:57:32--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 02:57:32 (61.4 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-18 02:57:32--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 02:57:33 (149 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-07-18 02:57:50--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  57.3MB/s    in 3m 25s  \n",
      "\n",
      "2023-07-18 03:01:16 (60.6 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-18 03:01:16--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  26.5MB/s    in 9m 18s  \n",
      "\n",
      "2023-07-18 03:10:34 (22.3 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-18 03:10:34--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 03:10:37 (47.7 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-18 03:10:37--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-07-18 03:10:37 (9.80 KB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-07-18 03:11:11--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  74.2MB/s    in 4m 43s  \n",
      "\n",
      "2023-07-18 03:15:54 (54.9 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-18 03:15:54--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  81.0MB/s    in 3m 51s  \n",
      "\n",
      "2023-07-18 03:19:45 (67.2 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-18 03:19:45--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  54.8MB/s    in 4m 48s  \n",
      "\n",
      "2023-07-18 03:24:33 (53.9 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-18 03:24:33--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  63.2MB/s    in 3m 53s  \n",
      "\n",
      "2023-07-18 03:28:26 (66.5 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-18 03:28:26--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 03:28:27 (61.4 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-18 03:28:27--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 03:28:27 (471 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-07-18 03:29:52--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  84.2MB/s    in 3m 58s  \n",
      "\n",
      "2023-07-18 03:33:50 (65.5 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:33:50--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  58.3MB/s    in 4m 20s  \n",
      "\n",
      "2023-07-18 03:38:10 (59.9 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:38:10--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  73.5MB/s    in 4m 38s  \n",
      "\n",
      "2023-07-18 03:42:48 (56.0 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:42:48--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  63.0MB/s    in 4m 6s   \n",
      "\n",
      "2023-07-18 03:46:55 (63.2 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:46:55--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  85.1MB/s    in 3m 40s  \n",
      "\n",
      "2023-07-18 03:50:35 (70.9 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:50:35--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  66.7MB/s    in 4m 22s  \n",
      "\n",
      "2023-07-18 03:54:57 (59.5 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 03:54:57--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  45.4MB/s    in 5m 53s  \n",
      "\n",
      "2023-07-18 04:00:51 (44.1 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 04:00:51--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  72.1MB/s    in 4m 57s  \n",
      "\n",
      "2023-07-18 04:05:48 (52.4 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-18 04:05:48--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 04:05:48 (47.0 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-18 04:05:48--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-18 04:05:49 (860 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/7B/ggml-model-f16.bin\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:5120 n_mult:256 n_head:40 n_layer:40\n",
      "Writing vocab...\n",
      "[  1/363] Writing tensor tok_embeddings.weight                  | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  2/363] Writing tensor norm.weight                            | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  4/363] Writing tensor layers.0.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  5/363] Writing tensor layers.0.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  6/363] Writing tensor layers.0.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  7/363] Writing tensor layers.0.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  8/363] Writing tensor layers.0.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  9/363] Writing tensor layers.0.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 10/363] Writing tensor layers.0.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 11/363] Writing tensor layers.0.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 12/363] Writing tensor layers.0.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 13/363] Writing tensor layers.1.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 14/363] Writing tensor layers.1.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 15/363] Writing tensor layers.1.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 16/363] Writing tensor layers.1.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 17/363] Writing tensor layers.1.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 18/363] Writing tensor layers.1.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 19/363] Writing tensor layers.1.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 20/363] Writing tensor layers.1.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 21/363] Writing tensor layers.1.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 22/363] Writing tensor layers.2.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 23/363] Writing tensor layers.2.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 24/363] Writing tensor layers.2.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 25/363] Writing tensor layers.2.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 26/363] Writing tensor layers.2.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 27/363] Writing tensor layers.2.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 28/363] Writing tensor layers.2.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 29/363] Writing tensor layers.2.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 30/363] Writing tensor layers.2.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 31/363] Writing tensor layers.3.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 32/363] Writing tensor layers.3.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 33/363] Writing tensor layers.3.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 34/363] Writing tensor layers.3.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 35/363] Writing tensor layers.3.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 36/363] Writing tensor layers.3.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 37/363] Writing tensor layers.3.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 38/363] Writing tensor layers.3.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 39/363] Writing tensor layers.3.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 40/363] Writing tensor layers.4.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 41/363] Writing tensor layers.4.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 42/363] Writing tensor layers.4.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 43/363] Writing tensor layers.4.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 44/363] Writing tensor layers.4.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 45/363] Writing tensor layers.4.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 46/363] Writing tensor layers.4.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 47/363] Writing tensor layers.4.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 48/363] Writing tensor layers.4.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 49/363] Writing tensor layers.5.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 50/363] Writing tensor layers.5.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 51/363] Writing tensor layers.5.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 52/363] Writing tensor layers.5.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 53/363] Writing tensor layers.5.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 54/363] Writing tensor layers.5.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 55/363] Writing tensor layers.5.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 56/363] Writing tensor layers.5.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 57/363] Writing tensor layers.5.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 58/363] Writing tensor layers.6.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 59/363] Writing tensor layers.6.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 60/363] Writing tensor layers.6.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 61/363] Writing tensor layers.6.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 62/363] Writing tensor layers.6.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 63/363] Writing tensor layers.6.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 64/363] Writing tensor layers.6.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 65/363] Writing tensor layers.6.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 66/363] Writing tensor layers.6.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 67/363] Writing tensor layers.7.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 68/363] Writing tensor layers.7.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 69/363] Writing tensor layers.7.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 70/363] Writing tensor layers.7.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 71/363] Writing tensor layers.7.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 72/363] Writing tensor layers.7.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 73/363] Writing tensor layers.7.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 74/363] Writing tensor layers.7.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 75/363] Writing tensor layers.7.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 76/363] Writing tensor layers.8.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 77/363] Writing tensor layers.8.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 78/363] Writing tensor layers.8.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 79/363] Writing tensor layers.8.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 80/363] Writing tensor layers.8.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 81/363] Writing tensor layers.8.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 82/363] Writing tensor layers.8.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 83/363] Writing tensor layers.8.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 84/363] Writing tensor layers.8.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 85/363] Writing tensor layers.9.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 86/363] Writing tensor layers.9.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 87/363] Writing tensor layers.9.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 88/363] Writing tensor layers.9.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 89/363] Writing tensor layers.9.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 90/363] Writing tensor layers.9.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 91/363] Writing tensor layers.9.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 92/363] Writing tensor layers.9.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 93/363] Writing tensor layers.9.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 94/363] Writing tensor layers.10.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 95/363] Writing tensor layers.10.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 96/363] Writing tensor layers.10.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 97/363] Writing tensor layers.10.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 98/363] Writing tensor layers.10.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 99/363] Writing tensor layers.10.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[100/363] Writing tensor layers.10.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[101/363] Writing tensor layers.10.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[102/363] Writing tensor layers.10.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[103/363] Writing tensor layers.11.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[104/363] Writing tensor layers.11.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[105/363] Writing tensor layers.11.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[106/363] Writing tensor layers.11.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[107/363] Writing tensor layers.11.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[108/363] Writing tensor layers.11.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[109/363] Writing tensor layers.11.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[110/363] Writing tensor layers.11.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[111/363] Writing tensor layers.11.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[112/363] Writing tensor layers.12.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[113/363] Writing tensor layers.12.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[114/363] Writing tensor layers.12.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[115/363] Writing tensor layers.12.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[116/363] Writing tensor layers.12.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[117/363] Writing tensor layers.12.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[118/363] Writing tensor layers.12.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[119/363] Writing tensor layers.12.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[120/363] Writing tensor layers.12.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[121/363] Writing tensor layers.13.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[122/363] Writing tensor layers.13.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[123/363] Writing tensor layers.13.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[124/363] Writing tensor layers.13.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[125/363] Writing tensor layers.13.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[126/363] Writing tensor layers.13.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[127/363] Writing tensor layers.13.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[128/363] Writing tensor layers.13.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[129/363] Writing tensor layers.13.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[130/363] Writing tensor layers.14.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[131/363] Writing tensor layers.14.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[132/363] Writing tensor layers.14.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[133/363] Writing tensor layers.14.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[134/363] Writing tensor layers.14.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[135/363] Writing tensor layers.14.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[136/363] Writing tensor layers.14.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[137/363] Writing tensor layers.14.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[138/363] Writing tensor layers.14.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[139/363] Writing tensor layers.15.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[140/363] Writing tensor layers.15.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[141/363] Writing tensor layers.15.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[142/363] Writing tensor layers.15.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[143/363] Writing tensor layers.15.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[144/363] Writing tensor layers.15.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[145/363] Writing tensor layers.15.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[146/363] Writing tensor layers.15.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[147/363] Writing tensor layers.15.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[148/363] Writing tensor layers.16.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[149/363] Writing tensor layers.16.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[150/363] Writing tensor layers.16.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[151/363] Writing tensor layers.16.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[152/363] Writing tensor layers.16.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[153/363] Writing tensor layers.16.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[154/363] Writing tensor layers.16.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[155/363] Writing tensor layers.16.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[156/363] Writing tensor layers.16.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[157/363] Writing tensor layers.17.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[158/363] Writing tensor layers.17.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[159/363] Writing tensor layers.17.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[160/363] Writing tensor layers.17.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[161/363] Writing tensor layers.17.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[162/363] Writing tensor layers.17.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[163/363] Writing tensor layers.17.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[164/363] Writing tensor layers.17.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[165/363] Writing tensor layers.17.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[166/363] Writing tensor layers.18.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[167/363] Writing tensor layers.18.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[168/363] Writing tensor layers.18.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[169/363] Writing tensor layers.18.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[170/363] Writing tensor layers.18.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[171/363] Writing tensor layers.18.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[172/363] Writing tensor layers.18.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[173/363] Writing tensor layers.18.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[174/363] Writing tensor layers.18.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[175/363] Writing tensor layers.19.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[176/363] Writing tensor layers.19.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[177/363] Writing tensor layers.19.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[178/363] Writing tensor layers.19.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[179/363] Writing tensor layers.19.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[180/363] Writing tensor layers.19.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[181/363] Writing tensor layers.19.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[182/363] Writing tensor layers.19.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[183/363] Writing tensor layers.19.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[184/363] Writing tensor layers.20.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[185/363] Writing tensor layers.20.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[186/363] Writing tensor layers.20.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[187/363] Writing tensor layers.20.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[188/363] Writing tensor layers.20.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[189/363] Writing tensor layers.20.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[190/363] Writing tensor layers.20.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[191/363] Writing tensor layers.20.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[192/363] Writing tensor layers.20.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[193/363] Writing tensor layers.21.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[194/363] Writing tensor layers.21.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[195/363] Writing tensor layers.21.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[196/363] Writing tensor layers.21.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[197/363] Writing tensor layers.21.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[198/363] Writing tensor layers.21.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[199/363] Writing tensor layers.21.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[200/363] Writing tensor layers.21.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[201/363] Writing tensor layers.21.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[202/363] Writing tensor layers.22.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[203/363] Writing tensor layers.22.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[204/363] Writing tensor layers.22.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[205/363] Writing tensor layers.22.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[206/363] Writing tensor layers.22.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[207/363] Writing tensor layers.22.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[208/363] Writing tensor layers.22.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[209/363] Writing tensor layers.22.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[210/363] Writing tensor layers.22.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[211/363] Writing tensor layers.23.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[212/363] Writing tensor layers.23.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[213/363] Writing tensor layers.23.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[214/363] Writing tensor layers.23.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[215/363] Writing tensor layers.23.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[216/363] Writing tensor layers.23.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[217/363] Writing tensor layers.23.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[218/363] Writing tensor layers.23.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[219/363] Writing tensor layers.23.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[220/363] Writing tensor layers.24.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[221/363] Writing tensor layers.24.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[222/363] Writing tensor layers.24.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[223/363] Writing tensor layers.24.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[224/363] Writing tensor layers.24.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[225/363] Writing tensor layers.24.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[226/363] Writing tensor layers.24.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[227/363] Writing tensor layers.24.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[228/363] Writing tensor layers.24.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[229/363] Writing tensor layers.25.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[230/363] Writing tensor layers.25.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[231/363] Writing tensor layers.25.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[232/363] Writing tensor layers.25.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[233/363] Writing tensor layers.25.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[234/363] Writing tensor layers.25.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[235/363] Writing tensor layers.25.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[236/363] Writing tensor layers.25.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[237/363] Writing tensor layers.25.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[238/363] Writing tensor layers.26.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[239/363] Writing tensor layers.26.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[240/363] Writing tensor layers.26.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[241/363] Writing tensor layers.26.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[242/363] Writing tensor layers.26.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[243/363] Writing tensor layers.26.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[244/363] Writing tensor layers.26.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[245/363] Writing tensor layers.26.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[246/363] Writing tensor layers.26.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[247/363] Writing tensor layers.27.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[248/363] Writing tensor layers.27.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[249/363] Writing tensor layers.27.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[250/363] Writing tensor layers.27.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[251/363] Writing tensor layers.27.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[252/363] Writing tensor layers.27.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[253/363] Writing tensor layers.27.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[254/363] Writing tensor layers.27.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[255/363] Writing tensor layers.27.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[256/363] Writing tensor layers.28.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[257/363] Writing tensor layers.28.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[258/363] Writing tensor layers.28.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[259/363] Writing tensor layers.28.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[260/363] Writing tensor layers.28.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[261/363] Writing tensor layers.28.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[262/363] Writing tensor layers.28.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[263/363] Writing tensor layers.28.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[264/363] Writing tensor layers.28.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[265/363] Writing tensor layers.29.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[266/363] Writing tensor layers.29.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[267/363] Writing tensor layers.29.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[268/363] Writing tensor layers.29.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[269/363] Writing tensor layers.29.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[270/363] Writing tensor layers.29.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[271/363] Writing tensor layers.29.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[272/363] Writing tensor layers.29.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[273/363] Writing tensor layers.29.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[274/363] Writing tensor layers.30.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[275/363] Writing tensor layers.30.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[276/363] Writing tensor layers.30.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[277/363] Writing tensor layers.30.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[278/363] Writing tensor layers.30.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[279/363] Writing tensor layers.30.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[280/363] Writing tensor layers.30.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[281/363] Writing tensor layers.30.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[282/363] Writing tensor layers.30.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[283/363] Writing tensor layers.31.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[284/363] Writing tensor layers.31.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[285/363] Writing tensor layers.31.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[286/363] Writing tensor layers.31.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[287/363] Writing tensor layers.31.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[288/363] Writing tensor layers.31.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[289/363] Writing tensor layers.31.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[290/363] Writing tensor layers.31.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[291/363] Writing tensor layers.31.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[292/363] Writing tensor layers.32.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[293/363] Writing tensor layers.32.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[294/363] Writing tensor layers.32.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[295/363] Writing tensor layers.32.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[296/363] Writing tensor layers.32.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[297/363] Writing tensor layers.32.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[298/363] Writing tensor layers.32.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[299/363] Writing tensor layers.32.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[300/363] Writing tensor layers.32.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[301/363] Writing tensor layers.33.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[302/363] Writing tensor layers.33.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[303/363] Writing tensor layers.33.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[304/363] Writing tensor layers.33.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[305/363] Writing tensor layers.33.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[306/363] Writing tensor layers.33.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[307/363] Writing tensor layers.33.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[308/363] Writing tensor layers.33.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[309/363] Writing tensor layers.33.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[310/363] Writing tensor layers.34.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[311/363] Writing tensor layers.34.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[312/363] Writing tensor layers.34.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[313/363] Writing tensor layers.34.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[314/363] Writing tensor layers.34.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[315/363] Writing tensor layers.34.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[316/363] Writing tensor layers.34.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[317/363] Writing tensor layers.34.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[318/363] Writing tensor layers.34.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[319/363] Writing tensor layers.35.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[320/363] Writing tensor layers.35.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[321/363] Writing tensor layers.35.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[322/363] Writing tensor layers.35.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[323/363] Writing tensor layers.35.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[324/363] Writing tensor layers.35.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[325/363] Writing tensor layers.35.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[326/363] Writing tensor layers.35.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[327/363] Writing tensor layers.35.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[328/363] Writing tensor layers.36.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[329/363] Writing tensor layers.36.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[330/363] Writing tensor layers.36.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[331/363] Writing tensor layers.36.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[332/363] Writing tensor layers.36.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[333/363] Writing tensor layers.36.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[334/363] Writing tensor layers.36.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[335/363] Writing tensor layers.36.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[336/363] Writing tensor layers.36.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[337/363] Writing tensor layers.37.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[338/363] Writing tensor layers.37.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[339/363] Writing tensor layers.37.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[340/363] Writing tensor layers.37.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[341/363] Writing tensor layers.37.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[342/363] Writing tensor layers.37.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[343/363] Writing tensor layers.37.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[344/363] Writing tensor layers.37.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[345/363] Writing tensor layers.37.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[346/363] Writing tensor layers.38.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[347/363] Writing tensor layers.38.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[348/363] Writing tensor layers.38.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[349/363] Writing tensor layers.38.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[350/363] Writing tensor layers.38.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[351/363] Writing tensor layers.38.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[352/363] Writing tensor layers.38.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[353/363] Writing tensor layers.38.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[354/363] Writing tensor layers.38.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[355/363] Writing tensor layers.39.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[356/363] Writing tensor layers.39.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[357/363] Writing tensor layers.39.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[358/363] Writing tensor layers.39.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[359/363] Writing tensor layers.39.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/13B/ggml-model-f16.bin\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:6656 n_mult:256 n_head:52 n_layer:60\n",
      "Writing vocab...\n",
      "[  1/543] Writing tensor tok_embeddings.weight                  | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  2/543] Writing tensor norm.weight                            | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  4/543] Writing tensor layers.0.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  5/543] Writing tensor layers.0.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  6/543] Writing tensor layers.0.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  7/543] Writing tensor layers.0.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  8/543] Writing tensor layers.0.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  9/543] Writing tensor layers.0.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 10/543] Writing tensor layers.0.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 11/543] Writing tensor layers.0.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 12/543] Writing tensor layers.0.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 13/543] Writing tensor layers.1.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 14/543] Writing tensor layers.1.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 15/543] Writing tensor layers.1.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 16/543] Writing tensor layers.1.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 17/543] Writing tensor layers.1.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 18/543] Writing tensor layers.1.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 19/543] Writing tensor layers.1.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 20/543] Writing tensor layers.1.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 21/543] Writing tensor layers.1.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 22/543] Writing tensor layers.2.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 23/543] Writing tensor layers.2.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 24/543] Writing tensor layers.2.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 25/543] Writing tensor layers.2.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 26/543] Writing tensor layers.2.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 27/543] Writing tensor layers.2.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 28/543] Writing tensor layers.2.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 29/543] Writing tensor layers.2.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 30/543] Writing tensor layers.2.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 31/543] Writing tensor layers.3.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 32/543] Writing tensor layers.3.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 33/543] Writing tensor layers.3.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 34/543] Writing tensor layers.3.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 35/543] Writing tensor layers.3.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 36/543] Writing tensor layers.3.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 37/543] Writing tensor layers.3.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 38/543] Writing tensor layers.3.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 39/543] Writing tensor layers.3.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 40/543] Writing tensor layers.4.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 41/543] Writing tensor layers.4.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 42/543] Writing tensor layers.4.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 43/543] Writing tensor layers.4.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 44/543] Writing tensor layers.4.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 45/543] Writing tensor layers.4.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 46/543] Writing tensor layers.4.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 47/543] Writing tensor layers.4.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 48/543] Writing tensor layers.4.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 49/543] Writing tensor layers.5.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 50/543] Writing tensor layers.5.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 51/543] Writing tensor layers.5.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 52/543] Writing tensor layers.5.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 53/543] Writing tensor layers.5.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 54/543] Writing tensor layers.5.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 55/543] Writing tensor layers.5.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 56/543] Writing tensor layers.5.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 57/543] Writing tensor layers.5.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 58/543] Writing tensor layers.6.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 59/543] Writing tensor layers.6.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 60/543] Writing tensor layers.6.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 61/543] Writing tensor layers.6.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 62/543] Writing tensor layers.6.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 63/543] Writing tensor layers.6.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 64/543] Writing tensor layers.6.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 65/543] Writing tensor layers.6.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 66/543] Writing tensor layers.6.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 67/543] Writing tensor layers.7.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 68/543] Writing tensor layers.7.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 69/543] Writing tensor layers.7.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 70/543] Writing tensor layers.7.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 71/543] Writing tensor layers.7.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 72/543] Writing tensor layers.7.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 73/543] Writing tensor layers.7.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 74/543] Writing tensor layers.7.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 75/543] Writing tensor layers.7.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 76/543] Writing tensor layers.8.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 77/543] Writing tensor layers.8.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 78/543] Writing tensor layers.8.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 79/543] Writing tensor layers.8.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 80/543] Writing tensor layers.8.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 81/543] Writing tensor layers.8.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 82/543] Writing tensor layers.8.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 83/543] Writing tensor layers.8.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 84/543] Writing tensor layers.8.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 85/543] Writing tensor layers.9.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 86/543] Writing tensor layers.9.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 87/543] Writing tensor layers.9.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 88/543] Writing tensor layers.9.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 89/543] Writing tensor layers.9.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 90/543] Writing tensor layers.9.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 91/543] Writing tensor layers.9.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 92/543] Writing tensor layers.9.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 93/543] Writing tensor layers.9.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 94/543] Writing tensor layers.10.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 95/543] Writing tensor layers.10.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 96/543] Writing tensor layers.10.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 97/543] Writing tensor layers.10.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 98/543] Writing tensor layers.10.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 99/543] Writing tensor layers.10.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[100/543] Writing tensor layers.10.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[101/543] Writing tensor layers.10.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[102/543] Writing tensor layers.10.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[103/543] Writing tensor layers.11.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[104/543] Writing tensor layers.11.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[105/543] Writing tensor layers.11.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[106/543] Writing tensor layers.11.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[107/543] Writing tensor layers.11.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[108/543] Writing tensor layers.11.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[109/543] Writing tensor layers.11.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[110/543] Writing tensor layers.11.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[111/543] Writing tensor layers.11.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[112/543] Writing tensor layers.12.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[113/543] Writing tensor layers.12.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[114/543] Writing tensor layers.12.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[115/543] Writing tensor layers.12.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[116/543] Writing tensor layers.12.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[117/543] Writing tensor layers.12.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[118/543] Writing tensor layers.12.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[119/543] Writing tensor layers.12.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[120/543] Writing tensor layers.12.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[121/543] Writing tensor layers.13.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[122/543] Writing tensor layers.13.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[123/543] Writing tensor layers.13.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[124/543] Writing tensor layers.13.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[125/543] Writing tensor layers.13.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[126/543] Writing tensor layers.13.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[127/543] Writing tensor layers.13.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[128/543] Writing tensor layers.13.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[129/543] Writing tensor layers.13.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[130/543] Writing tensor layers.14.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[131/543] Writing tensor layers.14.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[132/543] Writing tensor layers.14.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[133/543] Writing tensor layers.14.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[134/543] Writing tensor layers.14.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[135/543] Writing tensor layers.14.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[136/543] Writing tensor layers.14.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[137/543] Writing tensor layers.14.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[138/543] Writing tensor layers.14.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[139/543] Writing tensor layers.15.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[140/543] Writing tensor layers.15.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[141/543] Writing tensor layers.15.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[142/543] Writing tensor layers.15.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[143/543] Writing tensor layers.15.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[144/543] Writing tensor layers.15.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[145/543] Writing tensor layers.15.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[146/543] Writing tensor layers.15.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[147/543] Writing tensor layers.15.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[148/543] Writing tensor layers.16.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[149/543] Writing tensor layers.16.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[150/543] Writing tensor layers.16.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[151/543] Writing tensor layers.16.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[152/543] Writing tensor layers.16.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[153/543] Writing tensor layers.16.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[154/543] Writing tensor layers.16.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[155/543] Writing tensor layers.16.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[156/543] Writing tensor layers.16.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[157/543] Writing tensor layers.17.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[158/543] Writing tensor layers.17.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[159/543] Writing tensor layers.17.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[160/543] Writing tensor layers.17.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[161/543] Writing tensor layers.17.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[162/543] Writing tensor layers.17.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[163/543] Writing tensor layers.17.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[164/543] Writing tensor layers.17.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[165/543] Writing tensor layers.17.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[166/543] Writing tensor layers.18.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[167/543] Writing tensor layers.18.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[168/543] Writing tensor layers.18.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[169/543] Writing tensor layers.18.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[170/543] Writing tensor layers.18.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[171/543] Writing tensor layers.18.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[172/543] Writing tensor layers.18.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[173/543] Writing tensor layers.18.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[174/543] Writing tensor layers.18.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[175/543] Writing tensor layers.19.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[176/543] Writing tensor layers.19.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[177/543] Writing tensor layers.19.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[178/543] Writing tensor layers.19.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[179/543] Writing tensor layers.19.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[180/543] Writing tensor layers.19.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[181/543] Writing tensor layers.19.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[182/543] Writing tensor layers.19.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[183/543] Writing tensor layers.19.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[184/543] Writing tensor layers.20.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[185/543] Writing tensor layers.20.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[186/543] Writing tensor layers.20.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[187/543] Writing tensor layers.20.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[188/543] Writing tensor layers.20.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[189/543] Writing tensor layers.20.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[190/543] Writing tensor layers.20.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[191/543] Writing tensor layers.20.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[192/543] Writing tensor layers.20.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[193/543] Writing tensor layers.21.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[194/543] Writing tensor layers.21.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[195/543] Writing tensor layers.21.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[196/543] Writing tensor layers.21.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[197/543] Writing tensor layers.21.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[198/543] Writing tensor layers.21.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[199/543] Writing tensor layers.21.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[200/543] Writing tensor layers.21.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[201/543] Writing tensor layers.21.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[202/543] Writing tensor layers.22.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[203/543] Writing tensor layers.22.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[204/543] Writing tensor layers.22.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[205/543] Writing tensor layers.22.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[206/543] Writing tensor layers.22.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[207/543] Writing tensor layers.22.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[208/543] Writing tensor layers.22.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[209/543] Writing tensor layers.22.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[210/543] Writing tensor layers.22.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[211/543] Writing tensor layers.23.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[212/543] Writing tensor layers.23.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[213/543] Writing tensor layers.23.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[214/543] Writing tensor layers.23.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[215/543] Writing tensor layers.23.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[216/543] Writing tensor layers.23.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[217/543] Writing tensor layers.23.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[218/543] Writing tensor layers.23.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[219/543] Writing tensor layers.23.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[220/543] Writing tensor layers.24.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[221/543] Writing tensor layers.24.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[222/543] Writing tensor layers.24.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[223/543] Writing tensor layers.24.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[224/543] Writing tensor layers.24.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[225/543] Writing tensor layers.24.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[226/543] Writing tensor layers.24.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[227/543] Writing tensor layers.24.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[228/543] Writing tensor layers.24.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[229/543] Writing tensor layers.25.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[230/543] Writing tensor layers.25.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[231/543] Writing tensor layers.25.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[232/543] Writing tensor layers.25.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[233/543] Writing tensor layers.25.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[234/543] Writing tensor layers.25.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[235/543] Writing tensor layers.25.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[236/543] Writing tensor layers.25.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[237/543] Writing tensor layers.25.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[238/543] Writing tensor layers.26.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[239/543] Writing tensor layers.26.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[240/543] Writing tensor layers.26.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[241/543] Writing tensor layers.26.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[242/543] Writing tensor layers.26.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[243/543] Writing tensor layers.26.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[244/543] Writing tensor layers.26.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[245/543] Writing tensor layers.26.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[246/543] Writing tensor layers.26.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[247/543] Writing tensor layers.27.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[248/543] Writing tensor layers.27.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[249/543] Writing tensor layers.27.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[250/543] Writing tensor layers.27.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[251/543] Writing tensor layers.27.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[252/543] Writing tensor layers.27.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[253/543] Writing tensor layers.27.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[254/543] Writing tensor layers.27.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[255/543] Writing tensor layers.27.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[256/543] Writing tensor layers.28.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[257/543] Writing tensor layers.28.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[258/543] Writing tensor layers.28.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[259/543] Writing tensor layers.28.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[260/543] Writing tensor layers.28.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[261/543] Writing tensor layers.28.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[262/543] Writing tensor layers.28.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[263/543] Writing tensor layers.28.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[264/543] Writing tensor layers.28.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[265/543] Writing tensor layers.29.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[266/543] Writing tensor layers.29.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[267/543] Writing tensor layers.29.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[268/543] Writing tensor layers.29.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[269/543] Writing tensor layers.29.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[270/543] Writing tensor layers.29.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[271/543] Writing tensor layers.29.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[272/543] Writing tensor layers.29.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[273/543] Writing tensor layers.29.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[274/543] Writing tensor layers.30.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[275/543] Writing tensor layers.30.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[276/543] Writing tensor layers.30.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[277/543] Writing tensor layers.30.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[278/543] Writing tensor layers.30.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[279/543] Writing tensor layers.30.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[280/543] Writing tensor layers.30.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[281/543] Writing tensor layers.30.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[282/543] Writing tensor layers.30.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[283/543] Writing tensor layers.31.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[284/543] Writing tensor layers.31.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[285/543] Writing tensor layers.31.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[286/543] Writing tensor layers.31.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[287/543] Writing tensor layers.31.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[288/543] Writing tensor layers.31.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[289/543] Writing tensor layers.31.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[290/543] Writing tensor layers.31.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[291/543] Writing tensor layers.31.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[292/543] Writing tensor layers.32.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[293/543] Writing tensor layers.32.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[294/543] Writing tensor layers.32.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[295/543] Writing tensor layers.32.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[296/543] Writing tensor layers.32.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[297/543] Writing tensor layers.32.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[298/543] Writing tensor layers.32.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[299/543] Writing tensor layers.32.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[300/543] Writing tensor layers.32.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[301/543] Writing tensor layers.33.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[302/543] Writing tensor layers.33.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[303/543] Writing tensor layers.33.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[304/543] Writing tensor layers.33.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[305/543] Writing tensor layers.33.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[306/543] Writing tensor layers.33.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[307/543] Writing tensor layers.33.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[308/543] Writing tensor layers.33.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[309/543] Writing tensor layers.33.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[310/543] Writing tensor layers.34.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[311/543] Writing tensor layers.34.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[312/543] Writing tensor layers.34.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[313/543] Writing tensor layers.34.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[314/543] Writing tensor layers.34.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[315/543] Writing tensor layers.34.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[316/543] Writing tensor layers.34.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[317/543] Writing tensor layers.34.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[318/543] Writing tensor layers.34.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[319/543] Writing tensor layers.35.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[320/543] Writing tensor layers.35.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[321/543] Writing tensor layers.35.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[322/543] Writing tensor layers.35.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[323/543] Writing tensor layers.35.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[324/543] Writing tensor layers.35.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[325/543] Writing tensor layers.35.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[326/543] Writing tensor layers.35.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[327/543] Writing tensor layers.35.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[328/543] Writing tensor layers.36.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[329/543] Writing tensor layers.36.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[330/543] Writing tensor layers.36.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[331/543] Writing tensor layers.36.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[332/543] Writing tensor layers.36.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[333/543] Writing tensor layers.36.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[334/543] Writing tensor layers.36.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[335/543] Writing tensor layers.36.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[336/543] Writing tensor layers.36.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[337/543] Writing tensor layers.37.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[338/543] Writing tensor layers.37.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[339/543] Writing tensor layers.37.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[340/543] Writing tensor layers.37.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[341/543] Writing tensor layers.37.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[342/543] Writing tensor layers.37.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[343/543] Writing tensor layers.37.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[344/543] Writing tensor layers.37.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[345/543] Writing tensor layers.37.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[346/543] Writing tensor layers.38.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[347/543] Writing tensor layers.38.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[348/543] Writing tensor layers.38.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[349/543] Writing tensor layers.38.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[350/543] Writing tensor layers.38.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[351/543] Writing tensor layers.38.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[352/543] Writing tensor layers.38.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[353/543] Writing tensor layers.38.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[354/543] Writing tensor layers.38.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[355/543] Writing tensor layers.39.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[356/543] Writing tensor layers.39.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[357/543] Writing tensor layers.39.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[358/543] Writing tensor layers.39.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[359/543] Writing tensor layers.39.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[360/543] Writing tensor layers.39.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[361/543] Writing tensor layers.39.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[362/543] Writing tensor layers.39.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[363/543] Writing tensor layers.39.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[364/543] Writing tensor layers.40.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[365/543] Writing tensor layers.40.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[366/543] Writing tensor layers.40.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[367/543] Writing tensor layers.40.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[368/543] Writing tensor layers.40.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[369/543] Writing tensor layers.40.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[370/543] Writing tensor layers.40.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[371/543] Writing tensor layers.40.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[372/543] Writing tensor layers.40.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[373/543] Writing tensor layers.41.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[374/543] Writing tensor layers.41.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[375/543] Writing tensor layers.41.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[376/543] Writing tensor layers.41.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[377/543] Writing tensor layers.41.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[378/543] Writing tensor layers.41.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[379/543] Writing tensor layers.41.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[380/543] Writing tensor layers.41.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[381/543] Writing tensor layers.41.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[382/543] Writing tensor layers.42.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[383/543] Writing tensor layers.42.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[384/543] Writing tensor layers.42.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[385/543] Writing tensor layers.42.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[386/543] Writing tensor layers.42.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[387/543] Writing tensor layers.42.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[388/543] Writing tensor layers.42.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[389/543] Writing tensor layers.42.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[390/543] Writing tensor layers.42.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[391/543] Writing tensor layers.43.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[392/543] Writing tensor layers.43.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[393/543] Writing tensor layers.43.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[394/543] Writing tensor layers.43.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[395/543] Writing tensor layers.43.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[396/543] Writing tensor layers.43.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[397/543] Writing tensor layers.43.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[398/543] Writing tensor layers.43.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[399/543] Writing tensor layers.43.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[400/543] Writing tensor layers.44.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[401/543] Writing tensor layers.44.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[402/543] Writing tensor layers.44.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[403/543] Writing tensor layers.44.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[404/543] Writing tensor layers.44.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[405/543] Writing tensor layers.44.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[406/543] Writing tensor layers.44.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[407/543] Writing tensor layers.44.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[408/543] Writing tensor layers.44.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[409/543] Writing tensor layers.45.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[410/543] Writing tensor layers.45.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[411/543] Writing tensor layers.45.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[412/543] Writing tensor layers.45.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[413/543] Writing tensor layers.45.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[414/543] Writing tensor layers.45.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[415/543] Writing tensor layers.45.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[416/543] Writing tensor layers.45.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[417/543] Writing tensor layers.45.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[418/543] Writing tensor layers.46.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[419/543] Writing tensor layers.46.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[420/543] Writing tensor layers.46.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[421/543] Writing tensor layers.46.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[422/543] Writing tensor layers.46.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[423/543] Writing tensor layers.46.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[424/543] Writing tensor layers.46.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[425/543] Writing tensor layers.46.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[426/543] Writing tensor layers.46.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[427/543] Writing tensor layers.47.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[428/543] Writing tensor layers.47.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[429/543] Writing tensor layers.47.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[430/543] Writing tensor layers.47.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[431/543] Writing tensor layers.47.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[432/543] Writing tensor layers.47.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[433/543] Writing tensor layers.47.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[434/543] Writing tensor layers.47.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[435/543] Writing tensor layers.47.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[436/543] Writing tensor layers.48.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[437/543] Writing tensor layers.48.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[438/543] Writing tensor layers.48.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[439/543] Writing tensor layers.48.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[440/543] Writing tensor layers.48.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[441/543] Writing tensor layers.48.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[442/543] Writing tensor layers.48.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[443/543] Writing tensor layers.48.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[444/543] Writing tensor layers.48.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[445/543] Writing tensor layers.49.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[446/543] Writing tensor layers.49.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[447/543] Writing tensor layers.49.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[448/543] Writing tensor layers.49.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[449/543] Writing tensor layers.49.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[450/543] Writing tensor layers.49.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[451/543] Writing tensor layers.49.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[452/543] Writing tensor layers.49.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[453/543] Writing tensor layers.49.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[454/543] Writing tensor layers.50.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[455/543] Writing tensor layers.50.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[456/543] Writing tensor layers.50.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[457/543] Writing tensor layers.50.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[458/543] Writing tensor layers.50.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[459/543] Writing tensor layers.50.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[460/543] Writing tensor layers.50.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[461/543] Writing tensor layers.50.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[462/543] Writing tensor layers.50.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[463/543] Writing tensor layers.51.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[464/543] Writing tensor layers.51.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[465/543] Writing tensor layers.51.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[466/543] Writing tensor layers.51.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[467/543] Writing tensor layers.51.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[468/543] Writing tensor layers.51.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[469/543] Writing tensor layers.51.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[470/543] Writing tensor layers.51.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[471/543] Writing tensor layers.51.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[472/543] Writing tensor layers.52.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[473/543] Writing tensor layers.52.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[474/543] Writing tensor layers.52.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[475/543] Writing tensor layers.52.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[476/543] Writing tensor layers.52.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[477/543] Writing tensor layers.52.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[478/543] Writing tensor layers.52.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[479/543] Writing tensor layers.52.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[480/543] Writing tensor layers.52.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[481/543] Writing tensor layers.53.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[482/543] Writing tensor layers.53.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[483/543] Writing tensor layers.53.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[484/543] Writing tensor layers.53.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[485/543] Writing tensor layers.53.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[486/543] Writing tensor layers.53.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[487/543] Writing tensor layers.53.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[488/543] Writing tensor layers.53.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[489/543] Writing tensor layers.53.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[490/543] Writing tensor layers.54.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[491/543] Writing tensor layers.54.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[492/543] Writing tensor layers.54.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[493/543] Writing tensor layers.54.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[494/543] Writing tensor layers.54.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[495/543] Writing tensor layers.54.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[496/543] Writing tensor layers.54.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[497/543] Writing tensor layers.54.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[498/543] Writing tensor layers.54.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[499/543] Writing tensor layers.55.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[500/543] Writing tensor layers.55.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[501/543] Writing tensor layers.55.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[502/543] Writing tensor layers.55.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[503/543] Writing tensor layers.55.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[504/543] Writing tensor layers.55.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[505/543] Writing tensor layers.55.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[506/543] Writing tensor layers.55.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[507/543] Writing tensor layers.55.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[508/543] Writing tensor layers.56.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[509/543] Writing tensor layers.56.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[510/543] Writing tensor layers.56.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[511/543] Writing tensor layers.56.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[512/543] Writing tensor layers.56.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[513/543] Writing tensor layers.56.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[514/543] Writing tensor layers.56.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[515/543] Writing tensor layers.56.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[516/543] Writing tensor layers.56.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[517/543] Writing tensor layers.57.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[518/543] Writing tensor layers.57.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[519/543] Writing tensor layers.57.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[520/543] Writing tensor layers.57.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[521/543] Writing tensor layers.57.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[522/543] Writing tensor layers.57.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[523/543] Writing tensor layers.57.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[524/543] Writing tensor layers.57.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[525/543] Writing tensor layers.57.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[526/543] Writing tensor layers.58.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[527/543] Writing tensor layers.58.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[528/543] Writing tensor layers.58.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[529/543] Writing tensor layers.58.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[530/543] Writing tensor layers.58.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[531/543] Writing tensor layers.58.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[532/543] Writing tensor layers.58.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[533/543] Writing tensor layers.58.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[534/543] Writing tensor layers.58.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[535/543] Writing tensor layers.59.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[536/543] Writing tensor layers.59.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[537/543] Writing tensor layers.59.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[538/543] Writing tensor layers.59.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[539/543] Writing tensor layers.59.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[540/543] Writing tensor layers.59.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[541/543] Writing tensor layers.59.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[542/543] Writing tensor layers.59.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[543/543] Writing tensor layers.59.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/30B/ggml-model-f16.bin\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:8192 n_mult:256 n_head:64 n_layer:80\n",
      "Writing vocab...\n",
      "[  1/723] Writing tensor tok_embeddings.weight                  | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  2/723] Writing tensor norm.weight                            | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  4/723] Writing tensor layers.0.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  5/723] Writing tensor layers.0.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  6/723] Writing tensor layers.0.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  7/723] Writing tensor layers.0.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  8/723] Writing tensor layers.0.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  9/723] Writing tensor layers.0.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 10/723] Writing tensor layers.0.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 11/723] Writing tensor layers.0.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 12/723] Writing tensor layers.0.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 13/723] Writing tensor layers.1.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 14/723] Writing tensor layers.1.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 15/723] Writing tensor layers.1.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 16/723] Writing tensor layers.1.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 17/723] Writing tensor layers.1.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 18/723] Writing tensor layers.1.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 19/723] Writing tensor layers.1.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 20/723] Writing tensor layers.1.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 21/723] Writing tensor layers.1.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 22/723] Writing tensor layers.2.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 23/723] Writing tensor layers.2.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 24/723] Writing tensor layers.2.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 25/723] Writing tensor layers.2.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 26/723] Writing tensor layers.2.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 27/723] Writing tensor layers.2.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 28/723] Writing tensor layers.2.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 29/723] Writing tensor layers.2.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 30/723] Writing tensor layers.2.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 31/723] Writing tensor layers.3.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 32/723] Writing tensor layers.3.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 33/723] Writing tensor layers.3.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 34/723] Writing tensor layers.3.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 35/723] Writing tensor layers.3.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 36/723] Writing tensor layers.3.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 37/723] Writing tensor layers.3.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 38/723] Writing tensor layers.3.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 39/723] Writing tensor layers.3.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 40/723] Writing tensor layers.4.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 41/723] Writing tensor layers.4.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 42/723] Writing tensor layers.4.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 43/723] Writing tensor layers.4.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 44/723] Writing tensor layers.4.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 45/723] Writing tensor layers.4.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 46/723] Writing tensor layers.4.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 47/723] Writing tensor layers.4.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 48/723] Writing tensor layers.4.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 49/723] Writing tensor layers.5.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 50/723] Writing tensor layers.5.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 51/723] Writing tensor layers.5.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 52/723] Writing tensor layers.5.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 53/723] Writing tensor layers.5.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 54/723] Writing tensor layers.5.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 55/723] Writing tensor layers.5.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 56/723] Writing tensor layers.5.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 57/723] Writing tensor layers.5.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 58/723] Writing tensor layers.6.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 59/723] Writing tensor layers.6.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 60/723] Writing tensor layers.6.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 61/723] Writing tensor layers.6.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 62/723] Writing tensor layers.6.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 63/723] Writing tensor layers.6.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 64/723] Writing tensor layers.6.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 65/723] Writing tensor layers.6.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 66/723] Writing tensor layers.6.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 67/723] Writing tensor layers.7.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 68/723] Writing tensor layers.7.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 69/723] Writing tensor layers.7.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 70/723] Writing tensor layers.7.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 71/723] Writing tensor layers.7.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 72/723] Writing tensor layers.7.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 73/723] Writing tensor layers.7.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 74/723] Writing tensor layers.7.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 75/723] Writing tensor layers.7.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 76/723] Writing tensor layers.8.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 77/723] Writing tensor layers.8.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 78/723] Writing tensor layers.8.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 79/723] Writing tensor layers.8.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 80/723] Writing tensor layers.8.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 81/723] Writing tensor layers.8.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 82/723] Writing tensor layers.8.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 83/723] Writing tensor layers.8.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 84/723] Writing tensor layers.8.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 85/723] Writing tensor layers.9.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 86/723] Writing tensor layers.9.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 87/723] Writing tensor layers.9.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 88/723] Writing tensor layers.9.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 89/723] Writing tensor layers.9.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 90/723] Writing tensor layers.9.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 91/723] Writing tensor layers.9.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 92/723] Writing tensor layers.9.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 93/723] Writing tensor layers.9.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 94/723] Writing tensor layers.10.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 95/723] Writing tensor layers.10.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 96/723] Writing tensor layers.10.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 97/723] Writing tensor layers.10.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 98/723] Writing tensor layers.10.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 99/723] Writing tensor layers.10.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[100/723] Writing tensor layers.10.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[101/723] Writing tensor layers.10.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[102/723] Writing tensor layers.10.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[103/723] Writing tensor layers.11.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[104/723] Writing tensor layers.11.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[105/723] Writing tensor layers.11.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[106/723] Writing tensor layers.11.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[107/723] Writing tensor layers.11.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[108/723] Writing tensor layers.11.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[109/723] Writing tensor layers.11.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[110/723] Writing tensor layers.11.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[111/723] Writing tensor layers.11.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[112/723] Writing tensor layers.12.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[113/723] Writing tensor layers.12.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[114/723] Writing tensor layers.12.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[115/723] Writing tensor layers.12.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[116/723] Writing tensor layers.12.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[117/723] Writing tensor layers.12.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[118/723] Writing tensor layers.12.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[119/723] Writing tensor layers.12.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[120/723] Writing tensor layers.12.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[121/723] Writing tensor layers.13.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[122/723] Writing tensor layers.13.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[123/723] Writing tensor layers.13.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[124/723] Writing tensor layers.13.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[125/723] Writing tensor layers.13.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[126/723] Writing tensor layers.13.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[127/723] Writing tensor layers.13.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[128/723] Writing tensor layers.13.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[129/723] Writing tensor layers.13.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[130/723] Writing tensor layers.14.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[131/723] Writing tensor layers.14.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[132/723] Writing tensor layers.14.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[133/723] Writing tensor layers.14.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[134/723] Writing tensor layers.14.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[135/723] Writing tensor layers.14.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[136/723] Writing tensor layers.14.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[137/723] Writing tensor layers.14.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[138/723] Writing tensor layers.14.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[139/723] Writing tensor layers.15.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[140/723] Writing tensor layers.15.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[141/723] Writing tensor layers.15.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[142/723] Writing tensor layers.15.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[143/723] Writing tensor layers.15.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[144/723] Writing tensor layers.15.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[145/723] Writing tensor layers.15.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[146/723] Writing tensor layers.15.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[147/723] Writing tensor layers.15.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[148/723] Writing tensor layers.16.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[149/723] Writing tensor layers.16.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[150/723] Writing tensor layers.16.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[151/723] Writing tensor layers.16.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[152/723] Writing tensor layers.16.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[153/723] Writing tensor layers.16.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[154/723] Writing tensor layers.16.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[155/723] Writing tensor layers.16.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[156/723] Writing tensor layers.16.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[157/723] Writing tensor layers.17.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[158/723] Writing tensor layers.17.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[159/723] Writing tensor layers.17.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[160/723] Writing tensor layers.17.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[161/723] Writing tensor layers.17.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[162/723] Writing tensor layers.17.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[163/723] Writing tensor layers.17.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[164/723] Writing tensor layers.17.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[165/723] Writing tensor layers.17.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[166/723] Writing tensor layers.18.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[167/723] Writing tensor layers.18.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[168/723] Writing tensor layers.18.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[169/723] Writing tensor layers.18.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[170/723] Writing tensor layers.18.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[171/723] Writing tensor layers.18.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[172/723] Writing tensor layers.18.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[173/723] Writing tensor layers.18.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[174/723] Writing tensor layers.18.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[175/723] Writing tensor layers.19.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[176/723] Writing tensor layers.19.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[177/723] Writing tensor layers.19.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[178/723] Writing tensor layers.19.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[179/723] Writing tensor layers.19.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[180/723] Writing tensor layers.19.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[181/723] Writing tensor layers.19.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[182/723] Writing tensor layers.19.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[183/723] Writing tensor layers.19.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[184/723] Writing tensor layers.20.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[185/723] Writing tensor layers.20.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[186/723] Writing tensor layers.20.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[187/723] Writing tensor layers.20.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[188/723] Writing tensor layers.20.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[189/723] Writing tensor layers.20.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[190/723] Writing tensor layers.20.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[191/723] Writing tensor layers.20.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[192/723] Writing tensor layers.20.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[193/723] Writing tensor layers.21.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[194/723] Writing tensor layers.21.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[195/723] Writing tensor layers.21.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[196/723] Writing tensor layers.21.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[197/723] Writing tensor layers.21.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[198/723] Writing tensor layers.21.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[199/723] Writing tensor layers.21.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[200/723] Writing tensor layers.21.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[201/723] Writing tensor layers.21.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[202/723] Writing tensor layers.22.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[203/723] Writing tensor layers.22.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[204/723] Writing tensor layers.22.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[205/723] Writing tensor layers.22.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[206/723] Writing tensor layers.22.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[207/723] Writing tensor layers.22.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[208/723] Writing tensor layers.22.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[209/723] Writing tensor layers.22.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[210/723] Writing tensor layers.22.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[211/723] Writing tensor layers.23.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[212/723] Writing tensor layers.23.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[213/723] Writing tensor layers.23.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[214/723] Writing tensor layers.23.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[215/723] Writing tensor layers.23.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[216/723] Writing tensor layers.23.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[217/723] Writing tensor layers.23.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[218/723] Writing tensor layers.23.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[219/723] Writing tensor layers.23.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[220/723] Writing tensor layers.24.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[221/723] Writing tensor layers.24.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[222/723] Writing tensor layers.24.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[223/723] Writing tensor layers.24.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[224/723] Writing tensor layers.24.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[225/723] Writing tensor layers.24.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[226/723] Writing tensor layers.24.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[227/723] Writing tensor layers.24.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[228/723] Writing tensor layers.24.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[229/723] Writing tensor layers.25.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[230/723] Writing tensor layers.25.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[231/723] Writing tensor layers.25.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[232/723] Writing tensor layers.25.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[233/723] Writing tensor layers.25.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[234/723] Writing tensor layers.25.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[235/723] Writing tensor layers.25.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[236/723] Writing tensor layers.25.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[237/723] Writing tensor layers.25.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[238/723] Writing tensor layers.26.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[239/723] Writing tensor layers.26.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[240/723] Writing tensor layers.26.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[241/723] Writing tensor layers.26.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[242/723] Writing tensor layers.26.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[243/723] Writing tensor layers.26.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[244/723] Writing tensor layers.26.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[245/723] Writing tensor layers.26.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[246/723] Writing tensor layers.26.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[247/723] Writing tensor layers.27.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[248/723] Writing tensor layers.27.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[249/723] Writing tensor layers.27.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[250/723] Writing tensor layers.27.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[251/723] Writing tensor layers.27.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[252/723] Writing tensor layers.27.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[253/723] Writing tensor layers.27.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[254/723] Writing tensor layers.27.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[255/723] Writing tensor layers.27.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[256/723] Writing tensor layers.28.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[257/723] Writing tensor layers.28.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[258/723] Writing tensor layers.28.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[259/723] Writing tensor layers.28.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[260/723] Writing tensor layers.28.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[261/723] Writing tensor layers.28.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[262/723] Writing tensor layers.28.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[263/723] Writing tensor layers.28.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[264/723] Writing tensor layers.28.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[265/723] Writing tensor layers.29.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[266/723] Writing tensor layers.29.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[267/723] Writing tensor layers.29.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[268/723] Writing tensor layers.29.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[269/723] Writing tensor layers.29.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[270/723] Writing tensor layers.29.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[271/723] Writing tensor layers.29.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[272/723] Writing tensor layers.29.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[273/723] Writing tensor layers.29.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[274/723] Writing tensor layers.30.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[275/723] Writing tensor layers.30.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[276/723] Writing tensor layers.30.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[277/723] Writing tensor layers.30.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[278/723] Writing tensor layers.30.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[279/723] Writing tensor layers.30.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[280/723] Writing tensor layers.30.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[281/723] Writing tensor layers.30.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[282/723] Writing tensor layers.30.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[283/723] Writing tensor layers.31.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[284/723] Writing tensor layers.31.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[285/723] Writing tensor layers.31.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[286/723] Writing tensor layers.31.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[287/723] Writing tensor layers.31.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[288/723] Writing tensor layers.31.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[289/723] Writing tensor layers.31.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[290/723] Writing tensor layers.31.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[291/723] Writing tensor layers.31.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[292/723] Writing tensor layers.32.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[293/723] Writing tensor layers.32.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[294/723] Writing tensor layers.32.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[295/723] Writing tensor layers.32.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[296/723] Writing tensor layers.32.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[297/723] Writing tensor layers.32.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[298/723] Writing tensor layers.32.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[299/723] Writing tensor layers.32.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[300/723] Writing tensor layers.32.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[301/723] Writing tensor layers.33.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[302/723] Writing tensor layers.33.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[303/723] Writing tensor layers.33.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[304/723] Writing tensor layers.33.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[305/723] Writing tensor layers.33.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[306/723] Writing tensor layers.33.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[307/723] Writing tensor layers.33.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[308/723] Writing tensor layers.33.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[309/723] Writing tensor layers.33.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[310/723] Writing tensor layers.34.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[311/723] Writing tensor layers.34.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[312/723] Writing tensor layers.34.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[313/723] Writing tensor layers.34.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[314/723] Writing tensor layers.34.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[315/723] Writing tensor layers.34.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[316/723] Writing tensor layers.34.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[317/723] Writing tensor layers.34.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[318/723] Writing tensor layers.34.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[319/723] Writing tensor layers.35.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[320/723] Writing tensor layers.35.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[321/723] Writing tensor layers.35.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[322/723] Writing tensor layers.35.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[323/723] Writing tensor layers.35.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[324/723] Writing tensor layers.35.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[325/723] Writing tensor layers.35.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[326/723] Writing tensor layers.35.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[327/723] Writing tensor layers.35.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[328/723] Writing tensor layers.36.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[329/723] Writing tensor layers.36.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[330/723] Writing tensor layers.36.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[331/723] Writing tensor layers.36.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[332/723] Writing tensor layers.36.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[333/723] Writing tensor layers.36.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[334/723] Writing tensor layers.36.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[335/723] Writing tensor layers.36.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[336/723] Writing tensor layers.36.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[337/723] Writing tensor layers.37.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[338/723] Writing tensor layers.37.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[339/723] Writing tensor layers.37.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[340/723] Writing tensor layers.37.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[341/723] Writing tensor layers.37.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[342/723] Writing tensor layers.37.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[343/723] Writing tensor layers.37.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[344/723] Writing tensor layers.37.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[345/723] Writing tensor layers.37.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[346/723] Writing tensor layers.38.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[347/723] Writing tensor layers.38.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[348/723] Writing tensor layers.38.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[349/723] Writing tensor layers.38.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[350/723] Writing tensor layers.38.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[351/723] Writing tensor layers.38.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[352/723] Writing tensor layers.38.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[353/723] Writing tensor layers.38.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[354/723] Writing tensor layers.38.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[355/723] Writing tensor layers.39.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[356/723] Writing tensor layers.39.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[357/723] Writing tensor layers.39.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[358/723] Writing tensor layers.39.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[359/723] Writing tensor layers.39.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[360/723] Writing tensor layers.39.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[361/723] Writing tensor layers.39.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[362/723] Writing tensor layers.39.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[363/723] Writing tensor layers.39.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[364/723] Writing tensor layers.40.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[365/723] Writing tensor layers.40.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[366/723] Writing tensor layers.40.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[367/723] Writing tensor layers.40.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[368/723] Writing tensor layers.40.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[369/723] Writing tensor layers.40.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[370/723] Writing tensor layers.40.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[371/723] Writing tensor layers.40.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[372/723] Writing tensor layers.40.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[373/723] Writing tensor layers.41.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[374/723] Writing tensor layers.41.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[375/723] Writing tensor layers.41.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[376/723] Writing tensor layers.41.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[377/723] Writing tensor layers.41.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[378/723] Writing tensor layers.41.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[379/723] Writing tensor layers.41.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[380/723] Writing tensor layers.41.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[381/723] Writing tensor layers.41.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[382/723] Writing tensor layers.42.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[383/723] Writing tensor layers.42.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[384/723] Writing tensor layers.42.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[385/723] Writing tensor layers.42.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[386/723] Writing tensor layers.42.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[387/723] Writing tensor layers.42.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[388/723] Writing tensor layers.42.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[389/723] Writing tensor layers.42.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[390/723] Writing tensor layers.42.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[391/723] Writing tensor layers.43.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[392/723] Writing tensor layers.43.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[393/723] Writing tensor layers.43.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[394/723] Writing tensor layers.43.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[395/723] Writing tensor layers.43.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[396/723] Writing tensor layers.43.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[397/723] Writing tensor layers.43.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[398/723] Writing tensor layers.43.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[399/723] Writing tensor layers.43.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[400/723] Writing tensor layers.44.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[401/723] Writing tensor layers.44.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[402/723] Writing tensor layers.44.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[403/723] Writing tensor layers.44.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[404/723] Writing tensor layers.44.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[405/723] Writing tensor layers.44.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[406/723] Writing tensor layers.44.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[407/723] Writing tensor layers.44.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[408/723] Writing tensor layers.44.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[409/723] Writing tensor layers.45.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[410/723] Writing tensor layers.45.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[411/723] Writing tensor layers.45.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[412/723] Writing tensor layers.45.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[413/723] Writing tensor layers.45.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[414/723] Writing tensor layers.45.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[415/723] Writing tensor layers.45.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[416/723] Writing tensor layers.45.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[417/723] Writing tensor layers.45.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[418/723] Writing tensor layers.46.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[419/723] Writing tensor layers.46.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[420/723] Writing tensor layers.46.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[421/723] Writing tensor layers.46.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[422/723] Writing tensor layers.46.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[423/723] Writing tensor layers.46.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[424/723] Writing tensor layers.46.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[425/723] Writing tensor layers.46.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[426/723] Writing tensor layers.46.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[427/723] Writing tensor layers.47.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[428/723] Writing tensor layers.47.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[429/723] Writing tensor layers.47.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[430/723] Writing tensor layers.47.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[431/723] Writing tensor layers.47.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[432/723] Writing tensor layers.47.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[433/723] Writing tensor layers.47.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[434/723] Writing tensor layers.47.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[435/723] Writing tensor layers.47.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[436/723] Writing tensor layers.48.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[437/723] Writing tensor layers.48.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[438/723] Writing tensor layers.48.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[439/723] Writing tensor layers.48.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[440/723] Writing tensor layers.48.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[441/723] Writing tensor layers.48.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[442/723] Writing tensor layers.48.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[443/723] Writing tensor layers.48.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[444/723] Writing tensor layers.48.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[445/723] Writing tensor layers.49.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[446/723] Writing tensor layers.49.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[447/723] Writing tensor layers.49.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[448/723] Writing tensor layers.49.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[449/723] Writing tensor layers.49.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[450/723] Writing tensor layers.49.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[451/723] Writing tensor layers.49.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[452/723] Writing tensor layers.49.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[453/723] Writing tensor layers.49.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[454/723] Writing tensor layers.50.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[455/723] Writing tensor layers.50.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[456/723] Writing tensor layers.50.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[457/723] Writing tensor layers.50.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[458/723] Writing tensor layers.50.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[459/723] Writing tensor layers.50.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[460/723] Writing tensor layers.50.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[461/723] Writing tensor layers.50.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[462/723] Writing tensor layers.50.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[463/723] Writing tensor layers.51.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[464/723] Writing tensor layers.51.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[465/723] Writing tensor layers.51.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[466/723] Writing tensor layers.51.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[467/723] Writing tensor layers.51.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[468/723] Writing tensor layers.51.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[469/723] Writing tensor layers.51.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[470/723] Writing tensor layers.51.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[471/723] Writing tensor layers.51.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[472/723] Writing tensor layers.52.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[473/723] Writing tensor layers.52.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[474/723] Writing tensor layers.52.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[475/723] Writing tensor layers.52.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[476/723] Writing tensor layers.52.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[477/723] Writing tensor layers.52.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[478/723] Writing tensor layers.52.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[479/723] Writing tensor layers.52.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[480/723] Writing tensor layers.52.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[481/723] Writing tensor layers.53.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[482/723] Writing tensor layers.53.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[483/723] Writing tensor layers.53.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[484/723] Writing tensor layers.53.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[485/723] Writing tensor layers.53.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[486/723] Writing tensor layers.53.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[487/723] Writing tensor layers.53.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[488/723] Writing tensor layers.53.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[489/723] Writing tensor layers.53.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[490/723] Writing tensor layers.54.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[491/723] Writing tensor layers.54.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[492/723] Writing tensor layers.54.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[493/723] Writing tensor layers.54.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[494/723] Writing tensor layers.54.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[495/723] Writing tensor layers.54.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[496/723] Writing tensor layers.54.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[497/723] Writing tensor layers.54.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[498/723] Writing tensor layers.54.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[499/723] Writing tensor layers.55.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[500/723] Writing tensor layers.55.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[501/723] Writing tensor layers.55.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[502/723] Writing tensor layers.55.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[503/723] Writing tensor layers.55.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[504/723] Writing tensor layers.55.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[505/723] Writing tensor layers.55.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[506/723] Writing tensor layers.55.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[507/723] Writing tensor layers.55.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[508/723] Writing tensor layers.56.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[509/723] Writing tensor layers.56.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[510/723] Writing tensor layers.56.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[511/723] Writing tensor layers.56.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[512/723] Writing tensor layers.56.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[513/723] Writing tensor layers.56.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[514/723] Writing tensor layers.56.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[515/723] Writing tensor layers.56.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[516/723] Writing tensor layers.56.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[517/723] Writing tensor layers.57.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[518/723] Writing tensor layers.57.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[519/723] Writing tensor layers.57.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[520/723] Writing tensor layers.57.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[521/723] Writing tensor layers.57.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[522/723] Writing tensor layers.57.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[523/723] Writing tensor layers.57.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[524/723] Writing tensor layers.57.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[525/723] Writing tensor layers.57.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[526/723] Writing tensor layers.58.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[527/723] Writing tensor layers.58.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[528/723] Writing tensor layers.58.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[529/723] Writing tensor layers.58.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[530/723] Writing tensor layers.58.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[531/723] Writing tensor layers.58.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[532/723] Writing tensor layers.58.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[533/723] Writing tensor layers.58.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[534/723] Writing tensor layers.58.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[535/723] Writing tensor layers.59.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[536/723] Writing tensor layers.59.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[537/723] Writing tensor layers.59.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[538/723] Writing tensor layers.59.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[539/723] Writing tensor layers.59.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[540/723] Writing tensor layers.59.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[541/723] Writing tensor layers.59.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[542/723] Writing tensor layers.59.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[543/723] Writing tensor layers.59.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[544/723] Writing tensor layers.60.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[545/723] Writing tensor layers.60.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[546/723] Writing tensor layers.60.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[547/723] Writing tensor layers.60.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[548/723] Writing tensor layers.60.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[549/723] Writing tensor layers.60.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[550/723] Writing tensor layers.60.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[551/723] Writing tensor layers.60.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[552/723] Writing tensor layers.60.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[553/723] Writing tensor layers.61.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[554/723] Writing tensor layers.61.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[555/723] Writing tensor layers.61.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[556/723] Writing tensor layers.61.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[557/723] Writing tensor layers.61.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[558/723] Writing tensor layers.61.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[559/723] Writing tensor layers.61.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[560/723] Writing tensor layers.61.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[561/723] Writing tensor layers.61.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[562/723] Writing tensor layers.62.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[563/723] Writing tensor layers.62.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[564/723] Writing tensor layers.62.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[565/723] Writing tensor layers.62.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[566/723] Writing tensor layers.62.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[567/723] Writing tensor layers.62.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[568/723] Writing tensor layers.62.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[569/723] Writing tensor layers.62.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[570/723] Writing tensor layers.62.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[571/723] Writing tensor layers.63.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[572/723] Writing tensor layers.63.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[573/723] Writing tensor layers.63.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[574/723] Writing tensor layers.63.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[575/723] Writing tensor layers.63.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[576/723] Writing tensor layers.63.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[577/723] Writing tensor layers.63.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[578/723] Writing tensor layers.63.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[579/723] Writing tensor layers.63.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[580/723] Writing tensor layers.64.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[581/723] Writing tensor layers.64.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[582/723] Writing tensor layers.64.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[583/723] Writing tensor layers.64.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[584/723] Writing tensor layers.64.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[585/723] Writing tensor layers.64.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[586/723] Writing tensor layers.64.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[587/723] Writing tensor layers.64.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[588/723] Writing tensor layers.64.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[589/723] Writing tensor layers.65.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[590/723] Writing tensor layers.65.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[591/723] Writing tensor layers.65.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[592/723] Writing tensor layers.65.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[593/723] Writing tensor layers.65.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[594/723] Writing tensor layers.65.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[595/723] Writing tensor layers.65.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[596/723] Writing tensor layers.65.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[597/723] Writing tensor layers.65.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[598/723] Writing tensor layers.66.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[599/723] Writing tensor layers.66.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[600/723] Writing tensor layers.66.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[601/723] Writing tensor layers.66.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[602/723] Writing tensor layers.66.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[603/723] Writing tensor layers.66.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[604/723] Writing tensor layers.66.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[605/723] Writing tensor layers.66.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[606/723] Writing tensor layers.66.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[607/723] Writing tensor layers.67.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[608/723] Writing tensor layers.67.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[609/723] Writing tensor layers.67.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[610/723] Writing tensor layers.67.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[611/723] Writing tensor layers.67.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[612/723] Writing tensor layers.67.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[613/723] Writing tensor layers.67.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[614/723] Writing tensor layers.67.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[615/723] Writing tensor layers.67.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[616/723] Writing tensor layers.68.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[617/723] Writing tensor layers.68.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[618/723] Writing tensor layers.68.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[619/723] Writing tensor layers.68.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[620/723] Writing tensor layers.68.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[621/723] Writing tensor layers.68.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[622/723] Writing tensor layers.68.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[623/723] Writing tensor layers.68.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[624/723] Writing tensor layers.68.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[625/723] Writing tensor layers.69.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[626/723] Writing tensor layers.69.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[627/723] Writing tensor layers.69.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[628/723] Writing tensor layers.69.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[629/723] Writing tensor layers.69.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[630/723] Writing tensor layers.69.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[631/723] Writing tensor layers.69.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[632/723] Writing tensor layers.69.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[633/723] Writing tensor layers.69.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[634/723] Writing tensor layers.70.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[635/723] Writing tensor layers.70.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[636/723] Writing tensor layers.70.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[637/723] Writing tensor layers.70.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[638/723] Writing tensor layers.70.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[639/723] Writing tensor layers.70.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[640/723] Writing tensor layers.70.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[641/723] Writing tensor layers.70.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[642/723] Writing tensor layers.70.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[643/723] Writing tensor layers.71.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[644/723] Writing tensor layers.71.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[645/723] Writing tensor layers.71.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[646/723] Writing tensor layers.71.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[647/723] Writing tensor layers.71.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[648/723] Writing tensor layers.71.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[649/723] Writing tensor layers.71.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[650/723] Writing tensor layers.71.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[651/723] Writing tensor layers.71.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[652/723] Writing tensor layers.72.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[653/723] Writing tensor layers.72.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[654/723] Writing tensor layers.72.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[655/723] Writing tensor layers.72.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[656/723] Writing tensor layers.72.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[657/723] Writing tensor layers.72.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[658/723] Writing tensor layers.72.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[659/723] Writing tensor layers.72.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[660/723] Writing tensor layers.72.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[661/723] Writing tensor layers.73.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[662/723] Writing tensor layers.73.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[663/723] Writing tensor layers.73.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[664/723] Writing tensor layers.73.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[665/723] Writing tensor layers.73.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[666/723] Writing tensor layers.73.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[667/723] Writing tensor layers.73.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[668/723] Writing tensor layers.73.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[669/723] Writing tensor layers.73.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[670/723] Writing tensor layers.74.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[671/723] Writing tensor layers.74.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[672/723] Writing tensor layers.74.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[673/723] Writing tensor layers.74.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[674/723] Writing tensor layers.74.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[675/723] Writing tensor layers.74.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[676/723] Writing tensor layers.74.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[677/723] Writing tensor layers.74.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[678/723] Writing tensor layers.74.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[679/723] Writing tensor layers.75.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[680/723] Writing tensor layers.75.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[681/723] Writing tensor layers.75.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[682/723] Writing tensor layers.75.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[683/723] Writing tensor layers.75.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[684/723] Writing tensor layers.75.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[685/723] Writing tensor layers.75.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[686/723] Writing tensor layers.75.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[687/723] Writing tensor layers.75.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[688/723] Writing tensor layers.76.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[689/723] Writing tensor layers.76.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[690/723] Writing tensor layers.76.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[691/723] Writing tensor layers.76.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[692/723] Writing tensor layers.76.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[693/723] Writing tensor layers.76.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[694/723] Writing tensor layers.76.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[695/723] Writing tensor layers.76.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[696/723] Writing tensor layers.76.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[697/723] Writing tensor layers.77.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[698/723] Writing tensor layers.77.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[699/723] Writing tensor layers.77.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[700/723] Writing tensor layers.77.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[701/723] Writing tensor layers.77.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[702/723] Writing tensor layers.77.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[703/723] Writing tensor layers.77.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[704/723] Writing tensor layers.77.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[705/723] Writing tensor layers.77.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[706/723] Writing tensor layers.78.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[707/723] Writing tensor layers.78.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[708/723] Writing tensor layers.78.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[709/723] Writing tensor layers.78.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[710/723] Writing tensor layers.78.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[711/723] Writing tensor layers.78.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[712/723] Writing tensor layers.78.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[713/723] Writing tensor layers.78.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[714/723] Writing tensor layers.78.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[715/723] Writing tensor layers.79.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[716/723] Writing tensor layers.79.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[717/723] Writing tensor layers.79.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[718/723] Writing tensor layers.79.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[719/723] Writing tensor layers.79.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[720/723] Writing tensor layers.79.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[721/723] Writing tensor layers.79.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[722/723] Writing tensor layers.79.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[723/723] Writing tensor layers.79.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/65B/ggml-model-f16.bin\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/7B/ggml-model-q4_0.bin\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->   102.54 MB | hist: \n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 49700.37 ms\n",
      "main:    total time = 49700.37 ms\n",
      "main: build = 847 (7568d1a)\n",
      "main: quantizing './models/13B/ggml-model-f16.bin' to './models/13B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/13B/ggml-model-q4_0.bin\n",
      "[   1/ 363]                tok_embeddings.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->    87.89 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                          norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->   128.17 MB | hist: \n",
      "[   4/ 363]         layers.0.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]         layers.0.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]         layers.0.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]         layers.0.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]       layers.0.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   9/ 363]      layers.0.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 363]      layers.0.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 363]      layers.0.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 363]             layers.0.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  13/ 363]         layers.1.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]         layers.1.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]         layers.1.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]         layers.1.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]       layers.1.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  18/ 363]      layers.1.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]      layers.1.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]      layers.1.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 363]             layers.1.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  22/ 363]         layers.2.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]         layers.2.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]         layers.2.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]         layers.2.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]       layers.2.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  27/ 363]      layers.2.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]      layers.2.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]      layers.2.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 363]             layers.2.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  31/ 363]         layers.3.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]         layers.3.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]         layers.3.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]         layers.3.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]       layers.3.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  36/ 363]      layers.3.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]      layers.3.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]      layers.3.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 363]             layers.3.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  40/ 363]         layers.4.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]         layers.4.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]         layers.4.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]         layers.4.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]       layers.4.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  45/ 363]      layers.4.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]      layers.4.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]      layers.4.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 363]             layers.4.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  49/ 363]         layers.5.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]         layers.5.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]         layers.5.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]         layers.5.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]       layers.5.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  54/ 363]      layers.5.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]      layers.5.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]      layers.5.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 363]             layers.5.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  58/ 363]         layers.6.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]         layers.6.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]         layers.6.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]         layers.6.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]       layers.6.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  63/ 363]      layers.6.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 363]      layers.6.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 363]      layers.6.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 363]             layers.6.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  67/ 363]         layers.7.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]         layers.7.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]         layers.7.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]         layers.7.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]       layers.7.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  72/ 363]      layers.7.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 363]      layers.7.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 363]      layers.7.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 363]             layers.7.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  76/ 363]         layers.8.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]         layers.8.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]         layers.8.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]         layers.8.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]       layers.8.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  81/ 363]      layers.8.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]      layers.8.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]      layers.8.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 363]             layers.8.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  85/ 363]         layers.9.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]         layers.9.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]         layers.9.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]         layers.9.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]       layers.9.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  90/ 363]      layers.9.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 363]      layers.9.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 363]      layers.9.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 363]             layers.9.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  94/ 363]        layers.10.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]        layers.10.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]        layers.10.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]        layers.10.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]      layers.10.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  99/ 363]     layers.10.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 363]     layers.10.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 363]     layers.10.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 363]            layers.10.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]        layers.11.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]        layers.11.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]        layers.11.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]        layers.11.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]      layers.11.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 108/ 363]     layers.11.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 363]     layers.11.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 363]     layers.11.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 363]            layers.11.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]        layers.12.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]        layers.12.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]        layers.12.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]        layers.12.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]      layers.12.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 117/ 363]     layers.12.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 363]     layers.12.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 363]     layers.12.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 363]            layers.12.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]        layers.13.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]        layers.13.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]        layers.13.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]        layers.13.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]      layers.13.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 126/ 363]     layers.13.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 363]     layers.13.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 363]     layers.13.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 363]            layers.13.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]        layers.14.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]        layers.14.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]        layers.14.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]        layers.14.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]      layers.14.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 135/ 363]     layers.14.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 363]     layers.14.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 363]     layers.14.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 363]            layers.14.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]        layers.15.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]        layers.15.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]        layers.15.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]        layers.15.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]      layers.15.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 144/ 363]     layers.15.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]     layers.15.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]     layers.15.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 363]            layers.15.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]        layers.16.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]        layers.16.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]        layers.16.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]        layers.16.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]      layers.16.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 153/ 363]     layers.16.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 363]     layers.16.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 363]     layers.16.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 363]            layers.16.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]        layers.17.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]        layers.17.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]        layers.17.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]        layers.17.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]      layers.17.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 162/ 363]     layers.17.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]     layers.17.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]     layers.17.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 363]            layers.17.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]        layers.18.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]        layers.18.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]        layers.18.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]        layers.18.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]      layers.18.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 171/ 363]     layers.18.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]     layers.18.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]     layers.18.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 363]            layers.18.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]        layers.19.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]        layers.19.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]        layers.19.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]        layers.19.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]      layers.19.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 180/ 363]     layers.19.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]     layers.19.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]     layers.19.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 363]            layers.19.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]        layers.20.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]        layers.20.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]        layers.20.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]        layers.20.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]      layers.20.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 189/ 363]     layers.20.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]     layers.20.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]     layers.20.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 363]            layers.20.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]        layers.21.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]        layers.21.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]        layers.21.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]        layers.21.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]      layers.21.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 198/ 363]     layers.21.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]     layers.21.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]     layers.21.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 363]            layers.21.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]        layers.22.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]        layers.22.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]        layers.22.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]        layers.22.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]      layers.22.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 207/ 363]     layers.22.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]     layers.22.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]     layers.22.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 363]            layers.22.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]        layers.23.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]        layers.23.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]        layers.23.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]        layers.23.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]      layers.23.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 216/ 363]     layers.23.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]     layers.23.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]     layers.23.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 363]            layers.23.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]        layers.24.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]        layers.24.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]        layers.24.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]        layers.24.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]      layers.24.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 225/ 363]     layers.24.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]     layers.24.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]     layers.24.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 363]            layers.24.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]        layers.25.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]        layers.25.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]        layers.25.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]        layers.25.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]      layers.25.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 234/ 363]     layers.25.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]     layers.25.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]     layers.25.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 363]            layers.25.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]        layers.26.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]        layers.26.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]        layers.26.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]        layers.26.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]      layers.26.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 243/ 363]     layers.26.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]     layers.26.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]     layers.26.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 363]            layers.26.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]        layers.27.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]        layers.27.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]        layers.27.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]        layers.27.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]      layers.27.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 252/ 363]     layers.27.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]     layers.27.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]     layers.27.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 363]            layers.27.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]        layers.28.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]        layers.28.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]        layers.28.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]        layers.28.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]      layers.28.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 261/ 363]     layers.28.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]     layers.28.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]     layers.28.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 363]            layers.28.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]        layers.29.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]        layers.29.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]        layers.29.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]        layers.29.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]      layers.29.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 270/ 363]     layers.29.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]     layers.29.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]     layers.29.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 363]            layers.29.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]        layers.30.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]        layers.30.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]        layers.30.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]        layers.30.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]      layers.30.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 279/ 363]     layers.30.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]     layers.30.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]     layers.30.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 363]            layers.30.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]        layers.31.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]        layers.31.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]        layers.31.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]        layers.31.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]      layers.31.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 288/ 363]     layers.31.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]     layers.31.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]     layers.31.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 363]            layers.31.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]        layers.32.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]        layers.32.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]        layers.32.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]        layers.32.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]      layers.32.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 297/ 363]     layers.32.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]     layers.32.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]     layers.32.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 363]            layers.32.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]        layers.33.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]        layers.33.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]        layers.33.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]        layers.33.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]      layers.33.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 306/ 363]     layers.33.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]     layers.33.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]     layers.33.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 363]            layers.33.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]        layers.34.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]        layers.34.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]        layers.34.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]        layers.34.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]      layers.34.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 315/ 363]     layers.34.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]     layers.34.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]     layers.34.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 363]            layers.34.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]        layers.35.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]        layers.35.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]        layers.35.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]        layers.35.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]      layers.35.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 324/ 363]     layers.35.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]     layers.35.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]     layers.35.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 363]            layers.35.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]        layers.36.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]        layers.36.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]        layers.36.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]        layers.36.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]      layers.36.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 333/ 363]     layers.36.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 363]     layers.36.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 335/ 363]     layers.36.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 363]            layers.36.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]        layers.37.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]        layers.37.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]        layers.37.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]        layers.37.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]      layers.37.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 342/ 363]     layers.37.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 363]     layers.37.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 344/ 363]     layers.37.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 363]            layers.37.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]        layers.38.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]        layers.38.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]        layers.38.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]        layers.38.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]      layers.38.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 351/ 363]     layers.38.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 363]     layers.38.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 353/ 363]     layers.38.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 363]            layers.38.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]        layers.39.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]        layers.39.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]        layers.39.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 60200.24 ms\n",
      "main:    total time = 60200.24 ms\n",
      "main: build = 847 (7568d1a)\n",
      "main: quantizing './models/30B/ggml-model-f16.bin' to './models/30B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/30B/ggml-model-q4_0.bin\n",
      "[   1/ 543]                tok_embeddings.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   114.26 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                          norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   166.63 MB | hist: \n",
      "[   4/ 543]         layers.0.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]         layers.0.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]         layers.0.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]         layers.0.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]       layers.0.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   9/ 543]      layers.0.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 543]      layers.0.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 543]      layers.0.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 543]             layers.0.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  13/ 543]         layers.1.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]         layers.1.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]         layers.1.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]         layers.1.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]       layers.1.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  18/ 543]      layers.1.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]      layers.1.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]      layers.1.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 543]             layers.1.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  22/ 543]         layers.2.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]         layers.2.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]         layers.2.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]         layers.2.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]       layers.2.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  27/ 543]      layers.2.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]      layers.2.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]      layers.2.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 543]             layers.2.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  31/ 543]         layers.3.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]         layers.3.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]         layers.3.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]         layers.3.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]       layers.3.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  36/ 543]      layers.3.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 543]      layers.3.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  38/ 543]      layers.3.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 543]             layers.3.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  40/ 543]         layers.4.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]         layers.4.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]         layers.4.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]         layers.4.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]       layers.4.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  45/ 543]      layers.4.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]      layers.4.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]      layers.4.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 543]             layers.4.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  49/ 543]         layers.5.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]         layers.5.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]         layers.5.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]         layers.5.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]       layers.5.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  54/ 543]      layers.5.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]      layers.5.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]      layers.5.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 543]             layers.5.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  58/ 543]         layers.6.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]         layers.6.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]         layers.6.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]         layers.6.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]       layers.6.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  63/ 543]      layers.6.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]      layers.6.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]      layers.6.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 543]             layers.6.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  67/ 543]         layers.7.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]         layers.7.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]         layers.7.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]         layers.7.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]       layers.7.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  72/ 543]      layers.7.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]      layers.7.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]      layers.7.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 543]             layers.7.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  76/ 543]         layers.8.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]         layers.8.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]         layers.8.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]         layers.8.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]       layers.8.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  81/ 543]      layers.8.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]      layers.8.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]      layers.8.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 543]             layers.8.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  85/ 543]         layers.9.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]         layers.9.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]         layers.9.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]         layers.9.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]       layers.9.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  90/ 543]      layers.9.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]      layers.9.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]      layers.9.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 543]             layers.9.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  94/ 543]        layers.10.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]        layers.10.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]        layers.10.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]        layers.10.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]      layers.10.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  99/ 543]     layers.10.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]     layers.10.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]     layers.10.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 543]            layers.10.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]        layers.11.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]        layers.11.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]        layers.11.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]        layers.11.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]      layers.11.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 108/ 543]     layers.11.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]     layers.11.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]     layers.11.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 543]            layers.11.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]        layers.12.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]        layers.12.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]        layers.12.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]        layers.12.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]      layers.12.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 117/ 543]     layers.12.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]     layers.12.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]     layers.12.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 543]            layers.12.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]        layers.13.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]        layers.13.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]        layers.13.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]        layers.13.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]      layers.13.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 126/ 543]     layers.13.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]     layers.13.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]     layers.13.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 543]            layers.13.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]        layers.14.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]        layers.14.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]        layers.14.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]        layers.14.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]      layers.14.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 135/ 543]     layers.14.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 543]     layers.14.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 543]     layers.14.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 543]            layers.14.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]        layers.15.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]        layers.15.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]        layers.15.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]        layers.15.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]      layers.15.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 144/ 543]     layers.15.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]     layers.15.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]     layers.15.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 543]            layers.15.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]        layers.16.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]        layers.16.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]        layers.16.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]        layers.16.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]      layers.16.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 153/ 543]     layers.16.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]     layers.16.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]     layers.16.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 543]            layers.16.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]        layers.17.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]        layers.17.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]        layers.17.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]        layers.17.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]      layers.17.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 162/ 543]     layers.17.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]     layers.17.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]     layers.17.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 543]            layers.17.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]        layers.18.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]        layers.18.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]        layers.18.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]        layers.18.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]      layers.18.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 171/ 543]     layers.18.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]     layers.18.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]     layers.18.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 543]            layers.18.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]        layers.19.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]        layers.19.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]        layers.19.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]        layers.19.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]      layers.19.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 180/ 543]     layers.19.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 543]     layers.19.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 543]     layers.19.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 543]            layers.19.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]        layers.20.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]        layers.20.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]        layers.20.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]        layers.20.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]      layers.20.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 189/ 543]     layers.20.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 543]     layers.20.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 543]     layers.20.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 543]            layers.20.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]        layers.21.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]        layers.21.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]        layers.21.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]        layers.21.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]      layers.21.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 198/ 543]     layers.21.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]     layers.21.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]     layers.21.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 543]            layers.21.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]        layers.22.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]        layers.22.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]        layers.22.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]        layers.22.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]      layers.22.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 207/ 543]     layers.22.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]     layers.22.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]     layers.22.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 543]            layers.22.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]        layers.23.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]        layers.23.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]        layers.23.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]        layers.23.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]      layers.23.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 216/ 543]     layers.23.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]     layers.23.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]     layers.23.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 543]            layers.23.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]        layers.24.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]        layers.24.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]        layers.24.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]        layers.24.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]      layers.24.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 225/ 543]     layers.24.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]     layers.24.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]     layers.24.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 543]            layers.24.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]        layers.25.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]        layers.25.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]        layers.25.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]        layers.25.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]      layers.25.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 234/ 543]     layers.25.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]     layers.25.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]     layers.25.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 543]            layers.25.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]        layers.26.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]        layers.26.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]        layers.26.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]        layers.26.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]      layers.26.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 243/ 543]     layers.26.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]     layers.26.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]     layers.26.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 543]            layers.26.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]        layers.27.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]        layers.27.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]        layers.27.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]        layers.27.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]      layers.27.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 252/ 543]     layers.27.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]     layers.27.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]     layers.27.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 543]            layers.27.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]        layers.28.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]        layers.28.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]        layers.28.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]        layers.28.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]      layers.28.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 261/ 543]     layers.28.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]     layers.28.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]     layers.28.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 543]            layers.28.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]        layers.29.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]        layers.29.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]        layers.29.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]        layers.29.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]      layers.29.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 270/ 543]     layers.29.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]     layers.29.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]     layers.29.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 543]            layers.29.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]        layers.30.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]        layers.30.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]        layers.30.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]        layers.30.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]      layers.30.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 279/ 543]     layers.30.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]     layers.30.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]     layers.30.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 543]            layers.30.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]        layers.31.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]        layers.31.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]        layers.31.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]        layers.31.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]      layers.31.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 288/ 543]     layers.31.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]     layers.31.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]     layers.31.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 543]            layers.31.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]        layers.32.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]        layers.32.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]        layers.32.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]        layers.32.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]      layers.32.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 297/ 543]     layers.32.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]     layers.32.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]     layers.32.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 543]            layers.32.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]        layers.33.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]        layers.33.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]        layers.33.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]        layers.33.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]      layers.33.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 306/ 543]     layers.33.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]     layers.33.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]     layers.33.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 543]            layers.33.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]        layers.34.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]        layers.34.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]        layers.34.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]        layers.34.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]      layers.34.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 315/ 543]     layers.34.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]     layers.34.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]     layers.34.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 543]            layers.34.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]        layers.35.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]        layers.35.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]        layers.35.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]        layers.35.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]      layers.35.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 324/ 543]     layers.35.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]     layers.35.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]     layers.35.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 543]            layers.35.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]        layers.36.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]        layers.36.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]        layers.36.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]        layers.36.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]      layers.36.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 333/ 543]     layers.36.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]     layers.36.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]     layers.36.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 543]            layers.36.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]        layers.37.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]        layers.37.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]        layers.37.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]        layers.37.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]      layers.37.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 342/ 543]     layers.37.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]     layers.37.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]     layers.37.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 543]            layers.37.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]        layers.38.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]        layers.38.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]        layers.38.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]        layers.38.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]      layers.38.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 351/ 543]     layers.38.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]     layers.38.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]     layers.38.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 543]            layers.38.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]        layers.39.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]        layers.39.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]        layers.39.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]        layers.39.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]      layers.39.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 360/ 543]     layers.39.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]     layers.39.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]     layers.39.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 543]            layers.39.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]        layers.40.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]        layers.40.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]        layers.40.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]        layers.40.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]      layers.40.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 369/ 543]     layers.40.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]     layers.40.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]     layers.40.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 543]            layers.40.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]        layers.41.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]        layers.41.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]        layers.41.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]        layers.41.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]      layers.41.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 378/ 543]     layers.41.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]     layers.41.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]     layers.41.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 543]            layers.41.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]        layers.42.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]        layers.42.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]        layers.42.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]        layers.42.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]      layers.42.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 387/ 543]     layers.42.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]     layers.42.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]     layers.42.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 543]            layers.42.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]        layers.43.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]        layers.43.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]        layers.43.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]        layers.43.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]      layers.43.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 396/ 543]     layers.43.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]     layers.43.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]     layers.43.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 543]            layers.43.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]        layers.44.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]        layers.44.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]        layers.44.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]        layers.44.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]      layers.44.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 405/ 543]     layers.44.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]     layers.44.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]     layers.44.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 543]            layers.44.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]        layers.45.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]        layers.45.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]        layers.45.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]        layers.45.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]      layers.45.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 414/ 543]     layers.45.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]     layers.45.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]     layers.45.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 543]            layers.45.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]        layers.46.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]        layers.46.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]        layers.46.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]        layers.46.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]      layers.46.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 423/ 543]     layers.46.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]     layers.46.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]     layers.46.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 543]            layers.46.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]        layers.47.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]        layers.47.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]        layers.47.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]        layers.47.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]      layers.47.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 432/ 543]     layers.47.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]     layers.47.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]     layers.47.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 543]            layers.47.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]        layers.48.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]        layers.48.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]        layers.48.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]        layers.48.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]      layers.48.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 441/ 543]     layers.48.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]     layers.48.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]     layers.48.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 543]            layers.48.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]        layers.49.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]        layers.49.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]        layers.49.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]        layers.49.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]      layers.49.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 450/ 543]     layers.49.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]     layers.49.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]     layers.49.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 543]            layers.49.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]        layers.50.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]        layers.50.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]        layers.50.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]        layers.50.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]      layers.50.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 459/ 543]     layers.50.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]     layers.50.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]     layers.50.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 543]            layers.50.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]        layers.51.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]        layers.51.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]        layers.51.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]        layers.51.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]      layers.51.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 468/ 543]     layers.51.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]     layers.51.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]     layers.51.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 543]            layers.51.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]        layers.52.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]        layers.52.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]        layers.52.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]        layers.52.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]      layers.52.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 477/ 543]     layers.52.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]     layers.52.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]     layers.52.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 543]            layers.52.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]        layers.53.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]        layers.53.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]        layers.53.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]        layers.53.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]      layers.53.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 486/ 543]     layers.53.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]     layers.53.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]     layers.53.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 543]            layers.53.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]        layers.54.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]        layers.54.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]        layers.54.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]        layers.54.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]      layers.54.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 495/ 543]     layers.54.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]     layers.54.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]     layers.54.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 543]            layers.54.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]        layers.55.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]        layers.55.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]        layers.55.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]        layers.55.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]      layers.55.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 504/ 543]     layers.55.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]     layers.55.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]     layers.55.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 543]            layers.55.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]        layers.56.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]        layers.56.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]        layers.56.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]        layers.56.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]      layers.56.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 513/ 543]     layers.56.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 543]     layers.56.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 515/ 543]     layers.56.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 543]            layers.56.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]        layers.57.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]        layers.57.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]        layers.57.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]        layers.57.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]      layers.57.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 522/ 543]     layers.57.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 543]     layers.57.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 524/ 543]     layers.57.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 543]            layers.57.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]        layers.58.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]        layers.58.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]        layers.58.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]        layers.58.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]      layers.58.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 531/ 543]     layers.58.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 543]     layers.58.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 533/ 543]     layers.58.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 543]            layers.58.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]        layers.59.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]        layers.59.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]        layers.59.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]        layers.59.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]      layers.59.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 540/ 543]     layers.59.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 541/ 543]     layers.59.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 542/ 543]     layers.59.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 543/ 543]            layers.59.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 144095.67 ms\n",
      "main:    total time = 144095.67 ms\n",
      "main: build = 847 (7568d1a)\n",
      "main: quantizing './models/65B/ggml-model-f16.bin' to './models/65B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/65B/ggml-model-q4_0.bin\n",
      "[   1/ 723]                tok_embeddings.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   140.62 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                          norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   205.08 MB | hist: \n",
      "[   4/ 723]         layers.0.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]         layers.0.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]         layers.0.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]         layers.0.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]       layers.0.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   9/ 723]      layers.0.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[  10/ 723]      layers.0.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  11/ 723]      layers.0.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  12/ 723]             layers.0.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  13/ 723]         layers.1.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]         layers.1.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]         layers.1.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]         layers.1.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]       layers.1.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  18/ 723]      layers.1.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]      layers.1.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]      layers.1.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 723]             layers.1.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  22/ 723]         layers.2.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]         layers.2.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]         layers.2.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]         layers.2.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]       layers.2.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  27/ 723]      layers.2.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]      layers.2.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]      layers.2.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 723]             layers.2.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  31/ 723]         layers.3.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]         layers.3.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]         layers.3.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]         layers.3.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]       layers.3.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  36/ 723]      layers.3.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]      layers.3.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]      layers.3.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 723]             layers.3.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  40/ 723]         layers.4.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]         layers.4.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]         layers.4.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]         layers.4.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]       layers.4.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  45/ 723]      layers.4.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]      layers.4.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]      layers.4.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 723]             layers.4.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  49/ 723]         layers.5.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]         layers.5.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]         layers.5.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]         layers.5.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]       layers.5.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  54/ 723]      layers.5.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]      layers.5.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]      layers.5.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 723]             layers.5.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  58/ 723]         layers.6.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]         layers.6.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]         layers.6.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]         layers.6.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]       layers.6.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  63/ 723]      layers.6.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]      layers.6.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]      layers.6.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 723]             layers.6.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  67/ 723]         layers.7.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]         layers.7.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]         layers.7.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]         layers.7.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]       layers.7.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  72/ 723]      layers.7.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]      layers.7.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]      layers.7.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 723]             layers.7.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  76/ 723]         layers.8.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]         layers.8.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]         layers.8.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]         layers.8.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]       layers.8.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  81/ 723]      layers.8.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]      layers.8.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]      layers.8.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 723]             layers.8.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  85/ 723]         layers.9.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]         layers.9.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]         layers.9.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]         layers.9.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]       layers.9.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  90/ 723]      layers.9.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 723]      layers.9.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 723]      layers.9.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 723]             layers.9.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  94/ 723]        layers.10.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]        layers.10.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]        layers.10.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]        layers.10.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]      layers.10.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  99/ 723]     layers.10.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 723]     layers.10.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 723]     layers.10.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 723]            layers.10.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]        layers.11.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]        layers.11.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]        layers.11.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]        layers.11.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]      layers.11.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 108/ 723]     layers.11.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]     layers.11.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]     layers.11.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 723]            layers.11.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]        layers.12.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]        layers.12.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]        layers.12.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]        layers.12.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]      layers.12.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 117/ 723]     layers.12.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]     layers.12.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]     layers.12.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 723]            layers.12.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]        layers.13.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]        layers.13.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]        layers.13.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]        layers.13.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]      layers.13.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 126/ 723]     layers.13.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]     layers.13.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]     layers.13.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 723]            layers.13.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]        layers.14.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]        layers.14.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]        layers.14.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]        layers.14.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]      layers.14.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 135/ 723]     layers.14.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]     layers.14.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]     layers.14.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 723]            layers.14.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]        layers.15.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]        layers.15.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]        layers.15.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]        layers.15.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]      layers.15.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 144/ 723]     layers.15.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 723]     layers.15.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 723]     layers.15.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 723]            layers.15.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]        layers.16.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]        layers.16.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]        layers.16.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]        layers.16.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]      layers.16.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 153/ 723]     layers.16.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]     layers.16.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]     layers.16.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 723]            layers.16.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]        layers.17.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]        layers.17.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]        layers.17.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]        layers.17.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]      layers.17.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 162/ 723]     layers.17.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]     layers.17.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]     layers.17.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 723]            layers.17.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]        layers.18.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]        layers.18.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]        layers.18.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]        layers.18.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]      layers.18.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 171/ 723]     layers.18.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]     layers.18.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]     layers.18.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 723]            layers.18.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]        layers.19.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]        layers.19.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]        layers.19.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]        layers.19.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]      layers.19.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 180/ 723]     layers.19.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]     layers.19.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]     layers.19.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 723]            layers.19.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]        layers.20.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]        layers.20.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]        layers.20.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]        layers.20.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]      layers.20.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 189/ 723]     layers.20.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]     layers.20.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]     layers.20.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 723]            layers.20.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]        layers.21.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]        layers.21.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]        layers.21.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]        layers.21.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]      layers.21.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 198/ 723]     layers.21.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]     layers.21.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]     layers.21.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 723]            layers.21.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]        layers.22.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]        layers.22.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]        layers.22.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]        layers.22.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]      layers.22.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 207/ 723]     layers.22.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]     layers.22.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]     layers.22.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 723]            layers.22.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]        layers.23.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]        layers.23.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]        layers.23.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]        layers.23.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]      layers.23.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 216/ 723]     layers.23.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]     layers.23.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]     layers.23.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 723]            layers.23.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]        layers.24.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]        layers.24.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]        layers.24.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]        layers.24.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]      layers.24.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 225/ 723]     layers.24.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]     layers.24.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]     layers.24.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 723]            layers.24.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]        layers.25.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]        layers.25.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]        layers.25.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]        layers.25.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]      layers.25.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 234/ 723]     layers.25.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]     layers.25.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]     layers.25.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 723]            layers.25.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]        layers.26.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]        layers.26.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]        layers.26.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]        layers.26.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]      layers.26.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 243/ 723]     layers.26.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]     layers.26.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]     layers.26.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 723]            layers.26.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]        layers.27.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]        layers.27.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]        layers.27.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]        layers.27.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]      layers.27.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 252/ 723]     layers.27.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]     layers.27.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]     layers.27.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 723]            layers.27.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]        layers.28.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]        layers.28.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]        layers.28.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]        layers.28.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]      layers.28.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 261/ 723]     layers.28.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]     layers.28.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]     layers.28.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 723]            layers.28.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]        layers.29.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]        layers.29.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]        layers.29.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]        layers.29.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]      layers.29.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 270/ 723]     layers.29.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]     layers.29.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]     layers.29.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 723]            layers.29.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]        layers.30.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]        layers.30.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]        layers.30.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]        layers.30.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]      layers.30.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 279/ 723]     layers.30.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]     layers.30.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]     layers.30.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 723]            layers.30.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]        layers.31.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]        layers.31.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]        layers.31.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]        layers.31.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]      layers.31.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 288/ 723]     layers.31.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]     layers.31.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]     layers.31.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 723]            layers.31.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]        layers.32.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]        layers.32.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]        layers.32.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]        layers.32.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]      layers.32.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 297/ 723]     layers.32.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]     layers.32.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]     layers.32.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 723]            layers.32.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]        layers.33.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]        layers.33.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]        layers.33.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]        layers.33.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]      layers.33.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 306/ 723]     layers.33.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]     layers.33.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]     layers.33.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 723]            layers.33.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]        layers.34.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]        layers.34.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]        layers.34.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]        layers.34.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]      layers.34.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 315/ 723]     layers.34.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]     layers.34.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]     layers.34.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 723]            layers.34.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]        layers.35.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]        layers.35.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]        layers.35.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]        layers.35.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]      layers.35.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 324/ 723]     layers.35.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]     layers.35.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]     layers.35.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 723]            layers.35.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]        layers.36.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]        layers.36.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]        layers.36.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]        layers.36.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]      layers.36.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 333/ 723]     layers.36.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]     layers.36.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]     layers.36.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 723]            layers.36.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]        layers.37.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]        layers.37.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]        layers.37.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]        layers.37.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]      layers.37.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 342/ 723]     layers.37.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]     layers.37.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]     layers.37.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 723]            layers.37.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]        layers.38.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]        layers.38.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]        layers.38.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]        layers.38.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]      layers.38.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 351/ 723]     layers.38.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]     layers.38.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]     layers.38.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 723]            layers.38.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]        layers.39.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]        layers.39.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]        layers.39.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]        layers.39.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]      layers.39.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 360/ 723]     layers.39.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]     layers.39.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]     layers.39.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 723]            layers.39.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]        layers.40.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]        layers.40.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]        layers.40.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]        layers.40.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]      layers.40.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 369/ 723]     layers.40.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]     layers.40.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]     layers.40.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 723]            layers.40.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]        layers.41.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]        layers.41.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]        layers.41.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]        layers.41.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]      layers.41.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 378/ 723]     layers.41.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]     layers.41.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]     layers.41.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 723]            layers.41.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]        layers.42.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]        layers.42.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]        layers.42.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]        layers.42.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]      layers.42.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 387/ 723]     layers.42.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]     layers.42.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]     layers.42.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 723]            layers.42.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]        layers.43.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]        layers.43.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]        layers.43.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]        layers.43.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]      layers.43.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 396/ 723]     layers.43.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]     layers.43.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]     layers.43.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 723]            layers.43.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]        layers.44.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]        layers.44.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]        layers.44.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]        layers.44.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]      layers.44.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 405/ 723]     layers.44.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]     layers.44.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]     layers.44.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 723]            layers.44.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]        layers.45.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]        layers.45.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]        layers.45.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]        layers.45.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]      layers.45.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 414/ 723]     layers.45.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]     layers.45.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]     layers.45.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 723]            layers.45.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]        layers.46.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]        layers.46.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]        layers.46.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]        layers.46.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]      layers.46.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 423/ 723]     layers.46.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]     layers.46.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]     layers.46.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 723]            layers.46.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]        layers.47.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]        layers.47.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]        layers.47.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]        layers.47.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]      layers.47.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 432/ 723]     layers.47.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]     layers.47.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]     layers.47.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 723]            layers.47.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]        layers.48.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]        layers.48.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]        layers.48.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]        layers.48.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]      layers.48.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 441/ 723]     layers.48.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]     layers.48.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]     layers.48.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 723]            layers.48.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]        layers.49.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]        layers.49.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]        layers.49.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]        layers.49.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]      layers.49.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 450/ 723]     layers.49.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]     layers.49.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]     layers.49.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 723]            layers.49.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]        layers.50.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]        layers.50.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]        layers.50.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]        layers.50.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]      layers.50.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 459/ 723]     layers.50.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]     layers.50.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]     layers.50.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 723]            layers.50.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]        layers.51.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]        layers.51.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]        layers.51.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]        layers.51.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]      layers.51.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 468/ 723]     layers.51.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]     layers.51.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]     layers.51.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 723]            layers.51.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]        layers.52.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]        layers.52.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]        layers.52.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]        layers.52.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]      layers.52.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 477/ 723]     layers.52.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]     layers.52.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]     layers.52.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 723]            layers.52.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]        layers.53.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]        layers.53.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]        layers.53.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]        layers.53.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]      layers.53.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 486/ 723]     layers.53.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]     layers.53.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]     layers.53.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 723]            layers.53.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]        layers.54.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]        layers.54.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]        layers.54.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]        layers.54.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]      layers.54.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 495/ 723]     layers.54.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]     layers.54.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]     layers.54.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 723]            layers.54.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]        layers.55.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]        layers.55.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]        layers.55.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]        layers.55.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]      layers.55.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 504/ 723]     layers.55.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]     layers.55.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]     layers.55.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 723]            layers.55.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]        layers.56.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]        layers.56.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]        layers.56.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]        layers.56.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]      layers.56.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 513/ 723]     layers.56.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]     layers.56.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]     layers.56.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 723]            layers.56.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]        layers.57.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]        layers.57.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]        layers.57.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]        layers.57.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]      layers.57.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 522/ 723]     layers.57.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]     layers.57.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]     layers.57.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 723]            layers.57.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]        layers.58.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]        layers.58.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]        layers.58.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]        layers.58.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]      layers.58.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 531/ 723]     layers.58.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]     layers.58.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]     layers.58.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 723]            layers.58.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]        layers.59.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]        layers.59.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]        layers.59.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]        layers.59.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]      layers.59.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 540/ 723]     layers.59.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]     layers.59.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]     layers.59.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 543/ 723]            layers.59.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]        layers.60.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]        layers.60.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]        layers.60.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]        layers.60.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]      layers.60.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 549/ 723]     layers.60.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]     layers.60.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]     layers.60.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 552/ 723]            layers.60.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]        layers.61.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]        layers.61.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]        layers.61.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]        layers.61.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]      layers.61.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 558/ 723]     layers.61.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]     layers.61.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]     layers.61.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 561/ 723]            layers.61.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]        layers.62.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]        layers.62.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]        layers.62.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]        layers.62.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]      layers.62.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 567/ 723]     layers.62.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]     layers.62.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]     layers.62.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 570/ 723]            layers.62.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]        layers.63.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]        layers.63.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]        layers.63.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]        layers.63.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]      layers.63.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 576/ 723]     layers.63.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]     layers.63.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]     layers.63.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 579/ 723]            layers.63.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]        layers.64.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]        layers.64.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]        layers.64.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]        layers.64.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]      layers.64.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 585/ 723]     layers.64.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]     layers.64.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]     layers.64.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 588/ 723]            layers.64.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]        layers.65.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]        layers.65.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]        layers.65.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]        layers.65.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]      layers.65.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 594/ 723]     layers.65.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]     layers.65.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]     layers.65.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 597/ 723]            layers.65.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]        layers.66.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]        layers.66.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]        layers.66.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]        layers.66.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]      layers.66.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 603/ 723]     layers.66.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]     layers.66.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]     layers.66.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 606/ 723]            layers.66.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]        layers.67.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]        layers.67.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]        layers.67.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]        layers.67.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]      layers.67.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 612/ 723]     layers.67.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]     layers.67.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]     layers.67.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 615/ 723]            layers.67.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]        layers.68.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]        layers.68.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]        layers.68.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]        layers.68.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]      layers.68.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 621/ 723]     layers.68.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]     layers.68.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]     layers.68.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 624/ 723]            layers.68.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]        layers.69.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]        layers.69.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]        layers.69.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]        layers.69.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]      layers.69.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 630/ 723]     layers.69.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]     layers.69.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]     layers.69.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 633/ 723]            layers.69.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]        layers.70.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]        layers.70.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]        layers.70.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]        layers.70.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]      layers.70.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 639/ 723]     layers.70.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]     layers.70.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]     layers.70.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 642/ 723]            layers.70.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]        layers.71.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]        layers.71.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]        layers.71.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]        layers.71.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]      layers.71.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 648/ 723]     layers.71.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]     layers.71.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]     layers.71.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 651/ 723]            layers.71.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]        layers.72.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]        layers.72.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]        layers.72.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]        layers.72.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]      layers.72.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 657/ 723]     layers.72.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]     layers.72.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]     layers.72.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 660/ 723]            layers.72.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]        layers.73.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]        layers.73.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]        layers.73.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]        layers.73.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]      layers.73.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 666/ 723]     layers.73.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]     layers.73.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]     layers.73.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 669/ 723]            layers.73.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]        layers.74.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]        layers.74.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]        layers.74.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]        layers.74.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]      layers.74.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 675/ 723]     layers.74.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]     layers.74.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]     layers.74.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 678/ 723]            layers.74.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]        layers.75.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]        layers.75.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]        layers.75.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]        layers.75.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]      layers.75.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 684/ 723]     layers.75.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]     layers.75.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]     layers.75.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 687/ 723]            layers.75.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]        layers.76.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]        layers.76.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]        layers.76.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]        layers.76.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]      layers.76.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 693/ 723]     layers.76.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 694/ 723]     layers.76.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 695/ 723]     layers.76.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 696/ 723]            layers.76.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]        layers.77.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]        layers.77.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]        layers.77.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]        layers.77.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]      layers.77.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 702/ 723]     layers.77.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 703/ 723]     layers.77.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 704/ 723]     layers.77.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 705/ 723]            layers.77.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]        layers.78.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]        layers.78.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]        layers.78.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]        layers.78.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]      layers.78.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 711/ 723]     layers.78.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 712/ 723]     layers.78.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 713/ 723]     layers.78.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 714/ 723]            layers.78.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]        layers.79.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]        layers.79.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]        layers.79.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]        layers.79.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]      layers.79.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 720/ 723]     layers.79.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 721/ 723]     layers.79.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 722/ 723]     layers.79.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 723/ 723]            layers.79.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 272188.56 ms\n",
      "main:    total time = 272188.56 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.bin ./models/13B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.bin ./models/30B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.bin ./models/65B/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689654910\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find a purpose in your life that has an impact on others.\n",
      "If you are doing something that does not make you happy, then it’s time for a change. You have to do what makes you feel fulfilled and content. The reason why we are here is to live our lives the way we want to. We have only one shot at this life so we should make the most of it. I believe that every person has an opportunity to achieve their dreams if they put their heart into it.\n",
      "I think people should try and learn something new each day as well as be kind, honest and considerate to those around them. I am a firm believer in treating others the way you would like to be treated yourself.\n",
      "To me, success is when my hard work pays off. I believe that if you work hard enough then your dreams will come true.\n",
      "If there’s something I can do for you, I will try and do it. If I have a problem with you, please bring it up to me so we can discuss it as opposed to taking it out on others.\n",
      "I am very passionate about my job, and I want to make a difference in the lives of those who come into contact with me. It is my mission to provide exceptional service at all times.\n",
      "My vision for the future is to become an extremely well-known businessman, as well as help inspire others to be successful too.\n",
      "I will always look at ways I can improve and better myself in life so that I am able to achieve whatever it is I set my mind to do.\n",
      "I think that if you are passionate about something then you should always go for it no matter what people say, because the only person who knows what you’re capable of is yourself.\n",
      "A piece of advice I would like to give to others is to never let anyone stop you from achieving your dreams. If you want to become a successful businessman, then believe in yourself and work hard. You will be able to achieve anything as long as you don’t give up.\n",
      "I also believe that we should never judge people without getting to know them first. I think it is very important for us to treat others the way we would like to be treated ourselves because everyone deserves a chance.\n",
      "Another piece of advice I’d like to give is about respecting and being kind to animals, because they are helpless and can’t help themselves. This is\n",
      "llama_print_timings:        load time =  3710.00 ms\n",
      "llama_print_timings:      sample time =   283.98 ms /   512 runs   (    0.55 ms per token,  1802.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4786.50 ms /   265 tokens (   18.06 ms per token,    55.36 tokens per second)\n",
      "llama_print_timings:        eval time = 17116.61 ms /   510 runs   (   33.56 ms per token,    29.80 tokens per second)\n",
      "llama_print_timings:       total time = 22281.26 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7a0912-2746-4e7d-adaf-92b60cb2ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689654953\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it with as much love, hope, and faith in yourself.\n",
      "I am not going to lie, my life has been hard for me. I’ve had a lot of heartbreaks and struggles, but what makes me different from all the people out there who are struggling, is that I keep pushing no matter how tough things get. I have so much love in my heart for everything that happens to me because it makes me the person I am today.\n",
      "I think it’s important to share with everyone that you should never give up on your dreams or yourself. It’s okay to make mistakes, and it’s also okay if other people do not get it. The only way to live is by staying true to yourself at all times.\n",
      "Being a musician is my life, so I am going to share with you some of the things that I have learned from being in this business for 17 years now. It’s been a long journey and I could not be happier about it. If I did not stick to my dreams I would never know what I was capable of.\n",
      "The most important thing is to stay focused on your vision at all times. You should always have this in the back of your mind, because that’s the only way you will find success and happiness in whatever it is you do.\n",
      "When I was 17, my world came crashing down when my mother had a stroke. It changed me forever and I did not know how to cope with all of this until later on. That’s what made me focus more on becoming the best musician I could be. It also helped me see life differently, which is why I am still here today.\n",
      "My first record deal was with Sony Music. The album was called “Unconditional” and it got a lot of attention from many people. After that I was signed to 50 Cent’s G-Unit Records for 2 albums. After that I did some music with Lil Wayne, Rick Ross, and more. In total, I have been nominated for 39 awards in my career as an artist and producer.\n",
      "The most amazing thing about my life is the fact that it has helped me become a better person who never gives up on their dreams no matter how tough things get. It’s important to always stay focused on your vision, because that’s what will help you get through anything in life and make sure things\n",
      "llama_print_timings:        load time =  1189.42 ms\n",
      "llama_print_timings:      sample time =   282.48 ms /   512 runs   (    0.55 ms per token,  1812.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4869.65 ms /   265 tokens (   18.38 ms per token,    54.42 tokens per second)\n",
      "llama_print_timings:        eval time = 17035.11 ms /   510 runs   (   33.40 ms per token,    29.94 tokens per second)\n",
      "llama_print_timings:       total time = 22279.65 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3324a21-7a43-4c68-83eb-52417e487c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689654980\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it, and be happy.\n",
      "I believe that being alone with your thoughts can be a beautiful thing. It shows how intelligent you are and how much you know about yourself.\n",
      "I believe in the power of prayer. And I also believe in myself enough to know God always has my back. Even when things go wrong, he's got my best interest at heart.\n",
      "I believe that people are more important than material goods; it really doesn't matter what you have, so long as you have good people around you.\n",
      "I believe everyone deserves a second chance, even if they take that chance and fuck everything up again.\n",
      "I believe in the power of love. It can change lives, make people better, make you see things differently. And it's the kind of thing that makes the world go round.\n",
      "I think the meaning of life is to live it, and be happy with yourself.\n",
      "I don't know if there is a purpose for us all in this world, but I believe we are here to help each other and enjoy our lives as much as possible.\n",
      "What do you really want out of your life? Do you want to travel the world? To have a family or live on your own? What career path do you see yourself taking? Is it what you have been told to want, or is it something that you've always dreamed of doing?\n",
      "I believe in the power of love. It can change lives, make people better, and see things differently. And it's the kind of thing that makes the world go round.\n",
      "Don't try to hide who you are, live your life and be happy with yourself. Everyone has faults; no one is perfect. So don't beat yourself up over them. You just have to find a way to make them work for you in some way.\n",
      "I believe that everyone deserves a second chance, even if they take that chance and fuck everything up again. People can be unpredictable, but it doesn't mean you should give up on them.\n",
      "Don't ever forget who your friends are; no matter how far apart you are or what happens in life, they will always be there for you when you need them the most. They might even surprise you with a visit one day and make you feel like nothing has changed at all.\n",
      "I think love is something that makes the world go round, it's a reason to live. It can be\n",
      "llama_print_timings:        load time =  1213.54 ms\n",
      "llama_print_timings:      sample time =   289.93 ms /   512 runs   (    0.57 ms per token,  1765.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4789.08 ms /   265 tokens (   18.07 ms per token,    55.33 tokens per second)\n",
      "llama_print_timings:        eval time = 17042.90 ms /   510 runs   (   33.42 ms per token,    29.92 tokens per second)\n",
      "llama_print_timings:       total time = 22214.00 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98a39e0-c339-4234-b6d2-49d642ffb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655008\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love someone or something so much that you're willing to die for them.\n",
      "I think it is an important thing about human nature: If we have no choice, we can't be brave.\n",
      "People say nothing is impossible, but I do nothing every day.\n",
      "You can get away with murder if you say you’ve done it in the name of God.\n",
      "There are two ways through life: the way of Nature and the way of Grace. You have to choose which one you'll follow.\n",
      "It was love at first sight. The first time he saw her, he said, \"She's beautiful.\" The second time, he said, \"She's cute. And I like her because she smiles a lot.\"\n",
      "I don't believe in miracles - but I have seen the odd coincidence happen.\n",
      "If you can't change your fate don't complain about it.\n",
      "We must accept finite disappointment, but never lose infinite hope.\n",
      "You think that's wise? Well here's another bit of wisdom for you: Don't try to be a hero. Because sometimes even a genius can get it wrong.\n",
      "I have so much love for you, it could heal all the sick and wounded people in this world.\n",
      "Life isn't about finding yourself. It's about creating yourself.\n",
      "In order for us to see light we must first endure darkness.\n",
      "Don’t be afraid of death; be afraid of an unlived life. You don’t have to live forever, you just have to live.\n",
      "If you believe in your dreams then they might just come true.\n",
      "Never lose hope if what you really want is love and romance because someday it will happen.\n",
      "The only time we can be certain of having enough time is when we're no longer alive.\n",
      "Don’t try to be a hero, because sometimes even a genius can get it wrong.\n",
      "If you want to know who has your heart, look at what broke his.\n",
      "I think people should love who they want to love. It's stupid that some people deny others their happiness just because of jealousy or whatever reason.\n",
      "We are never too old for love and we are never too young. So why wait? We owe it to ourselves.\n",
      "A person's life purpose is found in one word: others.\n",
      "When it comes down to it,\n",
      "llama_print_timings:        load time =  6553.00 ms\n",
      "llama_print_timings:      sample time =   291.40 ms /   512 runs   (    0.57 ms per token,  1757.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6120.85 ms /   265 tokens (   23.10 ms per token,    43.29 tokens per second)\n",
      "llama_print_timings:        eval time = 21801.74 ms /   510 runs   (   42.75 ms per token,    23.39 tokens per second)\n",
      "llama_print_timings:       total time = 28307.36 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0296e978-7c63-4d87-a1a9-dc41b49dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655047\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to become all that we are capable of being.\n",
      "—\n",
      "JOHN WAYNE, 1974\n",
      "# **_One_**\n",
      "The last thing she did was to put on her bra. She took a deep breath and held it in her lungs, then slowly let the air out again, feeling the pressure from all sides. Her hands were shaking as she undid the clasp, but she didn't care because the bra was so old that even she barely recognized it anymore. It had been bought for her by her mother when they went shopping in London after she won the Commonwealth Games. She was fifteen years old then and still a virgin, so the underwear had never really served any purpose other than to make her feel sexy. She pulled the straps over her shoulders, put the cups around her breasts, and fastened them up tight. Her nipples grew hard in response to the pressure.\n",
      "She looked down at herself, noticing how much weight she'd lost since last year. It was only a few kilos, but it meant that a lot of her clothes didn't fit anymore, especially her bras and underwear. She liked feeling the pressure against her skin as she fastened them up tight.\n",
      "The bra felt good on her body. It made her feel sexy and desirable. It reminded her of the last time she had worn it: at the Commonwealth Games closing ceremony in Melbourne, when she was fifteen years old, with a whole new life ahead of her, feeling invincible as she received a gold medal from the Queen herself.\n",
      "That's where all my problems started. From then on I never felt like that again. It was almost too much to take—all that love and adoration. I didn't know how to handle it. I just wanted to be a normal girl, with no one looking at me differently because of the way I ran or what I wore.\n",
      "But I let people think they knew who I was; even my own family were so proud of me that they couldn't accept anything else. They wanted me to be perfect and strong for ever—but life isn't like that, is it?\n",
      "I had nothing to prove anymore, but I still kept running for a while. That was all right too because at least then I felt like the normal person again. I didn't have to pretend any longer. My family were\n",
      "llama_print_timings:        load time =  1840.17 ms\n",
      "llama_print_timings:      sample time =   286.19 ms /   512 runs   (    0.56 ms per token,  1789.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6162.65 ms /   265 tokens (   23.26 ms per token,    43.00 tokens per second)\n",
      "llama_print_timings:        eval time = 21942.04 ms /   510 runs   (   43.02 ms per token,    23.24 tokens per second)\n",
      "llama_print_timings:       total time = 28483.82 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebca2498-774d-4f51-ac56-a9da8f359523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655082\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living, not in some abstract concept.\n",
      "The meaning of life for me is to live a life that is full and rich, one where you can find happiness. A life where we have the freedom to express ourselves and our emotions. A life where we can express our love for others without fear or constraint. A life filled with joyful experiences.\n",
      "This blog is dedicated to those seeking answers on what it means to live a meaningful life, and how to go about doing that. It’s a journey I am exploring myself, so this blog will be as much an account of my own search for answers, as it will be a guide to living a life fuller and richer than ever before.\n",
      "I started this blog in 2012 after reading The Purpose Driven Life by Rick Warren. I read his book every year to remind myself what’s important in life: family, friends, career and serving others. In the past, I have worked for a not-for profit organization that is now known as Enable India (http://www.enableindia.org). They provide education and employment opportunities to persons with disabilities.\n",
      "My name is Aarushi Mudholkar; I am 32 years old and live in Bangalore, India. I believe life is about love for others and for ourselves, a journey of self discovery and an opportunity to express yourself. It’s also about trying your best at everything you do and not beating yourself up if you don’t succeed the first time around.\n",
      "I am a software professional by profession and a blogger by passion. I currently work as a senior developer with SAP Labs India in Bangalore, India. I am a strong believer in Karma and believe that good always triumphs over evil. In my free time you will find me reading books, writing poetry and short stories or playing with my 2 year old daughter.\n",
      "I hope this blog helps you find what it means to live a meaningful life, just as I am finding out for myself!\n",
      "I look forward to your comments and suggestions.\n",
      "Aarushi Mudholkar\n",
      "Pingback: What’s the Meaning of Life? | Aarushi's Blog\n",
      "You are a great blogger. Keep up the good work\n",
      "Wish you all success in whatever you do in life! ���\n",
      "Thank u\n",
      "llama_print_timings:        load time =  1803.30 ms\n",
      "llama_print_timings:      sample time =   299.30 ms /   512 runs   (    0.58 ms per token,  1710.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6127.54 ms /   265 tokens (   23.12 ms per token,    43.25 tokens per second)\n",
      "llama_print_timings:        eval time = 21876.67 ms /   510 runs   (   42.90 ms per token,    23.31 tokens per second)\n",
      "llama_print_timings:       total time = 28396.88 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d246e2-1ed4-46f7-aa3e-0d4787cc3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655116\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find\n",
      "your gift is the meaning\n",
      "In a very real sense, people who have the ability to\n",
      "influence by virtue of their position\n",
      "live in a kind of dictatorship.\n",
      "And as you said so well, most leaders -\n",
      "Most, not all, but most, fall prey\n",
      "to the arrogance of office, and they lose contact.\n",
      "What we did is we turned to each other.\n",
      "That was an incredible inspiration to me.\n",
      "Helen Keller, overcoming a seemingly insurmountable obstacle,\n",
      "became a spokesperson for perseverance and seeing past limitations.\n",
      "I don't see why we need to stand by and watch a country go right down the drain.\n",
      "It is time for us to speak up on their behalf.\n",
      "The most important thing in communication\n",
      "is hearing what isn't said.\n",
      "You never really understand a person until you consider things from his point of view... Until you climb inside of his skin and walk around in it.\n",
      "If I could say a few words, I would say:\n",
      "I have loved the stars too fondly to be fearful of the night.\n",
      "I'm always looking for meaning.\n",
      "You know how most illnesses are - there's no real treatment for them.\n",
      "But at least if you talk about it...\n",
      "if you acknowledge that you have an illness,\n",
      "then you can have some hope of dealing with it.\n",
      "There is no king who has not had a slave among his ancestors,\n",
      "and no slave who has not had kings among his.\n",
      "You cannot get through a single day without having\n",
      "an impact on the world around you.\n",
      "What you do makes a difference, and you have to decide what kind of difference you want to make.\n",
      "If you are always trying to be normal, you will never know how amazing you can be.\n",
      "The only reason I'm talking to you is because I'm in an airplane!\n",
      "Thank you for visiting: If we are to better understand human needs by Andrew Young from life Quotes and Sayings.\n",
      "If it's green, it's biology. If it stinks, it's chemistry. If it has numbers, it's math. If it doesn't work, it's technology.\n",
      "Helen Keller on Optimism\n",
      "I never\n",
      "llama_print_timings:        load time =  8781.35 ms\n",
      "llama_print_timings:      sample time =   287.59 ms /   512 runs   (    0.56 ms per token,  1780.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9437.14 ms /   265 tokens (   35.61 ms per token,    28.08 tokens per second)\n",
      "llama_print_timings:        eval time = 34586.70 ms /   510 runs   (   67.82 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time = 44405.80 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce27ab44-b446-4126-8972-7472c658a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655174\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give meaning to life.\n",
      "I am an American, born and raised in a small town in New Jersey, where I spent my childhood playing with friends, riding bikes, reading, writing poetry, listening to music and dreaming of faraway places.\n",
      "My parents are both first-generation Americans whose families immigrated from Russia and Poland just before World War Two.\n",
      "My father was a doctor and my mother stayed home to raise my sister and me. She taught us about the importance of family. She taught us about unconditional love. And she taught us about the value of hard work, commitment, and dedication. All lessons that I have carried with me throughout life.\n",
      "I always knew I wanted to be a doctor too, but I also knew I loved writing so much that I wanted to pursue it as well. In high school I wrote for our school newspaper and the local paper. I was editor of my yearbook in college. I wrote poetry and short stories on the side.\n",
      "I went to medical school at Columbia University in New York City, where I also completed a residency in internal medicine. After that, I moved to San Francisco and completed a fellowship in infectious diseases. When I finished training, I worked as an infectious disease specialist while pursuing my writing on the side.\n",
      "Over time, my writing took center stage in my life.\n",
      "In 1985, I quit medicine to focus entirely on my writing and publishing career. I’ve written over a dozen books since then: novels, nonfiction books, children’s books, poetry collections, short story anthologies, and textbooks. And I’ve edited dozens of books as well.\n",
      "I am also the founder and publisher of Story Circle Press, whose mission is to empower women’s voices through writing, publishing, and promotion. We are a community of writers who inspire each other to tell our stories and to share our words with others.\n",
      "In addition to books, I have written articles for national magazines and newspapers, and I am now a regular contributor to HuffPost.\n",
      "My latest book is The Joy Diet: 10 Daily Practices for a Happier Life. This book is my love letter to the world. It contains all I know about how we can live our best lives—with joy, gratitude, purpose, and presence.\n",
      "I have written non\n",
      "llama_print_timings:        load time =  3498.42 ms\n",
      "llama_print_timings:      sample time =   287.44 ms /   512 runs   (    0.56 ms per token,  1781.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9460.73 ms /   265 tokens (   35.70 ms per token,    28.01 tokens per second)\n",
      "llama_print_timings:        eval time = 34374.61 ms /   510 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
      "llama_print_timings:       total time = 44216.60 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eec90d1-7b7b-4071-af17-849bcb505108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655226\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a life full of purpose, love and passionate.\n",
      "I am very friendly and easy going person.\n",
      "I don't judge anyone and love to meet new people and make friends.\n",
      "Someone who is down to earth, has similar values as me and believes in living a healthy lifestyle and working on themselves. Someone that I can enjoy life with.\n",
      "Ana_721 hasn't asked any friends to write a recommendation yet.\n",
      "Ana_721 has not yet answered any of the optional questions that would tell you more about her.\n",
      "Ana_721 has not yet connected with any of their friends on Workout Trainer, but here are a few of her interests.\n",
      "Sports: Running, Skiing, Snowboarding, Volleyball, Yoga.\n",
      "Music: Alternative Rock, Indie, Pop, Reggae, R&B/Urban Soul, Soft Rock.\n",
      "TV Shows: Documentaries, Reality TV.\n",
      "Books: Biographies & Autobiographies, Health, Mind & Body, Non-Fiction.\n",
      "Ana_721 has not yet posted any media.\n",
      "Workout plan for weight loss and muscle gain?\n",
      "Is there a workout plan that will help me lose weight and build some lean muscle at the same time? I have been trying to do cardio exercise everyday (which is mainly just walking), but haven't really seen results since i tend to eat out quite often. I would like some suggestions on how can i change my eating habits, so that i will be able to lose weight more effectively and also build some muscle at the same time.\n",
      "Ana_721 has not yet posted any videos.\n",
      "Ana_721 has not received any message yet.\n",
      "Ana_721 has not uploaded any photo yet.\n",
      "Hello Ana! Please can I ask for your advice on how to lose weight?\n",
      "I'm a bit chubby and I want to slim down.\n",
      "Thanks for accepting my friend request! I hope you have a great weekend!!!\n",
      "Hi there...thanks for the add, wishing you all the best!!\n",
      "Hello Ana, thanks so much for accepting my invitation, it is very nice to meet you here in FitClick!\n",
      "Ana_721 does not have any current challenges.\n",
      "\n",
      "llama_print_timings:        load time =  3456.13 ms\n",
      "llama_print_timings:      sample time =   290.12 ms /   512 runs   (    0.57 ms per token,  1764.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9449.85 ms /   265 tokens (   35.66 ms per token,    28.04 tokens per second)\n",
      "llama_print_timings:        eval time = 34348.07 ms /   510 runs   (   67.35 ms per token,    14.85 tokens per second)\n",
      "llama_print_timings:       total time = 44182.07 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04719be2-a272-4fc9-aed8-193e4bfe9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655279\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and in order for me to be happy, I need to love myself.\n",
      "I have spent most of my life in search of happiness and trying to please others. I was a people pleaser. I put everyone else’s needs before my own, thinking that by doing so, it would make them like and appreciate me more. This led me down the road of self-destruction where I experienced depression, anxiety, and suicidal thoughts for many years.\n",
      "I have always been a very spiritual person, believing in karma, universal laws, miracles, and God. I was raised a Christian but never really followed that path. It didn’t feel right to me. When I started experiencing all of these negative emotions, I knew I had to find out why. This is when I began searching for answers through books, documentaries, movies, and seminars.\n",
      "I learned about the law of attraction and practiced it daily. I also became a certified life coach specializing in the law of attraction and started my own business helping others to apply the universal laws into their everyday lives. After doing this for several years, I felt unfulfilled because I was not really being the best version of myself, so I continued searching for answers through different books, documentaries, movies, and seminars.\n",
      "I then discovered Buddhism and meditation and became a student of Zen Master Thich Nhat Hanh. This is when I learned how to be present in my life by practicing mindfulness in everything that I do. By being present, I was able to let go of the past and worrying about the future.\n",
      "I began experiencing so much happiness because I started loving myself again! I no longer had negative thoughts entering my mind. Instead, I focused on positive affirmations, which led me to learn more about the power of the subconscious mind through Dr. Joseph Murphy’s book, The Power Of Your Subconscious Mind.\n",
      "Through this book, I learned that by changing our thoughts, we can change our life. In order for us to change our thoughts though, we have to be aware of them in the first place and then reprogram our mind to think positively. We also need to release all of the negative emotions such as guilt, fear, worry, doubt, frustration, anger, resentment, etc., that are blocking us from\n",
      "llama_print_timings:        load time =  6427.07 ms\n",
      "llama_print_timings:      sample time =   286.39 ms /   512 runs   (    0.56 ms per token,  1787.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13147.50 ms /   265 tokens (   49.61 ms per token,    20.16 tokens per second)\n",
      "llama_print_timings:        eval time = 49036.70 ms /   510 runs   (   96.15 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time = 62564.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1730091a-3146-476d-ab57-49f8722c351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655354\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "— Pablo Picasso (1881-1973) Spanish painter, sculptor and printmaker\n",
      "Tagged with: Pablo Picasso\n",
      "Previous PostI’m not here for a long time, I’m here for a good time.\n",
      "Next PostIf you are neutral in situations of injustice, you have chosen the side of the oppressor. If an elephant has its foot on the tail of a mouse and you say that you are neutral, the mouse will not appreciate your neutrality.\n",
      "I’m not here for a long time, I’m here for a good time.\n",
      "Everything is theoretically impossible until it is done. One could write a history of science in reverse by assembling the solemn pronouncements of highest authority about what would and would not work.\n",
      "— Robert A. Heinlein (1907-1988) American writer\n",
      "Never believe anything that can’t be looked up.\n",
      "Never stop learning; knowledge doubles every fourteen months.\n",
      "The less a man knows, the more he thinks he knows.\n",
      "When you are courting a nice girl an hour seems like a second. When you sit on a red-hot cinder a second seems like an hour. That’s relativity.\n",
      "You ain’t learnin’ nothin’ when you’re talkin’.\n",
      "The whole problem with the world is that fools and fanatics are always so certain of themselves, but wiser people so full of doubts.\n",
      "Never confuse “duty” with what other people expect of you; they are utterly different. Duty is a debt you owe to yourself to fulfill obligations you have assumed voluntarily.\n",
      "— Lazarus Long (Heinlein character)\n",
      "Love is that condition in which the happiness of another person is essential to your own\n",
      "The slickest way in the world to lie is to tell the right amount of truth at the right time — and then shut up.\n",
      "Tagged with: Robert A. Heinlein\n",
      "Posted in: Quotes, Science & Technology\n",
      "Previous Post: I’m not here for a long time, I’m here for a good time.\n",
      "Next Post: If you are neutral in situations of injustice, you have chosen the side of the oppressor\n",
      "llama_print_timings:        load time =  6192.22 ms\n",
      "llama_print_timings:      sample time =   291.39 ms /   512 runs   (    0.57 ms per token,  1757.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13033.99 ms /   265 tokens (   49.18 ms per token,    20.33 tokens per second)\n",
      "llama_print_timings:        eval time = 49010.31 ms /   510 runs   (   96.10 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time = 62429.24 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351608bd-64ae-4f26-9fb5-ccf3ee5fb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655427\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "I believe that we can be whatever we want to be if only we believe in ourselves and take action.\n",
      "I believe there is a God, but that he/she/it has far better things to do than worry about each one of us individually.\n",
      "I believe that the greatest gift we receive from our parents is the way they loved (or didn’t love) us.\n",
      "I believe that you can be happy right now – this very moment if you choose to be.\n",
      "I believe that life is a gift and not a given right.\n",
      "I believe in reincarnation, but I do NOT believe we come back as animals or trees. We only get one shot at being human. Make it count!\n",
      "I believe that the purpose of relationships is for us to grow. Relationships are here to push our buttons so we can learn more about ourselves and become better humans.\n",
      "I believe that the purpose of life is to love and be loved, but never to the detriment of loving yourself first.\n",
      "I believe that everything happens for a reason, even if we don’t understand the reason at the time.\n",
      "I believe in Karma – what goes around comes around.\n",
      "I believe all religions have value if you are looking for spiritual growth and enlightenment. I do not believe any religion has more value than another.\n",
      "I believe that spirituality and science can coexist.\n",
      "I believe that we create our own reality through the Law of Attraction.\n",
      "I believe that when we focus on lack, we attract more experiences of lack into our lives.\n",
      "I believe in a good education but not necessarily university or college. I am a firm believer in life-long learning and following your passion – wherever it may lead you.\n",
      "I believe that what you think about most of the time is exactly what you will manifest. Choose your thoughts wisely!\n",
      "I believe that if we want to change something in our lives, we must be willing to take action on a daily basis towards making that change happen.\n",
      "I believe that you have to believe in yourself before anyone else can or will. Don’t rely on others for your self-worth.\n",
      "I believe the greatest gift you can give your children is showing them how to love and respect themselves. Everything else they need to know, they will learn by watching you.\n",
      "I believe we are here to make a difference in this world and be of service to our fellow man/\n",
      "llama_print_timings:        load time =  6259.49 ms\n",
      "llama_print_timings:      sample time =   282.86 ms /   512 runs   (    0.55 ms per token,  1810.07 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12965.83 ms /   265 tokens (   48.93 ms per token,    20.44 tokens per second)\n",
      "llama_print_timings:        eval time = 48884.24 ms /   510 runs   (   95.85 ms per token,    10.43 tokens per second)\n",
      "llama_print_timings:       total time = 62226.73 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34085-ceab-48a8-96cb-f47d3f8cc5b3",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303e2727-09d0-47be-818f-2a6519aa5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655501\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I have a passion for people and helping them achieve their goals, whatever they may be!\n",
      "I am happiest when surrounded by my family or close friends. I enjoy camping, swimming, cooking, baking, reading and most importantly being involved in my community.\n",
      "The biggest lesson I’ve learned in life is to trust your gut feeling. If something doesn’t feel right don’t do it!\n",
      "I am a very social person so my favourite part of the day is in the evening sitting down with my family, chatting and sharing stories of our days adventures.\n",
      "My most memorable holiday was travelling to Vietnam as a teenager. It was such an amazing experience, I have lots of fond memories from that trip!\n",
      "I am very proud of having raised two incredible young men who are now both successful in their chosen careers.\n",
      "The best piece of advice I’ve ever received is “be the person you would want your children to be”.\n",
      "If I could give my younger self some advice it would be to relax and enjoy life!\n",
      "My favourite quote is “Life is short, break the Rules Forgive quickly Love truly Laugh uncontrollably and never regret anything that made you smile. Twenty Years from now you will be more disappointed by the things you didn’t do than by the ones you did do”. – Mark Twain\n",
      "I love to read so it is hard to pick a favourite book but my all time favourite is The Alchemist by Paulo Coelho! It is such a beautiful story. I find it very inspiring and motivating for many aspects of life!\n",
      "My most unusual talent would be that I can make a mean batch of vegemite sandwiches!\n",
      "I am incredibly passionate about Real Estate, being in the industry for 12 years now has only reinforced my love for the business.\n",
      "I have worked with some amazing people during this time and continue to learn something new every day from each one of them! Being able to share my knowledge and experience with others is very rewarding!\n",
      "My biggest achievement in life so far has been raising two wonderful young men, I know it’s cliche but it truly was the best moment of my life!\n",
      "I have a passion for travel, I would love to visit Antarctica one day.\n",
      "Favourite place on Earth\n",
      "llama_print_timings:        load time = 11691.42 ms\n",
      "llama_print_timings:      sample time =   289.34 ms /   512 runs   (    0.57 ms per token,  1769.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4840.10 ms /   265 tokens (   18.26 ms per token,    54.75 tokens per second)\n",
      "llama_print_timings:        eval time = 17909.93 ms /   510 runs   (   35.12 ms per token,    28.48 tokens per second)\n",
      "llama_print_timings:       total time = 23133.43 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061c4fdf-de31-47b9-a6f7-55873c2b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655541\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to enjoy and make the best of what we have. I’m a realist that appreciates beauty, goodness and truth in all forms.\n",
      "I'm an atheist, but I also like science fiction (Star Trek), fantasy (Lord of The Rings) and superhero stories (Batman).\n",
      "I believe in evolution, both biological and social.\n",
      "I love the mountains, especially the ones that have a little snow on them.\n",
      "I'm a big fan of jazz, funk and blues music, as well as some hip hop and country. I also enjoy classical music.\n",
      "I like to eat good food with my friends at nice restaurants.\n",
      "I work hard when it’s necessary but I take time off every day to relax and have fun doing things I enjoy. I enjoy traveling, movies, hiking, skiing, camping, kayaking, fishing, hunting, cooking and eating good food with friends.\n",
      "I enjoy a great glass of wine or beer on occasion but I don't drink every day (any more). I also like a cup of tea or coffee in the morning when I’m ready to get going.\n",
      "I love my family: my wife, my kids and their spouses, grandkids and even the dogs!\n",
      "I enjoy helping people when they need it. I give money and time to worthwhile causes whenever I can.\n",
      "I believe that all human beings are important and deserving of respect (no matter where they live or who they worship).\n",
      "And that’s me in a nutshell. I hope you enjoyed getting to know me!\n",
      "Thanks for visiting our website. We'd love to meet with you and share information about our products, services and community involvement. Please call us at 970-328-4126 or fill out this form if we can be of any assistance to you.\n",
      "Thank you! We will get back to you as soon as possible. In the meantime, please feel free to explore our website to learn more about us and what we have to offer.\n",
      "All content copyright © 2018 Meadows Mortgage, Inc., 345 S. Hwy 7 in Steamboat Springs, CO 80487. NMLS #189468. Equal Housing\n",
      "llama_print_timings:        load time =  2308.40 ms\n",
      "llama_print_timings:      sample time =   291.77 ms /   512 runs   (    0.57 ms per token,  1754.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4791.04 ms /   265 tokens (   18.08 ms per token,    55.31 tokens per second)\n",
      "llama_print_timings:        eval time = 17877.08 ms /   510 runs   (   35.05 ms per token,    28.53 tokens per second)\n",
      "llama_print_timings:       total time = 23053.87 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb37cbe3-16ab-4f63-9903-2e7951ce751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655570\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find out why you are here and then do something about it.\n",
      "I hope that my books will be a catalyst for readers, whether young or old; to become more interested in history. We are all part of an exciting continuum of events. It’s important we keep our eyes on the future. To me, there is no greater purpose than making life better for others and leaving this world a little bit better than when you were born into it.\n",
      "I am passionate about writing historical fiction as I believe this is one way to bring history alive. As a writer I can not only entertain my readers but also inform them. I hope that by doing so, my work will encourage others to learn more about the past and how it has shaped our world today.\n",
      "My first book The Great Gatsby Abridged (March 2015) is set in the Roaring Twenties during Prohibition when gangsters, bootleggers and corrupt politicians were rife. I was inspired to write this as it is my favourite era of history.\n",
      "The Great Gatsby Abridged is a fast-paced thriller based on Scott Fitzgerald’s classic novel. The story follows the fortunes of Tom Buchanan, a corrupt politician who is having an affair with his secretary. When he discovers she is pregnant by another man he decides to murder her and her lover. One day while out boating Tom meets Jordan Baker and they become friends. However, when the two return home from a night out together, Tom finds himself attracted to Daisy Buchanan his wife’s sister.\n",
      "The Great Gatsby Abridged brings to life Fitzgerald’s most famous novel in an action-packed story that is both entertaining and informative. It will appeal to readers young and old who are interested in the Roaring Twenties and American history. The book has already been very well received by reviewers:\n",
      "‘Fitzgerald’s great masterpiece given a fresh look for modern audiences…an excellent choice for older teenagers or adults who want to re-read this classic of the 1920s.’\n",
      "Dawn Barker, author of Let Her Go.\n",
      "‘A wonderful fusion of a famous novel and the 1920’s with fast paced action and an engaging storyline…\n",
      "llama_print_timings:        load time =  2302.82 ms\n",
      "llama_print_timings:      sample time =   298.63 ms /   512 runs   (    0.58 ms per token,  1714.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4809.38 ms /   265 tokens (   18.15 ms per token,    55.10 tokens per second)\n",
      "llama_print_timings:        eval time = 17878.17 ms /   510 runs   (   35.06 ms per token,    28.53 tokens per second)\n",
      "llama_print_timings:       total time = 23080.36 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33cb1a63-f863-4a7f-b627-6c76e534f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655600\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living and loving.\n",
      "I think it is also important that you make your own happiness and find what makes you happy, whether it be working with people or animals, going out to a club or staying at home, eating healthy or indulging! It’s really up to you. I believe life should be lived to the fullest and we should enjoy every moment as it is precious!\n",
      "I am a 28 year old girl from Norway. When I am not working, I love spending time with my friends and family, going out or relaxing at home watching movies and having a glass of wine or two! I have been an au pair for more than 7 years now and I still find it to be the perfect job for me! I get to spend time with children and also live in another country which is something that I love.\n",
      "I love spending time with my friends/family, going out for a drink or eating great food. As mentioned above, I really enjoy travelling and discovering new places and cultures! I like doing sports such as going to the gym or running but I also do yoga a few times a week which helps me relax after a long day at work and also keeps me in shape!\n",
      "I have previously worked for 5 different families and I have always been an au pair for children from 0-13 years old. I have had my own room/bathroom during all of these positions with the exception of one where I was sleeping on the sofa.\n",
      "I am a girl who is easy to talk to, easygoing, kind and happy! I like to make sure everyone around me are having fun and I love spending time with children as they bring out my inner child :) As an au pair, you have to be able to communicate with the whole family in order for everyone to feel comfortable.\n",
      "I am a very experienced au pair who has previously worked for two families in Norway with 2 kids each (a boy of 1 year old and a girl of 4 years) and three families abroad with 3 children each (2 girls of ages 5,8 and 10). All families have been more than happy to recommend me!\n",
      "Most recently I worked for a family in Switzerland where I was looking after two lovely twin boys who are now 7. Before that I took care of three wonderful children in Germany: a 3 year old daughter\n",
      "llama_print_timings:        load time = 22187.27 ms\n",
      "llama_print_timings:      sample time =   288.48 ms /   512 runs   (    0.56 ms per token,  1774.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6093.20 ms /   265 tokens (   22.99 ms per token,    43.49 tokens per second)\n",
      "llama_print_timings:        eval time = 23346.59 ms /   510 runs   (   45.78 ms per token,    21.84 tokens per second)\n",
      "llama_print_timings:       total time = 29821.43 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250c511c-864d-4b20-8d87-86f2409b277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655657\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "My goal and mission in life is to help others live a happier, healthy lifestyle.\n",
      "I hope you enjoy this site where you can learn more about me and my journey to happiness as well as the tools I have discovered that can help you get to your version of happiness!\n",
      "Hi! My name is Katie. I am a 2018 graduate of University of Oregon, Bachelor’s Degree in Health Promotion & Social Behavior with a minor in Public Health.\n",
      "I started my blog here at katiekayhealth.com to help share the tools that have helped me live a happy and healthy lifestyle while learning along the way!\n",
      "I hope you find some inspiration, motivation, or information that can help you on your own journey!\n",
      "I have always known I wanted to help people live happier lives.\n",
      "After completing my undergraduate degree in Health Promotion & Social Behavior from University of Oregon, I have realized just how big a role our health and happiness plays in our overall well-being. From there, I am now studying to be a Registered Dietitian Nutritionist at California State University, Monterey Bay.\n",
      "I want to help people live happier lives by helping them become the best versions of themselves through nutritious food and healthy lifestyle habits!\n",
      "In addition to my love for all things related to health and happiness, I also love traveling (especially internationally), going to concerts, running, and trying new recipes. Check out some of my favorite places I’ve traveled in the travel section of this blog!\n",
      "If you have any questions or comments, feel free to contact me at katiekayhealth@gmail.com!\n",
      "You can follow along on my journey to becoming a Registered Dietitian Nutritionist by following me on Instagram @katiekayhealth or subscribing to my blog here!\n",
      "I want to help people live happier lives through eating nutritious food and living a healthy lifestyle. I hope you enjoy the recipes, tips, tricks, and information I share with you all!\n",
      "Here are some of my favorite places I have traveled so far in life!\n",
      "I love to travel and experience new cultures, people, and foods. If you have any recommendations on a\n",
      "llama_print_timings:        load time =  3902.59 ms\n",
      "llama_print_timings:      sample time =   291.22 ms /   512 runs   (    0.57 ms per token,  1758.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6132.06 ms /   265 tokens (   23.14 ms per token,    43.22 tokens per second)\n",
      "llama_print_timings:        eval time = 23473.75 ms /   510 runs   (   46.03 ms per token,    21.73 tokens per second)\n",
      "llama_print_timings:       total time = 29991.75 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f792b2-0f7e-4b93-9d3e-69d4479f6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655695\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m helping others and that’s what I strive to do every day.\n",
      "My name is Christina and I am a wife, mother, nurse, teacher, runner, yogi, and lover of animals. I was born in North Carolina, but have lived most of my adult life in Virginia. My husband and I currently reside in Richmond with our two children and four dogs.\n",
      "My journey began 10 years ago when I got my first dog, a black Lab named Gracie. After adopting Grace from the SPCA, it didn’t take long for me to realize how special she was and how much her life had been impacted by previous owners. She taught me so much about unconditional love and forgiveness, that I decided to get my Bachelors degree in Nursing at Virginia Commonwealth University so that I could become a nurse who would help others heal through their pets.\n",
      "Gracie also introduced me to yoga. Although I had been physically active my whole life, doing yoga was something different and very special for me. It helped me learn how to be calm in the midst of chaos; it helped me deal with anxiety; it gave me strength when I needed it most; and it taught me to look at things from a different perspective. When Gracie passed away, I felt lost without her and yoga was my way of connecting with her spirit.\n",
      "My life changed forever in 2014 after the birth of our second child, William. It was during this time that I became passionate about helping new mothers connect with their inner strength through nursing education and prenatal/postpartum yoga. After attending a training on Mindfulness-Based Stress Reduction (MBSR) at the University of Virginia, I wanted to find ways to incorporate MBSR into my teaching.\n",
      "I then came across a book about Yogababy and fell in love with the concept immediately. The author, Lara Hauser, was one of my yoga teachers from Richmond. I took her Yoga for Fertility class at Flow Yoga Center which was taught to me by the owner and director, Danni Pomplun. The idea that I could teach pregnant women in a way that would empower them and help them connect with their baby through yoga and meditation, while giving birth partners an opportunity to be involved, intrigued\n",
      "llama_print_timings:        load time =  3906.97 ms\n",
      "llama_print_timings:      sample time =   286.48 ms /   512 runs   (    0.56 ms per token,  1787.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6095.13 ms /   265 tokens (   23.00 ms per token,    43.48 tokens per second)\n",
      "llama_print_timings:        eval time = 23342.98 ms /   510 runs   (   45.77 ms per token,    21.85 tokens per second)\n",
      "llama_print_timings:       total time = 29817.26 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd03d5a9-978f-454f-9dde-da67377225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655734\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning. And I hope you will join me in my mission to bring joy and happiness into this world through our simple acts, big or small.\n",
      "I am here to help people find their purpose in life, and live happier, healthier lives, so we can create a better tomorrow for ourselves, our families, and the whole humanity.\n",
      "I hope you will join me in my mission!\n",
      "I was born into the family of refugees who were escaping from Vietnam after the war. I’ve lived most of my life in Canada, and have been very fortunate to live in a safe and peaceful country surrounded by loving parents, caring relatives, and supportive friends.\n",
      "My parents had never had the chance to go through proper education themselves because they were born into poor families and raised during the war. So they couldn’t teach me much about life and how the world works. I was mostly left alone to figure things out on my own.\n",
      "I have always been curious, so I’ve always asked why this is happening or that is happening. And most of the time, I didn’t get any answer at all because people around me simply don’t know either. So I had to figure it out myself.\n",
      "Since I was a little boy, I have been learning things very fast and easily. I learned how to read when I was 3 years old, and how to write when I was 4 years old. After that, I started to study at school, but I wasn’t really good with memorizing stuffs. I always had trouble with math, even though I loved it so much!\n",
      "But, I always loved learning new things about the world around me. I wanted to understand how everything works so I can solve any problems that I might encounter in life. It was very frustrating for me when I couldn’t answer my own questions. And there were many times where I didn’t know what to do, and ended up doing nothing at all because of the fear of failure.\n",
      "But I was always a dreamer who believed anything is possible if we put our minds into it. So instead of giving up after a few failures, I just kept trying harder until I finally got the results that I wanted. And I always had a passion for learning new things.\n",
      "I have been in many different kinds of jobs, from working as a janitor to being an IT engineer. All these years, I have learned\n",
      "llama_print_timings:        load time = 55411.45 ms\n",
      "llama_print_timings:      sample time =   294.79 ms /   512 runs   (    0.58 ms per token,  1736.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9441.50 ms /   265 tokens (   35.63 ms per token,    28.07 tokens per second)\n",
      "llama_print_timings:        eval time = 38638.85 ms /   510 runs   (   75.76 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time = 48470.30 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a89a21-41b0-4075-9ed4-78c9c561bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655844\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning.\n",
      "When you think about it, there really aren’t too many things we can choose in our lives. We are born into our families and have limited control over where we live, what language we speak or even how we perceive the world around us. Our parents teach us their values and ways of living. Some of these are good, some bad and some that will need to change as we grow older and see more of the world. We can choose our friends (somewhat), but we have no control over who our neighbors or work colleagues are, or even who sits next to us on the bus.\n",
      "We can’t control what happens in politics, nor can we predict whether it will rain tomorrow. We don’t know how much longer this life will last and even if we had a crystal ball, it wouldn’t tell us much about our future.\n",
      "So what is it that we can control? Well, I believe the answer to that question lies in the power of choice. We are in control of ourselves, of what we choose to think, say or do at any given moment. This means that even though we have no control over the external circumstances and events that take place around us, we get to choose how we respond to them.\n",
      "We also have the ability to choose how we want to live our lives. We can choose to be positive, generous, kind, grateful and forgiving. We can choose how much effort we put into things like our career, relationships or health. We can decide what we do with our time, money and energy. It is up to us to decide where we focus all this, and in turn, how our lives will unfold.\n",
      "I’m not saying it’s easy. In fact, some days I feel like a tiny boat caught between two enormous waves of life. But when I take a moment to look back at my choices, I notice that they have been made for me by a part of myself that is guided by something bigger than fear or selfishness. It’s like there’s an inner voice that helps me navigate the stormy waters and choose the right direction.\n",
      "I believe we all have this “voice”. We just need to pay attention. So let me ask you, what choices are you making today? How do they make you feel? And where will your choices lead you tomorrow?\n",
      "The meaning of life is to give life meaning. As we choose how\n",
      "llama_print_timings:        load time =  8732.26 ms\n",
      "llama_print_timings:      sample time =   288.48 ms /   512 runs   (    0.56 ms per token,  1774.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9488.67 ms /   265 tokens (   35.81 ms per token,    27.93 tokens per second)\n",
      "llama_print_timings:        eval time = 38675.95 ms /   510 runs   (   75.84 ms per token,    13.19 tokens per second)\n",
      "llama_print_timings:       total time = 48548.04 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dda4493-8622-4e1c-8896-a63f40cc4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689655906\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m about our search for happiness. I’ve learned that this can be quite a struggle, because we often don’t know what brings us true happiness. Instead we pursue things that are only momentarily satisfying. We think possessions will bring us happiness but they never do. We think having more money and power will make us happier but they rarely do.\n",
      "The key to knowing how to live your life is learning that there are no shortcuts to happiness. The true path to lasting happiness requires you to face your fears, feel your feelings, let yourself be vulnerable, forgive yourself for mistakes you’ve made, and love wholeheartedly without expecting anything in return. These things take courage but they will bring you more joy than you can possibly imagine.\n",
      "We all want to live a life we are proud of. We want to make the most of our lives by becoming stronger versions of ourselves. But it takes time and effort to become who we really are, and this usually requires us to let go of the things that hold us back from growing as people. It’s important to remember that you can’t move forward if you keep looking at where you came from.\n",
      "If you want to get more out of life, you have to give yourself a chance. You need to take risks and push your boundaries so you can become the person you were meant to be. And don’t ever let anyone tell you that your dreams aren’t important or realistic because they are!\n",
      "If there was any way I could help you get more out of life, it would be by helping you discover what brings you happiness and then showing you how to make it a part of your life. The happier we are, the better our lives become. I want you to live a joyful life, so if that’s something you want too, please contact me for a free consultation.\n",
      "This entry was posted in Happiness, Mindfulness and tagged happiness, mindfulness on February 19, 2018 by motivationtruth.\n",
      "← The Power of Self-Awareness What Does it Mean to Live a Good Life? →\n",
      "One thought on “How Do You Get More Out Of Life?”\n",
      "Pingback: How Can I Change My Negative Attitude? | MotivationTruth.com - Motivational Articles & Video Blog – Inspiration for Positive Thinking, Self-Help & Goal Setting!\n",
      "llama_print_timings:        load time =  8515.68 ms\n",
      "llama_print_timings:      sample time =   283.81 ms /   512 runs   (    0.55 ms per token,  1804.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9438.29 ms /   265 tokens (   35.62 ms per token,    28.08 tokens per second)\n",
      "llama_print_timings:        eval time = 38751.18 ms /   510 runs   (   75.98 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time = 48566.92 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a43d3544-82dc-4b04-a60a-05f82c0b8883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689656152\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it, and to live it with love. We are all meant to be here on Earth for a finite amount of time, whether that's 10 years or 150 years. It doesn't matter. What matters is that we spend our lives in service of other people, in service of living the best lives possible, and making sure everyone we come into contact with feels loved, accepted, heard, seen, and appreciated. The meaning of life is to be human. I believe we are all created equal, and we are given our gifts to use for good. We have to remember that it's not about us as individuals; we are here on this earth together and we need each other. This is why the meaning of life is to live with love.\n",
      "I've always believed that I was meant to be a performer, ever since I was four years old. That has been my journey, and it still remains my passion. To make people laugh, think, feel, connect, and to entertain is one of the greatest gifts in life. But recently I feel like my purpose is much more than just being an actress or a comedian; it's about making sure that every single person I come into contact with feels heard, seen, loved, accepted, and appreciated. When we can all do this for each other as human beings—when everyone can have the same opportunities and love—that's how we live our purpose on Earth.\n",
      "I believe in kindness, compassion, empathy, equality, love, joy, laughter, peace, friendship, service of others, being a good listener, and being present. Every single person is unique, so their purpose will be different from everyone else's. But the main thing is that we can all live with love, and I think that is our main purpose on Earth: to love each other.\n",
      "I was born in Los Angeles, but my family moved around a lot when I was growing up. My dad was an actor, so his career took us places like New York City for theater roles, or to the suburbs of Chicago for a movie he was making—and then back to LA. But we spent most of our time in and around New York City. My mom was born and raised there, which means I am a proud native New Yorker through her bloodline!\n",
      "It's hard to sum up my childhood in just one or two sentences\n",
      "llama_print_timings:        load time = 17212.20 ms\n",
      "llama_print_timings:      sample time =   290.84 ms /   512 runs   (    0.57 ms per token,  1760.44 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13057.79 ms /   265 tokens (   49.27 ms per token,    20.29 tokens per second)\n",
      "llama_print_timings:        eval time = 59206.60 ms /   510 runs   (  116.09 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:       total time = 72650.05 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c2eec96-6ba0-441e-a493-077a6afad5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689656248\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I believe that if you don't derive a deep sense of purpose from what you do, if you don't come radiantly alive several times a day, if you don't feel deeply grateful at the tremendous good fortune that has been bestowed on you, then you are wasting your life. And life is too short to waste.\n",
      "I believe we are put here on earth to manifest God in our own unique way. We all have a special gift, and I believe it's incumbent upon us to find out what that is and offer it to the world. That is why I believe in human potential and that is why I believe there is always hope.\n",
      "I believe you can have everything you want if you help enough other people get what they need.\n",
      "I believe each of us is a piece of God, made in his image. And we are here to make manifest the glory of God within ourselves.\n",
      "I believe that if you work hard on developing your talents and use them in service of others, there is nothing you cannot achieve. I believe that if we do what we love, we will never \"work\" another day in our lives. I believe that if we follow our bliss, life becomes a joyous adventure.\n",
      "I believe that what makes America great is the fact that anyone can become an American; and there is nothing more beautiful than people from different cultures coming together to create something greater than themselves. This melting pot of ideas and ideals has produced the most innovative country in history, and I feel blessed to be living in the United States at this time.\n",
      "I believe we are all citizens of the earth, and that the problems facing us today — war, poverty, hunger, disease, environmental degradation — are global in nature. And only global solutions will solve them. I believe we must work together for the good of the whole human race or perish together as fools who believed that we could live on this planet without living with each other.\n",
      "I believe that if you treat people right, they'll treat you right 90 percent of the time. The key to success in life is learning how to handle that other ten percent. I believe that if you stand for a perfect world you will never accomplish anything. But if, like me, you are willing to stand up for an imperfect world, you can get something done.\n",
      "llama_print_timings:        load time = 17244.82 ms\n",
      "llama_print_timings:      sample time =   288.25 ms /   512 runs   (    0.56 ms per token,  1776.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13099.47 ms /   265 tokens (   49.43 ms per token,    20.23 tokens per second)\n",
      "llama_print_timings:        eval time = 59173.72 ms /   510 runs   (  116.03 ms per token,     8.62 tokens per second)\n",
      "llama_print_timings:       total time = 72655.96 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ae97aff-0677-4050-9da2-bd7567d05023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689656345\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and that we are all here to find happiness. To me, happiness can only be found in the present moment.\n",
      "So, my goal every day is to make myself happy right now. That’s the best way to have a good life. I do this by doing what makes me feel good in the here and now. I try not to think about things too much. As long as I am breathing, I know everything will be fine.\n",
      "The future is unknown and the past has already happened. There’s no point worrying about either of them. The only thing we can do is enjoy this moment right now. If you want to live a good life then think about how much fun you are having right at this second. Don’t stress about what could go wrong, or what went wrong in the past because it doesn’t matter!\n",
      "You can worry all day long but nothing will change until you take action. The only way to make things happen is to do them. I think most of us know that though. It’s not like we are all lazy. I don’t think there’s anything wrong with being lazy either. In fact, if we lived in a world where everyone was busy doing their own thing then laziness wouldn’t be so bad. But the problem is that people tend to work way too hard and they forget how to enjoy themselves.\n",
      "This post is about enjoying yourself right now, not worrying about things you can’t change, or things you need to do tomorrow. I want to emphasize that in order to live a good life you must make yourself happy right here, right now. Don’t think about anything else but the present moment.\n",
      "We only get one life so we better enjoy it while we still have it! This is your life and no one can tell you how to live it. If someone gives you advice then that’s great but don’t worry if you ignore them. It doesn’t matter what they think anyway because this is your life, not theirs.\n",
      "I am going to keep this post short because I want you to do things that make you happy right now! You will never be as young as you are today and I don’t want you to waste any more of your precious time on Earth reading my blog posts. Get out there and have fun! Make yourself happy, do whatever it takes!\n",
      "I hope you enjoyed this post! If you did then you should sign\n",
      "llama_print_timings:        load time = 17079.58 ms\n",
      "llama_print_timings:      sample time =   287.09 ms /   512 runs   (    0.56 ms per token,  1783.41 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13143.90 ms /   265 tokens (   49.60 ms per token,    20.16 tokens per second)\n",
      "llama_print_timings:        eval time = 59108.10 ms /   510 runs   (  115.90 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:       total time = 72633.33 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07afd7-0668-48b8-accc-160fe0aa19de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 847 (7568d1a)\n",
      "main: seed  = 1689656710\n",
      "ggml_init_cublas: found 6 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 3: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 4: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 5: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 127663.23 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: offloading 0 repeating layers to GPU\n",
      "llama_model_load_internal: offloaded 0/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 872 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 16 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is \\n in 50 words\u001b[0m or less.\n",
      "A 2018 calendar full of pithy and witty quotes from all your favorite authors and philosophers! Plus, some quotes are by people that you've never heard of. All for $9.99 on Amazon.\n",
      "This is a print-on-demand product. You will receive your purchase in 5-7 business days.\n",
      "The 2018 calendar will be shipped on November 6, 2017. If you want to order one and want it shipped sooner than that, please contact me at benjamin@benjaminwiley.com so I can ship it directly from my home studio.\n",
      "I'm a cartoonist and illustrator based in Seattle, WA. In addition to selling prints of my work on this site, I also do freelance illustration for various publications (The New Yorker, The New York Times, The Wall Street Journal, etc.). I've worked as an artist-in-residence at both Google and Pixar Animation Studios. I have a BFA from the Rhode Island School of Design. And that's the end of my professional bio. If you want to know anything else about me, please feel free to get in touch!\n",
      "If"
     ]
    }
   ],
   "source": [
    "# try cpu (3～4s/token)\n",
    "!./main --color  -ngl 0 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is \\n in 50 words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adc052-0240-4cce-a706-18cb16f8cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
