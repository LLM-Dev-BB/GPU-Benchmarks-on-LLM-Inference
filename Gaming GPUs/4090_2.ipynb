{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ab0e6f-86d6-41f5-a0cd-aaee6226fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 02:13:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    21W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:82:00.0 Off |                  Off |\n",
      "|  0%   28C    P8    26W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263860972 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388112b9-bf0d-4cc1-85c2-4e8e72f20df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40847d9f-7b14-44d5-b4df-8ad885a33a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19437e33-5986-4681-9cce-d497e03adb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kllama.cpp:2208:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‘\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K’ may be undefined [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsequence-point\u0007-Wsequence-point\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2208 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
      "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b76e0c-6e15-4d0b-b328-67556b448d82",
   "metadata": {},
   "source": [
    "### Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef56705-84d3-47e0-98b0-181273f6f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560032\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to help others.\n",
      "I'm a very outgoing person and love to meet new people. I am also a very funny and sarcastic person, so most likely will make you laugh! I'm always happy and smiling, but don't let that fool you because I can be serious when needed to be.\n",
      "I would like to find someone who is very loyal and trustworthy; someone who has the same interests as me and likes to do things outdoors. Someone who thinks of others before themselves. And lastly, someone who will always have my back no matter what!\n",
      "jess_b160 hasn't asked any friends to write a recommendation yet.\n",
      "jess_b160 has not yet answered any of the optional questions that would appear here if she had answered them.\n",
      "I am very easy going and like to enjoy life. I hate drama and will try my hardest not to cause it! I love to spend time with family and friends, be active and have fun! I'm looking for someone who is also outgoing and has the same interests. Someone who will always put a smile on my face no matter what!\n",
      "Too many to count! I love everything from country music to hip hop. I guess you could say I like anything except rap metal or punk rock.\n",
      "Traveling, camping, fishing, hiking and much more!\n",
      "I am looking for someone who would be my best friend; someone who is very outgoing and knows how to have fun! Someone with a great sense of humor and who likes to laugh. I'm not really into the whole party scene or the bar scene, so if you are then that won't work for me! Also, someone who doesn't like drama or fighting is also very important to me!\n",
      "jess_b160 has not yet answered any of the optional questions that would appear here if she had picked an answer.\n",
      "I love to be outdoors and do a lot of things that involve being outside; hiking, camping, fishing etc. I also like to go to concerts or just relax at home.\n",
      "I'm very energetic, so I stay very busy! I love to travel and try new things.\n",
      "I am very outgoing and fun loving. I have a great sense of humor; sometimes sarcastic but always a good time.\n",
      "I have\n",
      "llama_print_timings:        load time =  6072.23 ms\n",
      "llama_print_timings:      sample time =   443.46 ms /   512 runs   (    0.87 ms per token,  1154.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   920.34 ms /   265 tokens (    3.47 ms per token,   287.94 tokens per second)\n",
      "llama_print_timings:        eval time =  8217.31 ms /   510 runs   (   16.11 ms per token,    62.06 tokens per second)\n",
      "llama_print_timings:       total time =  9710.76 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd4adf2-f70f-4e26-8cc6-98e57e30db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560051\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to serve others, and it’s a privilege for me to be able to use my skills in architecture and design to improve people's lives. I strive to create positive impact through thoughtful spaces that are sustainable and economically feasible.\n",
      "I have been practicing architecture since 2016 after getting my Bachelor of Architecture from the University of Colorado Denver and my Master of Architecture from the University of Texas at Austin. In my free time, I like to run, hike and explore with my dog, Luna, who always comes along for the ride!\n",
      "\"I was impressed by the passion that everyone in the office has towards their work.\"\n",
      "What's your favorite thing about working at Hoefer Wysocki?\n",
      "I have been practicing architecture since 2016 after getting my Bachelor of Architecture from the University of Colorado Denver and my Master of Architecture from the University of Texas at Austin. In my free time, I like to run, hike and explore with my dog Luna, who always comes along for the ride!\n",
      "\"I was impressed by the passion that everyone in the office has towards their work.\"\n",
      "What’s your favorite thing about working at Hoefer Wysocki?\n",
      "The staff, who are all very intelligent, talented and knowledgeable people. I am inspired daily to be a better person with each new experience here at Hoefer Wysocki.\n",
      "I have been practicing architecture since 2016 after getting my Bachelor of Architecture from the University of Colorado Denver and my Master of Architecture from the University of Texas at Austin. In my free time, I like to run, hike and explore with my dog Luna, who always comes along for the ride!\n",
      "\"I was impressed by the passion that everyone in the office has towards their work.\"\n",
      "What’s your favorite thing about working at Hoefer Wysocki?\n",
      "It's a close tie between the people I work with and the projects themselves. Everyone is super friendly and supportive, and each project comes with its own unique challenges to solve, which I love!\n",
      "I have been practicing architecture since 2016 after getting my Bachelor of Architecture from the University of Colorado Denver and my Master of Architecture from the University of Texas at Austin. In my free time, I like to run, hike and explore with my dog Luna, who always\n",
      "llama_print_timings:        load time =  1207.75 ms\n",
      "llama_print_timings:      sample time =   395.05 ms /   512 runs   (    0.77 ms per token,  1296.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   914.13 ms /   265 tokens (    3.45 ms per token,   289.89 tokens per second)\n",
      "llama_print_timings:        eval time =  8249.45 ms /   510 runs   (   16.18 ms per token,    61.82 tokens per second)\n",
      "llama_print_timings:       total time =  9688.24 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31dae18-81cd-4e97-9c5d-dc830a7b7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560065\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, and the purpose of our time here on Earth. We are here because there are lessons to learn and there will always be someone who needs us; we should not think that it is all about us or what makes us happy.\n",
      "What’s your ultimate goal?\n",
      "To live my life in an authentic way, to be myself, as much as I can be. To have a purposeful life where I can give back and help others to become the best versions of themselves. A life that is full of love, passion, adventure, kindness, truth and honesty, in all ways possible.\n",
      "How does one do this?\n",
      "This question is really what it’s about for me – how to live my life in a way where I can fulfil the above goal.\n",
      "A lot of people talk about living your passion. What I think is more important than doing something that you love, but rather, finding the thing that makes you come alive and be the best version of yourself; the thing you are the most authentically YOU in; find out what it takes to bring out the best in you.\n",
      "What’s your philosophy on life?\n",
      "I believe that everything happens for a reason. So if things have happened to me, then they’ve happened for a reason and I must learn from them; there is a greater purpose to my being here and now. In order for me to become the best version of myself, I must accept responsibility for what has been – and take full accountability for all that it brought me.\n",
      "What are you thankful for today?\n",
      "I’m thankful for my life, my health and happiness, and all the people who have contributed in the making of me; the people I know and the people I don’t know yet.\n",
      "Doing what makes me happy and feeling good about myself (and not thinking negatively) are a few things that make me happy. Other than these two things, it is being with my family, friends and pets – all who make me feel loved and appreciated.\n",
      "How did you become the person you’re today?\n",
      "I have always been someone who has tried to live her life in an authentic way. I believe that this is where true happiness can be found; living your life without trying to impress or satisfy others, but rather do what makes YOU happy – and then expressing that happiness to those around you.\n",
      "My dad (my hero) and my grandma\n",
      "llama_print_timings:        load time =  1213.47 ms\n",
      "llama_print_timings:      sample time =   402.46 ms /   512 runs   (    0.79 ms per token,  1272.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   900.66 ms /   265 tokens (    3.40 ms per token,   294.23 tokens per second)\n",
      "llama_print_timings:        eval time =  8242.82 ms /   510 runs   (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_print_timings:       total time =  9675.91 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd12983-0e77-41ec-bcb8-ff63e18f0163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560078\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live a meaningful life.\n",
      "This entry was posted in Life, Philosophy and tagged Albert Einstein, John Lennon, Life on October 19, 2015 by Theryn.\n",
      "I agree with you – but it’s hard when things are going badly (or even just mildly inconvenient) to see the meaning in them. I think that’s why people turn to religion: a lot easier to believe in a higher power if your life sucks!\n",
      "Yep, and that’s not a bad thing for some people – it works for them. But if it doesn’t work for you then it isn’t right for you. I think it’s good that you can see the meaning in things even when they don’t go your way. I always have hope that there is something better around the corner, and I always find my faith rewarded. ����\n",
      "I like this! And yes, life IS what we make of it – so why not do our best?\n",
      "Thank you – I think it all comes down to how you want to feel. If you’re happy with your lot then carry on doing whatever it is that makes you happy. If you’re not then change something. It doesn’t have to be a big change, sometimes it only takes a small shift in perspective to make the world of difference. ���\n",
      "← How To Stay Motivated When You Work From Home\n",
      "Why I Am Grateful For My Kids →\n",
      "http://www.life-unlimited.com/2015/10/the-meaning-of-life\">\n",
      "Tweets by @Theryn_Miller\n",
      "Books To Read Before… on What Does It Take To Be A Writer?\n",
      "What Does It Take T… on What Does It Take To Be A Writer?\n",
      "Life-Unlimited.com | Theryn Miller © 2018. All Rights Reserved.\n",
      "Hosted by: Hosting.ca - Canada's Best Value in Web Hosting!\n",
      "Web Design By: Sites By Design | Privacy Policy | Earnings Disclaimer\n",
      "This site uses cookies to improve your experience. Please read and accept our privacy policy. Accept Reject Read More > Cookie Policy\n",
      "Thank you for subscribing to Life-Unlimited.com's Newsletter! You will\n",
      "llama_print_timings:        load time = 10567.02 ms\n",
      "llama_print_timings:      sample time =   486.98 ms /   512 runs   (    0.95 ms per token,  1051.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1263.12 ms /   265 tokens (    4.77 ms per token,   209.80 tokens per second)\n",
      "llama_print_timings:        eval time = 11399.93 ms /   510 runs   (   22.35 ms per token,    44.74 tokens per second)\n",
      "llama_print_timings:       total time = 13279.60 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47b6ecb-c212-4e12-bbd7-1ec4639bfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560105\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m found in love.\n",
      "You have to understand that this isn’t a new thought for me. In fact, I can trace its origins back nearly twenty years ago when my wife and I were both young Christians. It was during those years we began to learn what it meant to love God with all our heart, soul, mind and strength as Jesus instructed his disciples in Mark 12:30. We also discovered what it meant to love your neighbor as yourself according to the teaching of Leviticus 19:18.\n",
      "It was during those years I began to develop my own philosophy for life – which is, “Love is the meaning of life.” It is a belief that has grown in me over time and through experience. When I speak about it today, the words have become like a song I can’t seem to get out of my head.\n",
      "I believe love is the purpose of our existence.\n",
      "We are here on earth to be vessels of God’s love to one another. We are meant to love and serve each other in all we do – with our work, family, church, school, community, etc. You can only fulfill that calling when you understand what it means to love God and those around you.\n",
      "When Jesus gave the Sermon on the Mount he gave us a blueprint for living the life of a disciple. In fact, I think it is more than just a blueprint; it’s a map that we can follow which will lead us safely home. The first words out of his mouth were, “Blessed are the poor in spirit.” He goes on to say, “Blessed are those who mourn,” and then “blessed are the meek” – but what caught my attention was when he said, “blessed are you when people insult you, persecute you and falsely say all kinds of evil against you because of me.”\n",
      "This is the life Jesus called us to lead. He told his disciples, “Be merciful just as your Father is merciful,” and then “be perfect just as your heavenly father is perfect.” It’s as simple as that! Don’t get caught up in what others are doing or saying about you; instead, love them anyway and do the same for those who persecute you. That will be the mark of a disciple – of someone who has been transformed by Jesus Christ.\n",
      "The second\n",
      "llama_print_timings:        load time =  1711.22 ms\n",
      "llama_print_timings:      sample time =   485.01 ms /   512 runs   (    0.95 ms per token,  1055.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1276.57 ms /   265 tokens (    4.82 ms per token,   207.59 tokens per second)\n",
      "llama_print_timings:        eval time = 11400.67 ms /   510 runs   (   22.35 ms per token,    44.73 tokens per second)\n",
      "llama_print_timings:       total time = 13292.18 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49166db8-e77c-4e15-8194-9452eff672c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560122\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\"\n",
      "\"Life is a series of lessons; each day we must learn something new.\"\n",
      "\"It's not true that life is one damn thing after another - it's the same damn thing over and over.\"\n",
      "\"I've learned... That the best view comes after the hardest climb.\n",
      "That the best kind of friend is the kind you can sit on a porch and swing with, never say a word, and then walk away feeling like that was the best conversation you've ever had.\n",
      "That it's those small daily happenings that make life so spectacular.\"\n",
      "\"What really matters in life are your relationships to other people.\"\n",
      "\"Life is one big road with lots of signs. So when you riding through the ruts, don't complicate your mind. Flee from hate, mischief and jealousy. Don't bury your thoughts, put your vision to reality. Wake Up and Live!\" - Bob Marley\n",
      "\"Life is a succession of lessons which must be lived to be understood.\"\n",
      "\"Every man dies; not every man lives.\"\n",
      "\"In three words I can sum up everything I've learned about life: it goes on.\"\n",
      "\"It is in the shelter of each other that people live.\"\n",
      "\"If you want to achieve excellence, you can get there today. As of this second, quit doing less-than-excellent work.\" - Thomas J. Watson\n",
      "\"Life has no smooth road or joy-free zones - But has many a winding turn and hill to climb.\"\n",
      "\"The most beautiful experience we can have is the mysterious. It is the fundamental emotion which stands at the cradle of true art and true science.\" - Albert Einstein\n",
      "\"What lies behind us, what lies ahead of us are tiny matters compared to what lies within us.\"\n",
      "\"It's hard to be perfect but it's a whole lotta fun to try!\"\n",
      "\"Life is full of beauty. Notice it. Notice the bumblebee, the small child, and the smiling faces. Smell the rain, and feel the wind. Live your life to the fullest potential, and fight for your dreams.\" - Ashley Smith\n",
      "\"Nobody can go back and start a new beginning, but anyone can start today and make a new ending\n",
      "llama_print_timings:        load time =  1708.39 ms\n",
      "llama_print_timings:      sample time =   392.87 ms /   512 runs   (    0.77 ms per token,  1303.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1268.10 ms /   265 tokens (    4.79 ms per token,   208.97 tokens per second)\n",
      "llama_print_timings:        eval time = 11362.80 ms /   510 runs   (   22.28 ms per token,    44.88 tokens per second)\n",
      "llama_print_timings:       total time = 13152.97 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bd026e-0584-4617-abb5-9777259505da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560139\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. The more you love, smile and laugh – the happier you are.\n",
      "I believe in Karma, what goes around comes around.\n",
      "So how do I apply this philosophy in my daily life? Well, simply put … I just go with the flow.\n",
      "Everyone has their own definition of happiness. For me it’s about surrounding myself with people and things that inspire me to be a better person. It could be something as simple as having coffee with a friend who brings out the best in me, or going for a long run with my dog in the woods.\n",
      "I’ve learnt that we don’t really have control over what happens around us but we do have a choice on how we react to it. So I strive to make every day the best day and live by 2 simple golden rules:\n",
      "1. Don’t sweat the small stuff, because in the bigger picture they’re just that – small stuff.\n",
      "2. Treat everyone around you with kindness and respect.\n",
      "It is important for me to have a purpose in life and I try to live my life as if it were my last day on earth. This approach has led me to be more pro-active, open minded and curious about everything.\n",
      "I’ve learnt that we create our own luck by being positive and making things happen instead of waiting for them to happen to us. That is what has led me to where I am today.\n",
      "I’m a Swedish girl with an international mindset who has lived abroad in the Middle East, South America and Asia. My love affair with travel started at a very young age. As soon as I could walk I was off exploring my neighborhood. Then when I was a teenager I went to the US for two months to visit my sister (who’s an American) and her family – that is where I got bitten by the travel bug!\n",
      "I studied International Business in Sweden, where I received a BSc degree from University of Borås. After graduating I spent time working with branding and marketing for fashion companies such as H&M and Zara in Stockholm. Then I moved to Dubai to start my own business and set-up an online store selling vintage jewelry, handmade leather goods and unique clothing items.\n",
      "After 3 years in the UAE my husband and I decided to move to Chile for a new adventure. There\n",
      "llama_print_timings:        load time = 25878.77 ms\n",
      "llama_print_timings:      sample time =   393.07 ms /   512 runs   (    0.77 ms per token,  1302.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2211.16 ms /   265 tokens (    8.34 ms per token,   119.85 tokens per second)\n",
      "llama_print_timings:        eval time = 20456.37 ms /   510 runs   (   40.11 ms per token,    24.93 tokens per second)\n",
      "llama_print_timings:       total time = 23190.44 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710338f3-686c-4fd4-a5ca-c25984db27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560192\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift.\n",
      "To find your gift is the meaning of life.\n",
      "The purpose of life, is a life of purpose.\n",
      "I think the whole meaning of life is the relationships we have with other people.\n",
      "I'm not sure what the meaning of life is but I know that the most important thing on earth is to be in love.\n",
      "This is an exciting time for us at The Meaning of Life Project, and we hope you will find many opportunities to get involved.\n",
      "1. Join our Facebook group: The Meaning of Life Project - A Celebration of Humanity.\n",
      "2. Check out our website: www.themeaningoflife.com\n",
      "3. Follow us on Twitter: @TMLProject\n",
      "4. Send us your thoughts and comments, we'd love to hear from you!\n",
      "5. And of course we always welcome donations to support the project.\n",
      "We hope you will help us celebrate the meaning of life!\n",
      "The Meaning of Life Project was created by three friends, who met in a creative writing class at The University of New Brunswick. They were amazed by the diversity of their fellow students and realized that there must be so many people with unique perspectives on the meaning of life. In an attempt to celebrate humanity they decided to reach out to anyone and everyone who would talk to them about this topic, hoping that together they could create a piece of art that would capture the essence of our existence, and in doing so, bring us all closer together.\n",
      "In 2015, they began gathering stories by asking people what \"The Meaning of Life\" means to them. The project has grown rapidly since then, and is now an international movement. We hope that you will join us on this journey and help make the world a better place!\n",
      "We are three friends with big dreams and a passion for life. We believe in the power of art to bring people together, and we want to create something that will contribute to a more peaceful and meaningful planet.\n",
      "In 2015, as students at The University of New Brunswick, we began gathering stories by asking people what \"The Meaning of Life\" means to them. We were inspired by the profoundness of what we heard. People’s answers ranged from the simple to the complex; the comical to the serious. From our interviews it was clear\n",
      "llama_print_timings:        load time =  3360.33 ms\n",
      "llama_print_timings:      sample time =   393.45 ms /   512 runs   (    0.77 ms per token,  1301.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2207.33 ms /   265 tokens (    8.33 ms per token,   120.05 tokens per second)\n",
      "llama_print_timings:        eval time = 20365.91 ms /   510 runs   (   39.93 ms per token,    25.04 tokens per second)\n",
      "llama_print_timings:       total time = 23096.65 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b585f81c-99a5-4330-b46d-4a05c2c555db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560222\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "I love my husband and our children, and family members and friends and all animals.\n",
      "I have been trying to teach them to be kind to others as well.\n",
      "I also believe that we are supposed to take care of this world we live in, this planet Earth. It is a beautiful place but it is so fragile. We need to preserve what we have or lose it forever.\n",
      "There are billions of stars in the sky and millions upon millions of people on this planet we call home. I am just one of them. So small and yet so big when you consider how much I love and care about all who enter my life.\n",
      "I believe that this is true for everyone! We are all connected by our experiences, and through these connections we learn and grow and change.\n",
      "We all have the same amount of hours in a day, but what we do with them makes us different from each other. This blog is about me and my family and what we do with those hours. I hope you enjoy it!\n",
      "If you are interested in advertising on this site or just want to share your thoughts, please email me at thedailynutmeg@gmail.com or post a comment here. Thank you for visiting!\n",
      "This blog is written by me and the opinions expressed herein are my own and do not reflect those of any organization I am affiliated with. All content on this site is © The Daily Nutmeg unless otherwise stated. Unauthorized use and/or duplication of this material without express and written permission from this blog’s author and owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to The Daily Nutmeg with appropriate and specific direction to the original content. All images on this site are © The Daily Nutmeg unless otherwise stated, all rights reserved.\n",
      "Hey! We’ve been loving your blog for a while now, but we just wanted to let you know that we’ve nominated it for a Liebster Award. If you haven’t heard of the award yet (you can read more about it here: http://thepiggytoes.com/2015/07/13/liebster-award/), it’s basically a fun way to connect bloggers together and create new connections.\n",
      "We hope you accept, but if not – we still love your work!\n",
      "llama_print_timings:        load time =  3461.32 ms\n",
      "llama_print_timings:      sample time =   394.81 ms /   512 runs   (    0.77 ms per token,  1296.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2177.35 ms /   265 tokens (    8.22 ms per token,   121.71 tokens per second)\n",
      "llama_print_timings:        eval time = 20509.27 ms /   510 runs   (   40.21 ms per token,    24.87 tokens per second)\n",
      "llama_print_timings:       total time = 23212.12 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d4d2d2-0271-4f74-8af2-3d1e6521707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560252\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "I was born and raised in southern California, where I attended private schools for most of my education. After high school, I spent one year at an all women’s college, Mills College in Oakland, CA before transferring to UCLA. It was there that I majored in Psychology and received a Bachelor's degree.\n",
      "After a few years of working with children, adolescents, and families as a counselor, I decided to attend graduate school at California State University, Northridge (CSUN) where I earned my Master’s degree and License in Clinical Psychology. While attending CSUN, I worked as an extern at the Betty Ford Center, where I was trained in the areas of alcoholism, codependency, and family dynamics.\n",
      "I have been a Licensed Marriage & Family Therapist for over 20 years, during which time I've had the privilege of working with hundreds of children, teens, adults, couples, and families, dealing with a wide range of issues. My passion is for helping others find their own strengths and resources, so that they may be empowered to move toward living a life filled with meaning, purpose, and joy!\n",
      "I have had the honor of working as a therapist at two major psychiatric hospitals in Orange County: La Vida Hospital and College Hospital. While at these institutions I worked primarily with adults dealing with issues such as depression, anxiety, bipolar disorder, schizophrenia, eating disorders, self-injury, grief/loss, trauma, and personality disorders.\n",
      "I spent many years working in a private practice setting in Laguna Beach where I worked with children, adolescents, adults, couples, families and groups, dealing with issues such as: anxiety, depression, eating disorders, sexual abuse, self-injury, trauma, relationship difficulties, loss/grief, anger management, parenting problems, life transitions (divorce, career change, empty nest syndrome), and personal growth.\n",
      "I also worked for the Orange County Department of Education as a contract therapist at two high schools in Orange County: Saddleback High School in Santa Ana and Dana Hills High School in Dana Point\n",
      "llama_print_timings:        load time = 50829.91 ms\n",
      "llama_print_timings:      sample time =   547.39 ms /   512 runs   (    1.07 ms per token,   935.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3289.75 ms /   265 tokens (   12.41 ms per token,    80.55 tokens per second)\n",
      "llama_print_timings:        eval time = 31309.82 ms /   510 runs   (   61.39 ms per token,    16.29 tokens per second)\n",
      "llama_print_timings:       total time = 35276.31 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d79dd90-67fd-48dd-bca4-24daf152a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560343\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, enjoy each day and take advantage of every opportunity.\n",
      "I’m not afraid to admit that I don’t know something but it doesn’t stop me from finding out! My motto in life is “you can do anything you want if you put your mind to it”. I love meeting new people and travelling around the world learning about different cultures, eating different foods, seeing different sights and trying different things.\n",
      "I started my career as a trainee management accountant at M&S where I worked for five years but was always interested in a number of different areas within business so decided to move into consultancy where I can gain experience across a number of different sectors and industries.\n",
      "I am a keen skier/snowboarder and have travelled all over Europe (and now America) looking for new slopes to conquer. When I’m not in the mountains you will find me on the water either surfing, wake boarding or kayaking depending on where I am!\n",
      "I love reading and spend most evenings with a good book but when I’ve managed to put that down I can often be found at the cinema watching the latest releases.\n",
      "I have an interest in everything and believe there is always something new to learn about life, people and businesses. I enjoy meeting new people, seeing different sights and trying new things.\n",
      "The best thing about working at Buzzacott is being able to work with so many different types of clients from a variety of sectors which gives me the opportunity to build my experience across a number of areas. I also love working in a team environment where everyone is willing to help out when needed. Everyone is very supportive and always happy to share their knowledge or answer any questions that you may have.\n",
      "I enjoy playing sports, particularly hockey, football and squash. I’m not afraid to admit that I don’t know something but it doesn’t stop me from finding out! My motto in life is “you can do anything you want if you put your mind to it”.\n",
      "Tax updates: what the Spring Statement 2019 means for you 1 March 2019\n",
      "On Wednesday, Chancellor Philip Hammond delivered his Spring Statement. In this article we give a brief overview of the key takeaways.\n",
      "The tax benefits of holding UK commercial property in an offshore special\n",
      "llama_print_timings:        load time =  6351.45 ms\n",
      "llama_print_timings:      sample time =   468.89 ms /   512 runs   (    0.92 ms per token,  1091.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3227.11 ms /   265 tokens (   12.18 ms per token,    82.12 tokens per second)\n",
      "llama_print_timings:        eval time = 31603.49 ms /   510 runs   (   61.97 ms per token,    16.14 tokens per second)\n",
      "llama_print_timings:       total time = 35428.67 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138ce498-9718-44ef-a37d-9acaaa5ff0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560389\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m happiness.\n",
      "Happiness for yourself and others – if you achieve this then your life has been a success, no matter how long you lived it or what else you did. And there are many ways to be happy: with your partner, your friends, your family, your kids, your pet, in nature, through music, art, food, drinks, work, travel… anything that makes you smile, laugh and feel good is important!\n",
      "What I’ve learned recently – as a 30 year old woman who has been dating for almost half her life – is that being happy with yourself on your own is the hardest type of happiness to achieve. And also the most valuable. Because if you are not happy alone, then no one else can make you truly happy.\n",
      "I’ve learned that it doesn’t matter how many friends you have, how much you like your job or how amazing your boyfriend/husband is – none of this will make you happy if you don’t love yourself. And I don’t mean just physically loving yourself and being happy with the way you look (although that helps). I mean loving who you are as a person, knowing what makes you tick, understanding your strengths and weaknesses, accepting yourself in all of your complexities and learning to be comfortable in your own skin.\n",
      "I’ve never had a problem making friends or getting a boyfriend. But when it comes to spending time alone (or even just being in the company of someone I don’t know well), my mind starts doing strange things – thinking about all sorts of random stuff that is often very negative, and playing over and over again scenes from my past which make me feel bad about myself or guilty.\n",
      "I guess this is a result of growing up as an only child in a country where I didn’t speak the language (and hence felt isolated), and then moving to another foreign country with no friends. Or maybe it’s because I don’t have enough people around me who can love me for who I am, without trying to change me? Who knows – I spent too much time over-analysing this already, when I should just be focusing on how to fix the problem.\n",
      "The first step is accepting that you are not happy alone and need to do something about it. That’s actually harder than it sounds because most people live in denial of their faults (or they don’t even real\n",
      "llama_print_timings:        load time =  6340.90 ms\n",
      "llama_print_timings:      sample time =   479.57 ms /   512 runs   (    0.94 ms per token,  1067.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3252.37 ms /   265 tokens (   12.27 ms per token,    81.48 tokens per second)\n",
      "llama_print_timings:        eval time = 31580.07 ms /   510 runs   (   61.92 ms per token,    16.15 tokens per second)\n",
      "llama_print_timings:       total time = 35441.77 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf297fc-6ba8-4e95-9d94-679c563df6c5",
   "metadata": {},
   "source": [
    "### f16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11219943-66d6-4e82-8f62-7b2897e9e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560435\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find one's purpose and live that purpose. Everyone has a different one, but everyone should strive for it.\n",
      "It's important because our future depends on this. If we don't take care of our planet now, there won't be a place to live in the future. I really want to help with this problem and do my best to make things better.\n",
      "My favorite subject is math. There are a lot of interesting problems that can be solved mathematically and I like solving them. My favorite math topic is geometry because it involves visualization, which I find exciting. Another reason why I love math is because I find it very logical and it helps me think about things in different ways.\n",
      "I am currently taking four AP classes: AP Calculus BC, AP Biology, AP Chemistry, and AP English Language and Composition.\n",
      "My goals for the future are to get into a good college that will help me major in something that I'm interested in.\n",
      "I plan on being a pediatrician because it involves helping children which is what I love doing most of all.\n",
      "If you have any questions or comments about me, feel free to email me at eisengart@lwsd.org or talk with me in class!\n",
      "Most Important Lesson Learned: It's important to do your best work and put effort into everything that you do. No matter how small of a job it may seem, if you want something done right, you have to try hard enough until you get it exactly as you wanted!\n",
      "The Most Memorable Moment in My Lifetime: Going scuba diving at the Great Barrier Reef and swimming with fish that came up to me. The most memorable part of this was watching these fish follow my fins around just like a dog would chase after your feet. It was also incredibly beautiful because I've never seen anywhere in the world as colorful as there.\n",
      "My Favorite Food: Any dessert! Especially if it has lots of chocolate and ice cream.\n",
      "The most challenging part about being a student at LWSD is making sure that you always do your best work so you can be successful, because otherwise you will fall behind in school very quickly. I know this from experience; when I first came to the United States, my English was not very good and it was difficult for me to keep up with assignments or\n",
      "llama_print_timings:        load time = 18688.51 ms\n",
      "llama_print_timings:      sample time =   390.43 ms /   512 runs   (    0.76 ms per token,  1311.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   919.16 ms /   265 tokens (    3.47 ms per token,   288.31 tokens per second)\n",
      "llama_print_timings:        eval time = 11004.24 ms /   510 runs   (   21.58 ms per token,    46.35 tokens per second)\n",
      "llama_print_timings:       total time = 12442.85 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4ab840-58b0-4dc5-9d40-55ae1ae5987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560470\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, to achieve your full potential and give back to others.\n",
      "I started my career with the Australian Army in 1978. After 25 years as a soldier, I went on to work for a few more years in the Defence industry. In 2004, I began my second career as an Industrial Relations consultant. As a Certified Practising Industrial Relations Consultant, I have been helping employers and employees with their IR issues for over 15 years.\n",
      "I am married to Judy, who has worked in the Defence industry for over 30 years. We live on the Sunshine Coast and love spending time with our two sons and three granddaughters. My hobbies are walking, swimming and golfing.\n",
      "The main thing I enjoy most about being an IR consultant is helping people solve problems. No matter how difficult a situation may seem, there’s always a solution that can be found to help the employer and employee achieve a win-win outcome. My philosophy is to first understand both parties’ needs, then work with them to find a practical solution.\n",
      "Whatever your IR issue is, I have faced it before. So don’t hesitate to contact me for advice or assistance by phone 0417 693 258 or email gary@garysaunders.com.au today. You will receive my reply within one business day.\n",
      "I believe the meaning of life is to be happy, achieve your full potential and give back to others.\n",
      "After 25 years as an Australian Army officer and later a military industrial relations consultant, in 2004 I began my second career as an Industrial Relations consultant. As a Certified Practising Industrial Relations Consultant, I have been helping employers and employees with their IR issues for over 15 years.\n",
      "I am married to Judy, who has worked in the Defence industry for over 30 years. We live on the Sunshine Coast and love spending time with our two sons and three granddaughters. My hobbies are walking, swimming and golfing. The main thing I enjoy most about being an IR consultant is helping people solve problems. No matter how difficult a situation may seem, there’s always a solution that can be found to overcome it.\n",
      "I have been\n",
      "llama_print_timings:        load time =  2300.94 ms\n",
      "llama_print_timings:      sample time =   389.99 ms /   512 runs   (    0.76 ms per token,  1312.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   901.30 ms /   265 tokens (    3.40 ms per token,   294.02 tokens per second)\n",
      "llama_print_timings:        eval time = 11025.53 ms /   510 runs   (   21.62 ms per token,    46.26 tokens per second)\n",
      "llama_print_timings:       total time = 12445.81 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846f1f9a-4fc9-48d0-8d04-d4eb7999ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560487\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m simple:\n",
      "When you’re born, you are given a choice. You can choose to be happy or sad. No matter what happens in your life, you have that choice. Happiness will bring you peace and contentment, while sadness brings only pain and suffering. For me, choosing happiness means living life to the fullest, every moment of every day. It means never giving up on my dreams. It means living with courage, honesty, and compassion. It means trying to be a better person than I was yesterday.\n",
      "But most importantly, it’s about always remembering that there is someone greater in control, who will make everything right if you just have faith. Always believe that we are never given more than we can handle, and always look on the bright side of life. If you do this, you’ll be a happier person with fewer regrets.\n",
      "Happiness is something everyone wants to find in their life. No matter what your age or situation, it’s important for each of us to stay happy throughout our lives. Some people believe that happiness comes from achieving something they want – perhaps passing an exam, getting married, or finding a great job. But the truth is, these things will not make you happy if you don’t find happiness within yourself first.\n",
      "We are all born with the same potential to be happy and live a full life. The problem is that most people never realize their true potential because they spend so much time worrying about what others think of them or trying to avoid making mistakes, taking risks, or being vulnerable. For me, learning to accept who I am and embrace my flaws has been the key to finding happiness.\n",
      "So how do you find your own happiness? How can you learn to accept yourself for who you are? Here’s a list of 10 tips:\n",
      "Tip #1 – Live in the moment\n",
      "You can never be happy if you keep thinking about the past or worrying about the future. If you spend too much time thinking about what has happened and what might happen, it will have a negative impact on your happiness. So for now, forget about everything else that’s going on in your life and focus on enjoying this moment.\n",
      "Tip #2 – Be content with yourself\n",
      "If you can learn to be happy with who you are and the things you have achieved so far, it will make it much easier to enjoy what is happening around you\n",
      "llama_print_timings:        load time =  2292.34 ms\n",
      "llama_print_timings:      sample time =   389.55 ms /   512 runs   (    0.76 ms per token,  1314.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   900.64 ms /   265 tokens (    3.40 ms per token,   294.23 tokens per second)\n",
      "llama_print_timings:        eval time = 11034.89 ms /   510 runs   (   21.64 ms per token,    46.22 tokens per second)\n",
      "llama_print_timings:       total time = 12453.89 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b997d2b2-3217-47cc-9a70-854ee0ab4638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560505\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m found in self-fulfillment. The world is a big place and there are so many different people that all have different tastes, likes and dislikes. This also means that everyone has different ways to find their own self-fulfillment as well. Whether it be a job, an artistic outlet or just helping others, I truly think that in order for someone to live a happy and fulfilled life they have to find something that makes them unique and what better way than to do whatever you want?\n",
      "I am not saying that everyone should have their own business. In fact, it is very hard work and takes a lot of time and dedication. I would never encourage anyone to start a business unless they have an extreme passion for what they are doing and think it is something they can commit to. What I mean by self-fulfillment is that you should be able to do whatever makes you happy, even if it's just working in retail or waiting tables. You can live your life the way you want to!\n",
      "I had a conversation with one of my sisters about this topic and she said something so true and interesting: \"You know what I hate? When people say that they are going to do something but never follow through with it.\"\n",
      "She continued by saying, \"So many people tell me things like 'I'm gonna start a business,' or 'I'm gonna write a book.' But those people don’t actually go and do those things. I think that if you are going to say something then you should actually do it.\"\n",
      "This is so true! Everyone has passions and interests, but only some of us will take the plunge and try to make our dreams come true. So, what's stopping people? Is it fear or lack of motivation? Or maybe it is because people truly just don’t know how to get started.\n",
      "I believe that if you have a passion then there should be no reason why you shouldn't pursue it. I truly think that the meaning of life for each person is different, and I believe that everyone deserves to find their own meaning and purpose in life. This is where self-fulfillment comes into play. Everyone has passions; everyone has dreams and goals. If you are interested in something then go for it! You have nothing to lose except your happiness if you don't do anything about it.\n",
      "I think that most people\n",
      "llama_print_timings:        load time = 34438.93 ms\n",
      "llama_print_timings:      sample time =   389.80 ms /   512 runs   (    0.76 ms per token,  1313.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1268.56 ms /   265 tokens (    4.79 ms per token,   208.90 tokens per second)\n",
      "llama_print_timings:        eval time = 16659.54 ms /   510 runs   (   32.67 ms per token,    30.61 tokens per second)\n",
      "llama_print_timings:       total time = 18447.06 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2aedcd-60fc-4d07-8921-91c11695c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560562\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be in harmony with yourself, your family and your friends.\n",
      "I was born in 1947 in a small village near Rennes (France). After studies in Paris at Ecole des Arts et Métiers, I became an engineer working for the French Government. I enjoyed my work as it helped me to understand the world around us, but I always felt that there was something more important in life than technical knowledge and engineering.\n",
      "I discovered Reiki at a time when I had become very depressed and my health was suffering. Since then, I have learnt many healing techniques including Thai massage, Reflexology, Aromatherapy, Bach flowers remedies, Emotional Freedom Technique (EFT), Hypnotherapy, NLP, CBT and Mindfulness meditation.\n",
      "When my children were born in 1974 and 1976, I gave up my engineering job to become a housewife as they needed more attention than the company could accept! They grew up with all these therapies so that now I can see very clearly how important it is to teach them from an early age.\n",
      "In 2013, after spending many years working in various holistic centres and healing clinics, I decided that it was time for me to make a difference by creating this organisation, The Family Healer CIC, which enables parents to become aware of the importance of teaching their children from an early age.\n",
      "The Family Healer is also about you as a parent or grandparent: we can all learn how to help our children and ourselves in dealing with any difficult situation that life may bring us.\n",
      "If you are interested in becoming an instructor for Kids-EFT, please contact me at: info@familyhealer.co.uk.\n",
      "I am also a certified practitioner of EFT and NLP. I have a Masters degree from the University of London and I am an accredited coach with The Coaching Academy.\n",
      "I have helped many people to change their lives, from managers to housewives, artists to engineers, children to retired people.\n",
      "I can help you to deal with any difficult issue: stress, depression, anxiety, phobias, fears, pain management etc.\n",
      "If you are interested in learning EFT or NLP contact me at info@familyhealer.co.uk.\n",
      "For\n",
      "llama_print_timings:        load time =  4016.15 ms\n",
      "llama_print_timings:      sample time =   393.47 ms /   512 runs   (    0.77 ms per token,  1301.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1279.45 ms /   265 tokens (    4.83 ms per token,   207.12 tokens per second)\n",
      "llama_print_timings:        eval time = 16646.87 ms /   510 runs   (   32.64 ms per token,    30.64 tokens per second)\n",
      "llama_print_timings:       total time = 18449.08 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d26e34-4e99-4ed8-8072-8c55fe117344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560588\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it, to taste experience to the utmost, to reach out eagerly and without fear for newer and richer experience.\n",
      "I always feel like there’s a lot of pressure on kids who are actors from childhood. You see somebody who was in a movie when they were 10 years old, and by 20, everybody expects them to be an alcoholic drug addict. I just want to make sure that, for the rest of my life, that doesn’t happen to me.\n",
      "I believe that everything happens for a reason. People change so you can learn to let go, things go wrong so you appreciate them when they’re right, you believe lies so you eventually learn to trust no one but yourself, and sometimes good things fall apart so better things can fall together.\n",
      "A great book should leave you with many experiences, and slightly exhausted at the end. You live several lives while reading.\n",
      "You have to be a little brave for love.\n",
      "When I’m an old woman I shall wear purple With a red hat which doesn’t go And doesn’t suit me.\n",
      "I know the voices are there, but I don’t hear them. The voices want me to hurt myself or someone else and that’s why they’re always yelling at me. It’s like having two different personalities inside you at the same time – like being schizophrenic. But I’m not crazy.\n",
      "I hate stupid people. There is nothing worse than a stupid person who doesn’t know it. When I encounter one of those idiots, I just smile and move on because trying to explain things to them will only make me feel worse for wasting my breath.\n",
      "Sometimes you have to stop analyzing the past, stop planning the future, stop figuring out precisely how you feel, stop deciding exactly what you want or need, and just let go.\n",
      "I have never known a musician who regretted being one. Whatever deceptions life may have in store for you, music itself is not going to let you down.\n",
      "No matter where you are, no matter how far you’ve gone, it’s never too late to be whoever you want to be. I hope you remember that.\n",
      "The most important thing a father can do for his children is love their mother.\n",
      "I think if anyone has to tell you they’re funny, they’\n",
      "llama_print_timings:        load time =  4071.51 ms\n",
      "llama_print_timings:      sample time =   433.70 ms /   512 runs   (    0.85 ms per token,  1180.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1263.37 ms /   265 tokens (    4.77 ms per token,   209.76 tokens per second)\n",
      "llama_print_timings:        eval time = 16644.51 ms /   510 runs   (   32.64 ms per token,    30.64 tokens per second)\n",
      "llama_print_timings:       total time = 18470.76 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6236b49b-14c5-4502-8ddb-0af61a8eb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560613\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c019a4d-0356-485f-a7c0-605440790a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560702\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3619e5a4-ac77-4bf4-bace-ef7a991134e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689560712\n",
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319efa8-99e1-4534-9287-8626ee27fb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
