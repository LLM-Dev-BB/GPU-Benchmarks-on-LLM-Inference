{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 03:05:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "|100%   24C    P8    20W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:21:00.0 Off |                  N/A |\n",
      "|100%   24C    P8    27W / 420W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:22:00.0 Off |                  N/A |\n",
      "|100%   24C    P8    14W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "model name\t: AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "============Memory================\n",
      "MemTotal:       263845160 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 4916, done.\u001b[K\n",
      "remote: Counting objects: 100% (1714/1714), done.\u001b[K\n",
      "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
      "remote: Total 4916 (delta 1641), reused 1594 (delta 1577), pack-reused 3202\u001b[K\n",
      "Receiving objects: 100% (4916/4916), 4.07 MiB | 23.93 MiB/s, done.\n",
      "Resolving deltas: 100% (3367/3367), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa7f97-4a2e-47ab-8c66-ca34489ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4126f1-7c42-4d66-8a7d-8e5c29c322da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eaea4f0-2d91-41ef-9f32-4103f2858796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   8340      0 --:--:-- --:--:-- --:--:--  8374\n",
      "Downloading tokenizer\n",
      "--2023-07-17 03:06:19--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-07-17 03:06:20 (28.1 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-07-17 03:06:20--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:06:20 (58.2 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-07-17 03:06:20--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  35.9MB/s    in 7m 3s   \n",
      "\n",
      "2023-07-17 03:13:24 (30.4 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-07-17 03:13:24--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:13:24 (60.2 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 03:13:24--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:13:24 (177 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-07-17 03:13:42--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  68.1MB/s    in 3m 10s  \n",
      "\n",
      "2023-07-17 03:16:52 (65.4 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 03:16:52--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  55.5MB/s    in 5m 14s  \n",
      "\n",
      "2023-07-17 03:22:07 (39.5 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 03:22:07--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:22:07 (56.3 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 03:22:07--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:22:08 (233 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-07-17 03:22:42--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  66.9MB/s    in 3m 57s  \n",
      "\n",
      "2023-07-17 03:26:39 (65.5 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:26:39--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  52.1MB/s    in 3m 57s  \n",
      "\n",
      "2023-07-17 03:30:36 (65.5 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:30:36--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  70.4MB/s    in 5m 24s  \n",
      "\n",
      "2023-07-17 03:36:00 (47.9 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:36:00--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  79.5MB/s    in 3m 31s  \n",
      "\n",
      "2023-07-17 03:39:31 (73.5 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:39:31--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0.002s  \n",
      "\n",
      "2023-07-17 03:39:31 (43.1 KB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 03:39:31--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:39:32 (423 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-07-17 03:40:57--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  68.1MB/s    in 4m 9s   \n",
      "\n",
      "2023-07-17 03:45:06 (62.6 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:45:06--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  49.4MB/s    in 4m 23s  \n",
      "\n",
      "2023-07-17 03:49:29 (59.1 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:49:29--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  72.9MB/s    in 4m 8s   \n",
      "\n",
      "2023-07-17 03:53:38 (62.7 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:53:38--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  69.8MB/s    in 4m 5s   \n",
      "\n",
      "2023-07-17 03:57:43 (63.5 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:57:43--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated  39%[======>             ]   6.03G  45.8MB/s    in 1m 47s  \n",
      "\n",
      "2023-07-17 03:59:31 (57.8 MB/s) - Connection closed at byte 6474813440. Retrying.\n",
      "\n",
      "--2023-07-17 03:59:32--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 9849146009 (9.2G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[+++++++============>]  15.20G  75.2MB/s    in 2m 34s  \n",
      "\n",
      "2023-07-17 04:02:06 (61.2 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 04:02:06--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  65.6MB/s    in 4m 18s  \n",
      "\n",
      "2023-07-17 04:06:24 (60.3 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 04:06:24--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  63.2MB/s    in 4m 12s  \n",
      "\n",
      "2023-07-17 04:10:38 (61.7 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 04:10:38--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  53.2MB/s    in 5m 16s  \n",
      "\n",
      "2023-07-17 04:15:54 (49.2 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 04:15:54--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0.002s  \n",
      "\n",
      "2023-07-17 04:15:55 (41.0 KB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 04:15:55--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.64.80.1, 2606:4700:130:436c:6f75:6466:6c61:7265\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.64.80.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 04:15:55 (786 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/7B/ggml-model-f16.bin\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:5120 n_mult:256 n_head:40 n_layer:40\n",
      "Writing vocab...\n",
      "[  1/363] Writing tensor tok_embeddings.weight                  | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  2/363] Writing tensor norm.weight                            | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  4/363] Writing tensor layers.0.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  5/363] Writing tensor layers.0.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  6/363] Writing tensor layers.0.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  7/363] Writing tensor layers.0.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  8/363] Writing tensor layers.0.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  9/363] Writing tensor layers.0.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 10/363] Writing tensor layers.0.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 11/363] Writing tensor layers.0.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 12/363] Writing tensor layers.0.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 13/363] Writing tensor layers.1.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 14/363] Writing tensor layers.1.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 15/363] Writing tensor layers.1.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 16/363] Writing tensor layers.1.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 17/363] Writing tensor layers.1.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 18/363] Writing tensor layers.1.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 19/363] Writing tensor layers.1.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 20/363] Writing tensor layers.1.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 21/363] Writing tensor layers.1.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 22/363] Writing tensor layers.2.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 23/363] Writing tensor layers.2.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 24/363] Writing tensor layers.2.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 25/363] Writing tensor layers.2.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 26/363] Writing tensor layers.2.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 27/363] Writing tensor layers.2.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 28/363] Writing tensor layers.2.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 29/363] Writing tensor layers.2.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 30/363] Writing tensor layers.2.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 31/363] Writing tensor layers.3.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 32/363] Writing tensor layers.3.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 33/363] Writing tensor layers.3.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 34/363] Writing tensor layers.3.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 35/363] Writing tensor layers.3.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 36/363] Writing tensor layers.3.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 37/363] Writing tensor layers.3.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 38/363] Writing tensor layers.3.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 39/363] Writing tensor layers.3.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 40/363] Writing tensor layers.4.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 41/363] Writing tensor layers.4.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 42/363] Writing tensor layers.4.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 43/363] Writing tensor layers.4.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 44/363] Writing tensor layers.4.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 45/363] Writing tensor layers.4.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 46/363] Writing tensor layers.4.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 47/363] Writing tensor layers.4.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 48/363] Writing tensor layers.4.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 49/363] Writing tensor layers.5.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 50/363] Writing tensor layers.5.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 51/363] Writing tensor layers.5.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 52/363] Writing tensor layers.5.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 53/363] Writing tensor layers.5.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 54/363] Writing tensor layers.5.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 55/363] Writing tensor layers.5.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 56/363] Writing tensor layers.5.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 57/363] Writing tensor layers.5.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 58/363] Writing tensor layers.6.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 59/363] Writing tensor layers.6.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 60/363] Writing tensor layers.6.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 61/363] Writing tensor layers.6.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 62/363] Writing tensor layers.6.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 63/363] Writing tensor layers.6.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 64/363] Writing tensor layers.6.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 65/363] Writing tensor layers.6.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 66/363] Writing tensor layers.6.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 67/363] Writing tensor layers.7.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 68/363] Writing tensor layers.7.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 69/363] Writing tensor layers.7.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 70/363] Writing tensor layers.7.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 71/363] Writing tensor layers.7.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 72/363] Writing tensor layers.7.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 73/363] Writing tensor layers.7.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 74/363] Writing tensor layers.7.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 75/363] Writing tensor layers.7.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 76/363] Writing tensor layers.8.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 77/363] Writing tensor layers.8.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 78/363] Writing tensor layers.8.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 79/363] Writing tensor layers.8.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 80/363] Writing tensor layers.8.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 81/363] Writing tensor layers.8.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 82/363] Writing tensor layers.8.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 83/363] Writing tensor layers.8.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 84/363] Writing tensor layers.8.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 85/363] Writing tensor layers.9.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 86/363] Writing tensor layers.9.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 87/363] Writing tensor layers.9.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 88/363] Writing tensor layers.9.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 89/363] Writing tensor layers.9.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 90/363] Writing tensor layers.9.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 91/363] Writing tensor layers.9.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 92/363] Writing tensor layers.9.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 93/363] Writing tensor layers.9.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 94/363] Writing tensor layers.10.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 95/363] Writing tensor layers.10.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 96/363] Writing tensor layers.10.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 97/363] Writing tensor layers.10.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 98/363] Writing tensor layers.10.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 99/363] Writing tensor layers.10.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[100/363] Writing tensor layers.10.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[101/363] Writing tensor layers.10.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[102/363] Writing tensor layers.10.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[103/363] Writing tensor layers.11.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[104/363] Writing tensor layers.11.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[105/363] Writing tensor layers.11.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[106/363] Writing tensor layers.11.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[107/363] Writing tensor layers.11.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[108/363] Writing tensor layers.11.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[109/363] Writing tensor layers.11.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[110/363] Writing tensor layers.11.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[111/363] Writing tensor layers.11.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[112/363] Writing tensor layers.12.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[113/363] Writing tensor layers.12.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[114/363] Writing tensor layers.12.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[115/363] Writing tensor layers.12.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[116/363] Writing tensor layers.12.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[117/363] Writing tensor layers.12.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[118/363] Writing tensor layers.12.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[119/363] Writing tensor layers.12.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[120/363] Writing tensor layers.12.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[121/363] Writing tensor layers.13.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[122/363] Writing tensor layers.13.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[123/363] Writing tensor layers.13.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[124/363] Writing tensor layers.13.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[125/363] Writing tensor layers.13.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[126/363] Writing tensor layers.13.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[127/363] Writing tensor layers.13.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[128/363] Writing tensor layers.13.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[129/363] Writing tensor layers.13.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[130/363] Writing tensor layers.14.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[131/363] Writing tensor layers.14.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[132/363] Writing tensor layers.14.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[133/363] Writing tensor layers.14.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[134/363] Writing tensor layers.14.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[135/363] Writing tensor layers.14.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[136/363] Writing tensor layers.14.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[137/363] Writing tensor layers.14.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[138/363] Writing tensor layers.14.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[139/363] Writing tensor layers.15.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[140/363] Writing tensor layers.15.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[141/363] Writing tensor layers.15.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[142/363] Writing tensor layers.15.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[143/363] Writing tensor layers.15.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[144/363] Writing tensor layers.15.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[145/363] Writing tensor layers.15.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[146/363] Writing tensor layers.15.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[147/363] Writing tensor layers.15.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[148/363] Writing tensor layers.16.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[149/363] Writing tensor layers.16.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[150/363] Writing tensor layers.16.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[151/363] Writing tensor layers.16.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[152/363] Writing tensor layers.16.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[153/363] Writing tensor layers.16.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[154/363] Writing tensor layers.16.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[155/363] Writing tensor layers.16.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[156/363] Writing tensor layers.16.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[157/363] Writing tensor layers.17.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[158/363] Writing tensor layers.17.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[159/363] Writing tensor layers.17.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[160/363] Writing tensor layers.17.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[161/363] Writing tensor layers.17.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[162/363] Writing tensor layers.17.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[163/363] Writing tensor layers.17.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[164/363] Writing tensor layers.17.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[165/363] Writing tensor layers.17.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[166/363] Writing tensor layers.18.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[167/363] Writing tensor layers.18.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[168/363] Writing tensor layers.18.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[169/363] Writing tensor layers.18.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[170/363] Writing tensor layers.18.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[171/363] Writing tensor layers.18.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[172/363] Writing tensor layers.18.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[173/363] Writing tensor layers.18.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[174/363] Writing tensor layers.18.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[175/363] Writing tensor layers.19.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[176/363] Writing tensor layers.19.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[177/363] Writing tensor layers.19.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[178/363] Writing tensor layers.19.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[179/363] Writing tensor layers.19.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[180/363] Writing tensor layers.19.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[181/363] Writing tensor layers.19.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[182/363] Writing tensor layers.19.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[183/363] Writing tensor layers.19.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[184/363] Writing tensor layers.20.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[185/363] Writing tensor layers.20.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[186/363] Writing tensor layers.20.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[187/363] Writing tensor layers.20.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[188/363] Writing tensor layers.20.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[189/363] Writing tensor layers.20.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[190/363] Writing tensor layers.20.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[191/363] Writing tensor layers.20.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[192/363] Writing tensor layers.20.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[193/363] Writing tensor layers.21.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[194/363] Writing tensor layers.21.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[195/363] Writing tensor layers.21.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[196/363] Writing tensor layers.21.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[197/363] Writing tensor layers.21.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[198/363] Writing tensor layers.21.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[199/363] Writing tensor layers.21.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[200/363] Writing tensor layers.21.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[201/363] Writing tensor layers.21.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[202/363] Writing tensor layers.22.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[203/363] Writing tensor layers.22.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[204/363] Writing tensor layers.22.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[205/363] Writing tensor layers.22.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[206/363] Writing tensor layers.22.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[207/363] Writing tensor layers.22.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[208/363] Writing tensor layers.22.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[209/363] Writing tensor layers.22.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[210/363] Writing tensor layers.22.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[211/363] Writing tensor layers.23.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[212/363] Writing tensor layers.23.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[213/363] Writing tensor layers.23.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[214/363] Writing tensor layers.23.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[215/363] Writing tensor layers.23.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[216/363] Writing tensor layers.23.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[217/363] Writing tensor layers.23.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[218/363] Writing tensor layers.23.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[219/363] Writing tensor layers.23.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[220/363] Writing tensor layers.24.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[221/363] Writing tensor layers.24.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[222/363] Writing tensor layers.24.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[223/363] Writing tensor layers.24.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[224/363] Writing tensor layers.24.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[225/363] Writing tensor layers.24.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[226/363] Writing tensor layers.24.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[227/363] Writing tensor layers.24.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[228/363] Writing tensor layers.24.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[229/363] Writing tensor layers.25.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[230/363] Writing tensor layers.25.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[231/363] Writing tensor layers.25.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[232/363] Writing tensor layers.25.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[233/363] Writing tensor layers.25.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[234/363] Writing tensor layers.25.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[235/363] Writing tensor layers.25.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[236/363] Writing tensor layers.25.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[237/363] Writing tensor layers.25.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[238/363] Writing tensor layers.26.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[239/363] Writing tensor layers.26.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[240/363] Writing tensor layers.26.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[241/363] Writing tensor layers.26.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[242/363] Writing tensor layers.26.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[243/363] Writing tensor layers.26.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[244/363] Writing tensor layers.26.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[245/363] Writing tensor layers.26.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[246/363] Writing tensor layers.26.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[247/363] Writing tensor layers.27.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[248/363] Writing tensor layers.27.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[249/363] Writing tensor layers.27.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[250/363] Writing tensor layers.27.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[251/363] Writing tensor layers.27.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[252/363] Writing tensor layers.27.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[253/363] Writing tensor layers.27.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[254/363] Writing tensor layers.27.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[255/363] Writing tensor layers.27.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[256/363] Writing tensor layers.28.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[257/363] Writing tensor layers.28.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[258/363] Writing tensor layers.28.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[259/363] Writing tensor layers.28.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[260/363] Writing tensor layers.28.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[261/363] Writing tensor layers.28.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[262/363] Writing tensor layers.28.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[263/363] Writing tensor layers.28.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[264/363] Writing tensor layers.28.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[265/363] Writing tensor layers.29.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[266/363] Writing tensor layers.29.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[267/363] Writing tensor layers.29.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[268/363] Writing tensor layers.29.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[269/363] Writing tensor layers.29.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[270/363] Writing tensor layers.29.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[271/363] Writing tensor layers.29.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[272/363] Writing tensor layers.29.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[273/363] Writing tensor layers.29.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[274/363] Writing tensor layers.30.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[275/363] Writing tensor layers.30.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[276/363] Writing tensor layers.30.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[277/363] Writing tensor layers.30.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[278/363] Writing tensor layers.30.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[279/363] Writing tensor layers.30.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[280/363] Writing tensor layers.30.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[281/363] Writing tensor layers.30.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[282/363] Writing tensor layers.30.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[283/363] Writing tensor layers.31.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[284/363] Writing tensor layers.31.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[285/363] Writing tensor layers.31.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[286/363] Writing tensor layers.31.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[287/363] Writing tensor layers.31.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[288/363] Writing tensor layers.31.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[289/363] Writing tensor layers.31.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[290/363] Writing tensor layers.31.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[291/363] Writing tensor layers.31.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[292/363] Writing tensor layers.32.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[293/363] Writing tensor layers.32.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[294/363] Writing tensor layers.32.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[295/363] Writing tensor layers.32.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[296/363] Writing tensor layers.32.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[297/363] Writing tensor layers.32.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[298/363] Writing tensor layers.32.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[299/363] Writing tensor layers.32.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[300/363] Writing tensor layers.32.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[301/363] Writing tensor layers.33.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[302/363] Writing tensor layers.33.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[303/363] Writing tensor layers.33.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[304/363] Writing tensor layers.33.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[305/363] Writing tensor layers.33.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[306/363] Writing tensor layers.33.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[307/363] Writing tensor layers.33.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[308/363] Writing tensor layers.33.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[309/363] Writing tensor layers.33.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[310/363] Writing tensor layers.34.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[311/363] Writing tensor layers.34.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[312/363] Writing tensor layers.34.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[313/363] Writing tensor layers.34.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[314/363] Writing tensor layers.34.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[315/363] Writing tensor layers.34.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[316/363] Writing tensor layers.34.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[317/363] Writing tensor layers.34.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[318/363] Writing tensor layers.34.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[319/363] Writing tensor layers.35.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[320/363] Writing tensor layers.35.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[321/363] Writing tensor layers.35.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[322/363] Writing tensor layers.35.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[323/363] Writing tensor layers.35.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[324/363] Writing tensor layers.35.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[325/363] Writing tensor layers.35.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[326/363] Writing tensor layers.35.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[327/363] Writing tensor layers.35.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[328/363] Writing tensor layers.36.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[329/363] Writing tensor layers.36.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[330/363] Writing tensor layers.36.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[331/363] Writing tensor layers.36.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[332/363] Writing tensor layers.36.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[333/363] Writing tensor layers.36.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[334/363] Writing tensor layers.36.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[335/363] Writing tensor layers.36.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[336/363] Writing tensor layers.36.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[337/363] Writing tensor layers.37.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[338/363] Writing tensor layers.37.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[339/363] Writing tensor layers.37.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[340/363] Writing tensor layers.37.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[341/363] Writing tensor layers.37.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[342/363] Writing tensor layers.37.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[343/363] Writing tensor layers.37.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[344/363] Writing tensor layers.37.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[345/363] Writing tensor layers.37.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[346/363] Writing tensor layers.38.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[347/363] Writing tensor layers.38.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[348/363] Writing tensor layers.38.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[349/363] Writing tensor layers.38.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[350/363] Writing tensor layers.38.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[351/363] Writing tensor layers.38.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[352/363] Writing tensor layers.38.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[353/363] Writing tensor layers.38.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[354/363] Writing tensor layers.38.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[355/363] Writing tensor layers.39.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[356/363] Writing tensor layers.39.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[357/363] Writing tensor layers.39.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[358/363] Writing tensor layers.39.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[359/363] Writing tensor layers.39.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/13B/ggml-model-f16.bin\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:6656 n_mult:256 n_head:52 n_layer:60\n",
      "Writing vocab...\n",
      "[  1/543] Writing tensor tok_embeddings.weight                  | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  2/543] Writing tensor norm.weight                            | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  4/543] Writing tensor layers.0.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  5/543] Writing tensor layers.0.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  6/543] Writing tensor layers.0.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  7/543] Writing tensor layers.0.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  8/543] Writing tensor layers.0.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  9/543] Writing tensor layers.0.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 10/543] Writing tensor layers.0.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 11/543] Writing tensor layers.0.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 12/543] Writing tensor layers.0.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 13/543] Writing tensor layers.1.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 14/543] Writing tensor layers.1.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 15/543] Writing tensor layers.1.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 16/543] Writing tensor layers.1.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 17/543] Writing tensor layers.1.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 18/543] Writing tensor layers.1.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 19/543] Writing tensor layers.1.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 20/543] Writing tensor layers.1.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 21/543] Writing tensor layers.1.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 22/543] Writing tensor layers.2.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 23/543] Writing tensor layers.2.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 24/543] Writing tensor layers.2.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 25/543] Writing tensor layers.2.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 26/543] Writing tensor layers.2.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 27/543] Writing tensor layers.2.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 28/543] Writing tensor layers.2.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 29/543] Writing tensor layers.2.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 30/543] Writing tensor layers.2.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 31/543] Writing tensor layers.3.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 32/543] Writing tensor layers.3.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 33/543] Writing tensor layers.3.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 34/543] Writing tensor layers.3.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 35/543] Writing tensor layers.3.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 36/543] Writing tensor layers.3.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 37/543] Writing tensor layers.3.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 38/543] Writing tensor layers.3.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 39/543] Writing tensor layers.3.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 40/543] Writing tensor layers.4.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 41/543] Writing tensor layers.4.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 42/543] Writing tensor layers.4.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 43/543] Writing tensor layers.4.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 44/543] Writing tensor layers.4.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 45/543] Writing tensor layers.4.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 46/543] Writing tensor layers.4.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 47/543] Writing tensor layers.4.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 48/543] Writing tensor layers.4.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 49/543] Writing tensor layers.5.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 50/543] Writing tensor layers.5.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 51/543] Writing tensor layers.5.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 52/543] Writing tensor layers.5.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 53/543] Writing tensor layers.5.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 54/543] Writing tensor layers.5.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 55/543] Writing tensor layers.5.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 56/543] Writing tensor layers.5.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 57/543] Writing tensor layers.5.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 58/543] Writing tensor layers.6.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 59/543] Writing tensor layers.6.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 60/543] Writing tensor layers.6.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 61/543] Writing tensor layers.6.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 62/543] Writing tensor layers.6.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 63/543] Writing tensor layers.6.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 64/543] Writing tensor layers.6.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 65/543] Writing tensor layers.6.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 66/543] Writing tensor layers.6.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 67/543] Writing tensor layers.7.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 68/543] Writing tensor layers.7.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 69/543] Writing tensor layers.7.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 70/543] Writing tensor layers.7.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 71/543] Writing tensor layers.7.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 72/543] Writing tensor layers.7.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 73/543] Writing tensor layers.7.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 74/543] Writing tensor layers.7.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 75/543] Writing tensor layers.7.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 76/543] Writing tensor layers.8.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 77/543] Writing tensor layers.8.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 78/543] Writing tensor layers.8.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 79/543] Writing tensor layers.8.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 80/543] Writing tensor layers.8.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 81/543] Writing tensor layers.8.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 82/543] Writing tensor layers.8.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 83/543] Writing tensor layers.8.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 84/543] Writing tensor layers.8.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 85/543] Writing tensor layers.9.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 86/543] Writing tensor layers.9.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 87/543] Writing tensor layers.9.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 88/543] Writing tensor layers.9.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 89/543] Writing tensor layers.9.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 90/543] Writing tensor layers.9.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 91/543] Writing tensor layers.9.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 92/543] Writing tensor layers.9.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 93/543] Writing tensor layers.9.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 94/543] Writing tensor layers.10.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 95/543] Writing tensor layers.10.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 96/543] Writing tensor layers.10.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 97/543] Writing tensor layers.10.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 98/543] Writing tensor layers.10.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 99/543] Writing tensor layers.10.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[100/543] Writing tensor layers.10.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[101/543] Writing tensor layers.10.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[102/543] Writing tensor layers.10.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[103/543] Writing tensor layers.11.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[104/543] Writing tensor layers.11.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[105/543] Writing tensor layers.11.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[106/543] Writing tensor layers.11.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[107/543] Writing tensor layers.11.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[108/543] Writing tensor layers.11.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[109/543] Writing tensor layers.11.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[110/543] Writing tensor layers.11.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[111/543] Writing tensor layers.11.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[112/543] Writing tensor layers.12.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[113/543] Writing tensor layers.12.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[114/543] Writing tensor layers.12.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[115/543] Writing tensor layers.12.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[116/543] Writing tensor layers.12.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[117/543] Writing tensor layers.12.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[118/543] Writing tensor layers.12.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[119/543] Writing tensor layers.12.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[120/543] Writing tensor layers.12.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[121/543] Writing tensor layers.13.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[122/543] Writing tensor layers.13.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[123/543] Writing tensor layers.13.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[124/543] Writing tensor layers.13.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[125/543] Writing tensor layers.13.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[126/543] Writing tensor layers.13.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[127/543] Writing tensor layers.13.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[128/543] Writing tensor layers.13.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[129/543] Writing tensor layers.13.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[130/543] Writing tensor layers.14.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[131/543] Writing tensor layers.14.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[132/543] Writing tensor layers.14.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[133/543] Writing tensor layers.14.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[134/543] Writing tensor layers.14.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[135/543] Writing tensor layers.14.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[136/543] Writing tensor layers.14.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[137/543] Writing tensor layers.14.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[138/543] Writing tensor layers.14.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[139/543] Writing tensor layers.15.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[140/543] Writing tensor layers.15.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[141/543] Writing tensor layers.15.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[142/543] Writing tensor layers.15.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[143/543] Writing tensor layers.15.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[144/543] Writing tensor layers.15.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[145/543] Writing tensor layers.15.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[146/543] Writing tensor layers.15.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[147/543] Writing tensor layers.15.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[148/543] Writing tensor layers.16.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[149/543] Writing tensor layers.16.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[150/543] Writing tensor layers.16.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[151/543] Writing tensor layers.16.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[152/543] Writing tensor layers.16.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[153/543] Writing tensor layers.16.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[154/543] Writing tensor layers.16.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[155/543] Writing tensor layers.16.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[156/543] Writing tensor layers.16.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[157/543] Writing tensor layers.17.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[158/543] Writing tensor layers.17.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[159/543] Writing tensor layers.17.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[160/543] Writing tensor layers.17.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[161/543] Writing tensor layers.17.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[162/543] Writing tensor layers.17.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[163/543] Writing tensor layers.17.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[164/543] Writing tensor layers.17.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[165/543] Writing tensor layers.17.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[166/543] Writing tensor layers.18.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[167/543] Writing tensor layers.18.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[168/543] Writing tensor layers.18.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[169/543] Writing tensor layers.18.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[170/543] Writing tensor layers.18.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[171/543] Writing tensor layers.18.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[172/543] Writing tensor layers.18.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[173/543] Writing tensor layers.18.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[174/543] Writing tensor layers.18.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[175/543] Writing tensor layers.19.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[176/543] Writing tensor layers.19.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[177/543] Writing tensor layers.19.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[178/543] Writing tensor layers.19.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[179/543] Writing tensor layers.19.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[180/543] Writing tensor layers.19.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[181/543] Writing tensor layers.19.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[182/543] Writing tensor layers.19.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[183/543] Writing tensor layers.19.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[184/543] Writing tensor layers.20.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[185/543] Writing tensor layers.20.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[186/543] Writing tensor layers.20.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[187/543] Writing tensor layers.20.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[188/543] Writing tensor layers.20.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[189/543] Writing tensor layers.20.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[190/543] Writing tensor layers.20.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[191/543] Writing tensor layers.20.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[192/543] Writing tensor layers.20.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[193/543] Writing tensor layers.21.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[194/543] Writing tensor layers.21.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[195/543] Writing tensor layers.21.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[196/543] Writing tensor layers.21.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[197/543] Writing tensor layers.21.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[198/543] Writing tensor layers.21.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[199/543] Writing tensor layers.21.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[200/543] Writing tensor layers.21.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[201/543] Writing tensor layers.21.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[202/543] Writing tensor layers.22.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[203/543] Writing tensor layers.22.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[204/543] Writing tensor layers.22.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[205/543] Writing tensor layers.22.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[206/543] Writing tensor layers.22.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[207/543] Writing tensor layers.22.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[208/543] Writing tensor layers.22.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[209/543] Writing tensor layers.22.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[210/543] Writing tensor layers.22.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[211/543] Writing tensor layers.23.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[212/543] Writing tensor layers.23.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[213/543] Writing tensor layers.23.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[214/543] Writing tensor layers.23.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[215/543] Writing tensor layers.23.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[216/543] Writing tensor layers.23.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[217/543] Writing tensor layers.23.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[218/543] Writing tensor layers.23.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[219/543] Writing tensor layers.23.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[220/543] Writing tensor layers.24.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[221/543] Writing tensor layers.24.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[222/543] Writing tensor layers.24.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[223/543] Writing tensor layers.24.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[224/543] Writing tensor layers.24.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[225/543] Writing tensor layers.24.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[226/543] Writing tensor layers.24.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[227/543] Writing tensor layers.24.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[228/543] Writing tensor layers.24.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[229/543] Writing tensor layers.25.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[230/543] Writing tensor layers.25.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[231/543] Writing tensor layers.25.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[232/543] Writing tensor layers.25.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[233/543] Writing tensor layers.25.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[234/543] Writing tensor layers.25.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[235/543] Writing tensor layers.25.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[236/543] Writing tensor layers.25.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[237/543] Writing tensor layers.25.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[238/543] Writing tensor layers.26.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[239/543] Writing tensor layers.26.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[240/543] Writing tensor layers.26.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[241/543] Writing tensor layers.26.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[242/543] Writing tensor layers.26.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[243/543] Writing tensor layers.26.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[244/543] Writing tensor layers.26.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[245/543] Writing tensor layers.26.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[246/543] Writing tensor layers.26.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[247/543] Writing tensor layers.27.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[248/543] Writing tensor layers.27.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[249/543] Writing tensor layers.27.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[250/543] Writing tensor layers.27.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[251/543] Writing tensor layers.27.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[252/543] Writing tensor layers.27.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[253/543] Writing tensor layers.27.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[254/543] Writing tensor layers.27.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[255/543] Writing tensor layers.27.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[256/543] Writing tensor layers.28.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[257/543] Writing tensor layers.28.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[258/543] Writing tensor layers.28.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[259/543] Writing tensor layers.28.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[260/543] Writing tensor layers.28.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[261/543] Writing tensor layers.28.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[262/543] Writing tensor layers.28.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[263/543] Writing tensor layers.28.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[264/543] Writing tensor layers.28.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[265/543] Writing tensor layers.29.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[266/543] Writing tensor layers.29.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[267/543] Writing tensor layers.29.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[268/543] Writing tensor layers.29.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[269/543] Writing tensor layers.29.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[270/543] Writing tensor layers.29.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[271/543] Writing tensor layers.29.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[272/543] Writing tensor layers.29.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[273/543] Writing tensor layers.29.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[274/543] Writing tensor layers.30.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[275/543] Writing tensor layers.30.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[276/543] Writing tensor layers.30.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[277/543] Writing tensor layers.30.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[278/543] Writing tensor layers.30.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[279/543] Writing tensor layers.30.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[280/543] Writing tensor layers.30.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[281/543] Writing tensor layers.30.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[282/543] Writing tensor layers.30.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[283/543] Writing tensor layers.31.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[284/543] Writing tensor layers.31.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[285/543] Writing tensor layers.31.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[286/543] Writing tensor layers.31.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[287/543] Writing tensor layers.31.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[288/543] Writing tensor layers.31.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[289/543] Writing tensor layers.31.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[290/543] Writing tensor layers.31.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[291/543] Writing tensor layers.31.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[292/543] Writing tensor layers.32.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[293/543] Writing tensor layers.32.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[294/543] Writing tensor layers.32.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[295/543] Writing tensor layers.32.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[296/543] Writing tensor layers.32.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[297/543] Writing tensor layers.32.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[298/543] Writing tensor layers.32.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[299/543] Writing tensor layers.32.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[300/543] Writing tensor layers.32.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[301/543] Writing tensor layers.33.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[302/543] Writing tensor layers.33.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[303/543] Writing tensor layers.33.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[304/543] Writing tensor layers.33.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[305/543] Writing tensor layers.33.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[306/543] Writing tensor layers.33.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[307/543] Writing tensor layers.33.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[308/543] Writing tensor layers.33.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[309/543] Writing tensor layers.33.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[310/543] Writing tensor layers.34.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[311/543] Writing tensor layers.34.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[312/543] Writing tensor layers.34.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[313/543] Writing tensor layers.34.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[314/543] Writing tensor layers.34.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[315/543] Writing tensor layers.34.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[316/543] Writing tensor layers.34.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[317/543] Writing tensor layers.34.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[318/543] Writing tensor layers.34.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[319/543] Writing tensor layers.35.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[320/543] Writing tensor layers.35.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[321/543] Writing tensor layers.35.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[322/543] Writing tensor layers.35.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[323/543] Writing tensor layers.35.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[324/543] Writing tensor layers.35.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[325/543] Writing tensor layers.35.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[326/543] Writing tensor layers.35.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[327/543] Writing tensor layers.35.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[328/543] Writing tensor layers.36.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[329/543] Writing tensor layers.36.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[330/543] Writing tensor layers.36.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[331/543] Writing tensor layers.36.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[332/543] Writing tensor layers.36.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[333/543] Writing tensor layers.36.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[334/543] Writing tensor layers.36.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[335/543] Writing tensor layers.36.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[336/543] Writing tensor layers.36.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[337/543] Writing tensor layers.37.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[338/543] Writing tensor layers.37.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[339/543] Writing tensor layers.37.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[340/543] Writing tensor layers.37.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[341/543] Writing tensor layers.37.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[342/543] Writing tensor layers.37.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[343/543] Writing tensor layers.37.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[344/543] Writing tensor layers.37.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[345/543] Writing tensor layers.37.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[346/543] Writing tensor layers.38.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[347/543] Writing tensor layers.38.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[348/543] Writing tensor layers.38.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[349/543] Writing tensor layers.38.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[350/543] Writing tensor layers.38.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[351/543] Writing tensor layers.38.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[352/543] Writing tensor layers.38.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[353/543] Writing tensor layers.38.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[354/543] Writing tensor layers.38.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[355/543] Writing tensor layers.39.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[356/543] Writing tensor layers.39.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[357/543] Writing tensor layers.39.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[358/543] Writing tensor layers.39.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[359/543] Writing tensor layers.39.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[360/543] Writing tensor layers.39.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[361/543] Writing tensor layers.39.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[362/543] Writing tensor layers.39.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[363/543] Writing tensor layers.39.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[364/543] Writing tensor layers.40.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[365/543] Writing tensor layers.40.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[366/543] Writing tensor layers.40.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[367/543] Writing tensor layers.40.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[368/543] Writing tensor layers.40.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[369/543] Writing tensor layers.40.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[370/543] Writing tensor layers.40.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[371/543] Writing tensor layers.40.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[372/543] Writing tensor layers.40.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[373/543] Writing tensor layers.41.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[374/543] Writing tensor layers.41.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[375/543] Writing tensor layers.41.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[376/543] Writing tensor layers.41.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[377/543] Writing tensor layers.41.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[378/543] Writing tensor layers.41.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[379/543] Writing tensor layers.41.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[380/543] Writing tensor layers.41.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[381/543] Writing tensor layers.41.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[382/543] Writing tensor layers.42.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[383/543] Writing tensor layers.42.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[384/543] Writing tensor layers.42.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[385/543] Writing tensor layers.42.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[386/543] Writing tensor layers.42.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[387/543] Writing tensor layers.42.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[388/543] Writing tensor layers.42.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[389/543] Writing tensor layers.42.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[390/543] Writing tensor layers.42.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[391/543] Writing tensor layers.43.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[392/543] Writing tensor layers.43.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[393/543] Writing tensor layers.43.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[394/543] Writing tensor layers.43.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[395/543] Writing tensor layers.43.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[396/543] Writing tensor layers.43.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[397/543] Writing tensor layers.43.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[398/543] Writing tensor layers.43.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[399/543] Writing tensor layers.43.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[400/543] Writing tensor layers.44.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[401/543] Writing tensor layers.44.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[402/543] Writing tensor layers.44.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[403/543] Writing tensor layers.44.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[404/543] Writing tensor layers.44.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[405/543] Writing tensor layers.44.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[406/543] Writing tensor layers.44.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[407/543] Writing tensor layers.44.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[408/543] Writing tensor layers.44.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[409/543] Writing tensor layers.45.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[410/543] Writing tensor layers.45.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[411/543] Writing tensor layers.45.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[412/543] Writing tensor layers.45.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[413/543] Writing tensor layers.45.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[414/543] Writing tensor layers.45.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[415/543] Writing tensor layers.45.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[416/543] Writing tensor layers.45.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[417/543] Writing tensor layers.45.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[418/543] Writing tensor layers.46.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[419/543] Writing tensor layers.46.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[420/543] Writing tensor layers.46.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[421/543] Writing tensor layers.46.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[422/543] Writing tensor layers.46.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[423/543] Writing tensor layers.46.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[424/543] Writing tensor layers.46.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[425/543] Writing tensor layers.46.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[426/543] Writing tensor layers.46.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[427/543] Writing tensor layers.47.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[428/543] Writing tensor layers.47.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[429/543] Writing tensor layers.47.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[430/543] Writing tensor layers.47.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[431/543] Writing tensor layers.47.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[432/543] Writing tensor layers.47.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[433/543] Writing tensor layers.47.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[434/543] Writing tensor layers.47.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[435/543] Writing tensor layers.47.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[436/543] Writing tensor layers.48.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[437/543] Writing tensor layers.48.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[438/543] Writing tensor layers.48.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[439/543] Writing tensor layers.48.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[440/543] Writing tensor layers.48.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[441/543] Writing tensor layers.48.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[442/543] Writing tensor layers.48.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[443/543] Writing tensor layers.48.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[444/543] Writing tensor layers.48.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[445/543] Writing tensor layers.49.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[446/543] Writing tensor layers.49.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[447/543] Writing tensor layers.49.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[448/543] Writing tensor layers.49.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[449/543] Writing tensor layers.49.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[450/543] Writing tensor layers.49.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[451/543] Writing tensor layers.49.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[452/543] Writing tensor layers.49.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[453/543] Writing tensor layers.49.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[454/543] Writing tensor layers.50.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[455/543] Writing tensor layers.50.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[456/543] Writing tensor layers.50.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[457/543] Writing tensor layers.50.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[458/543] Writing tensor layers.50.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[459/543] Writing tensor layers.50.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[460/543] Writing tensor layers.50.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[461/543] Writing tensor layers.50.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[462/543] Writing tensor layers.50.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[463/543] Writing tensor layers.51.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[464/543] Writing tensor layers.51.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[465/543] Writing tensor layers.51.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[466/543] Writing tensor layers.51.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[467/543] Writing tensor layers.51.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[468/543] Writing tensor layers.51.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[469/543] Writing tensor layers.51.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[470/543] Writing tensor layers.51.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[471/543] Writing tensor layers.51.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[472/543] Writing tensor layers.52.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[473/543] Writing tensor layers.52.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[474/543] Writing tensor layers.52.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[475/543] Writing tensor layers.52.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[476/543] Writing tensor layers.52.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[477/543] Writing tensor layers.52.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[478/543] Writing tensor layers.52.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[479/543] Writing tensor layers.52.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[480/543] Writing tensor layers.52.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[481/543] Writing tensor layers.53.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[482/543] Writing tensor layers.53.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[483/543] Writing tensor layers.53.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[484/543] Writing tensor layers.53.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[485/543] Writing tensor layers.53.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[486/543] Writing tensor layers.53.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[487/543] Writing tensor layers.53.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[488/543] Writing tensor layers.53.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[489/543] Writing tensor layers.53.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[490/543] Writing tensor layers.54.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[491/543] Writing tensor layers.54.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[492/543] Writing tensor layers.54.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[493/543] Writing tensor layers.54.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[494/543] Writing tensor layers.54.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[495/543] Writing tensor layers.54.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[496/543] Writing tensor layers.54.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[497/543] Writing tensor layers.54.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[498/543] Writing tensor layers.54.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[499/543] Writing tensor layers.55.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[500/543] Writing tensor layers.55.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[501/543] Writing tensor layers.55.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[502/543] Writing tensor layers.55.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[503/543] Writing tensor layers.55.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[504/543] Writing tensor layers.55.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[505/543] Writing tensor layers.55.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[506/543] Writing tensor layers.55.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[507/543] Writing tensor layers.55.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[508/543] Writing tensor layers.56.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[509/543] Writing tensor layers.56.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[510/543] Writing tensor layers.56.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[511/543] Writing tensor layers.56.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[512/543] Writing tensor layers.56.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[513/543] Writing tensor layers.56.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[514/543] Writing tensor layers.56.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[515/543] Writing tensor layers.56.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[516/543] Writing tensor layers.56.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[517/543] Writing tensor layers.57.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[518/543] Writing tensor layers.57.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[519/543] Writing tensor layers.57.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[520/543] Writing tensor layers.57.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[521/543] Writing tensor layers.57.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[522/543] Writing tensor layers.57.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[523/543] Writing tensor layers.57.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[524/543] Writing tensor layers.57.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[525/543] Writing tensor layers.57.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[526/543] Writing tensor layers.58.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[527/543] Writing tensor layers.58.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[528/543] Writing tensor layers.58.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[529/543] Writing tensor layers.58.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[530/543] Writing tensor layers.58.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[531/543] Writing tensor layers.58.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[532/543] Writing tensor layers.58.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[533/543] Writing tensor layers.58.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[534/543] Writing tensor layers.58.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[535/543] Writing tensor layers.59.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[536/543] Writing tensor layers.59.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[537/543] Writing tensor layers.59.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[538/543] Writing tensor layers.59.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[539/543] Writing tensor layers.59.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[540/543] Writing tensor layers.59.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[541/543] Writing tensor layers.59.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[542/543] Writing tensor layers.59.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[543/543] Writing tensor layers.59.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/30B/ggml-model-f16.bin\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:8192 n_mult:256 n_head:64 n_layer:80\n",
      "Writing vocab...\n",
      "[  1/723] Writing tensor tok_embeddings.weight                  | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  2/723] Writing tensor norm.weight                            | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  4/723] Writing tensor layers.0.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  5/723] Writing tensor layers.0.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  6/723] Writing tensor layers.0.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  7/723] Writing tensor layers.0.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  8/723] Writing tensor layers.0.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  9/723] Writing tensor layers.0.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 10/723] Writing tensor layers.0.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 11/723] Writing tensor layers.0.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 12/723] Writing tensor layers.0.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 13/723] Writing tensor layers.1.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 14/723] Writing tensor layers.1.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 15/723] Writing tensor layers.1.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 16/723] Writing tensor layers.1.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 17/723] Writing tensor layers.1.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 18/723] Writing tensor layers.1.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 19/723] Writing tensor layers.1.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 20/723] Writing tensor layers.1.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 21/723] Writing tensor layers.1.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 22/723] Writing tensor layers.2.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 23/723] Writing tensor layers.2.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 24/723] Writing tensor layers.2.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 25/723] Writing tensor layers.2.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 26/723] Writing tensor layers.2.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 27/723] Writing tensor layers.2.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 28/723] Writing tensor layers.2.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 29/723] Writing tensor layers.2.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 30/723] Writing tensor layers.2.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 31/723] Writing tensor layers.3.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 32/723] Writing tensor layers.3.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 33/723] Writing tensor layers.3.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 34/723] Writing tensor layers.3.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 35/723] Writing tensor layers.3.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 36/723] Writing tensor layers.3.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 37/723] Writing tensor layers.3.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 38/723] Writing tensor layers.3.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 39/723] Writing tensor layers.3.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 40/723] Writing tensor layers.4.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 41/723] Writing tensor layers.4.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 42/723] Writing tensor layers.4.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 43/723] Writing tensor layers.4.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 44/723] Writing tensor layers.4.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 45/723] Writing tensor layers.4.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 46/723] Writing tensor layers.4.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 47/723] Writing tensor layers.4.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 48/723] Writing tensor layers.4.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 49/723] Writing tensor layers.5.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 50/723] Writing tensor layers.5.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 51/723] Writing tensor layers.5.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 52/723] Writing tensor layers.5.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 53/723] Writing tensor layers.5.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 54/723] Writing tensor layers.5.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 55/723] Writing tensor layers.5.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 56/723] Writing tensor layers.5.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 57/723] Writing tensor layers.5.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 58/723] Writing tensor layers.6.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 59/723] Writing tensor layers.6.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 60/723] Writing tensor layers.6.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 61/723] Writing tensor layers.6.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 62/723] Writing tensor layers.6.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 63/723] Writing tensor layers.6.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 64/723] Writing tensor layers.6.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 65/723] Writing tensor layers.6.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 66/723] Writing tensor layers.6.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 67/723] Writing tensor layers.7.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 68/723] Writing tensor layers.7.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 69/723] Writing tensor layers.7.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 70/723] Writing tensor layers.7.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 71/723] Writing tensor layers.7.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 72/723] Writing tensor layers.7.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 73/723] Writing tensor layers.7.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 74/723] Writing tensor layers.7.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 75/723] Writing tensor layers.7.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 76/723] Writing tensor layers.8.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 77/723] Writing tensor layers.8.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 78/723] Writing tensor layers.8.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 79/723] Writing tensor layers.8.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 80/723] Writing tensor layers.8.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 81/723] Writing tensor layers.8.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 82/723] Writing tensor layers.8.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 83/723] Writing tensor layers.8.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 84/723] Writing tensor layers.8.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 85/723] Writing tensor layers.9.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 86/723] Writing tensor layers.9.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 87/723] Writing tensor layers.9.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 88/723] Writing tensor layers.9.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 89/723] Writing tensor layers.9.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 90/723] Writing tensor layers.9.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 91/723] Writing tensor layers.9.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 92/723] Writing tensor layers.9.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 93/723] Writing tensor layers.9.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 94/723] Writing tensor layers.10.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 95/723] Writing tensor layers.10.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 96/723] Writing tensor layers.10.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 97/723] Writing tensor layers.10.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 98/723] Writing tensor layers.10.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 99/723] Writing tensor layers.10.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[100/723] Writing tensor layers.10.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[101/723] Writing tensor layers.10.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[102/723] Writing tensor layers.10.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[103/723] Writing tensor layers.11.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[104/723] Writing tensor layers.11.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[105/723] Writing tensor layers.11.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[106/723] Writing tensor layers.11.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[107/723] Writing tensor layers.11.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[108/723] Writing tensor layers.11.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[109/723] Writing tensor layers.11.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[110/723] Writing tensor layers.11.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[111/723] Writing tensor layers.11.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[112/723] Writing tensor layers.12.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[113/723] Writing tensor layers.12.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[114/723] Writing tensor layers.12.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[115/723] Writing tensor layers.12.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[116/723] Writing tensor layers.12.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[117/723] Writing tensor layers.12.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[118/723] Writing tensor layers.12.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[119/723] Writing tensor layers.12.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[120/723] Writing tensor layers.12.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[121/723] Writing tensor layers.13.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[122/723] Writing tensor layers.13.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[123/723] Writing tensor layers.13.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[124/723] Writing tensor layers.13.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[125/723] Writing tensor layers.13.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[126/723] Writing tensor layers.13.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[127/723] Writing tensor layers.13.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[128/723] Writing tensor layers.13.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[129/723] Writing tensor layers.13.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[130/723] Writing tensor layers.14.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[131/723] Writing tensor layers.14.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[132/723] Writing tensor layers.14.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[133/723] Writing tensor layers.14.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[134/723] Writing tensor layers.14.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[135/723] Writing tensor layers.14.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[136/723] Writing tensor layers.14.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[137/723] Writing tensor layers.14.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[138/723] Writing tensor layers.14.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[139/723] Writing tensor layers.15.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[140/723] Writing tensor layers.15.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[141/723] Writing tensor layers.15.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[142/723] Writing tensor layers.15.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[143/723] Writing tensor layers.15.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[144/723] Writing tensor layers.15.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[145/723] Writing tensor layers.15.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[146/723] Writing tensor layers.15.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[147/723] Writing tensor layers.15.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[148/723] Writing tensor layers.16.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[149/723] Writing tensor layers.16.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[150/723] Writing tensor layers.16.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[151/723] Writing tensor layers.16.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[152/723] Writing tensor layers.16.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[153/723] Writing tensor layers.16.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[154/723] Writing tensor layers.16.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[155/723] Writing tensor layers.16.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[156/723] Writing tensor layers.16.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[157/723] Writing tensor layers.17.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[158/723] Writing tensor layers.17.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[159/723] Writing tensor layers.17.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[160/723] Writing tensor layers.17.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[161/723] Writing tensor layers.17.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[162/723] Writing tensor layers.17.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[163/723] Writing tensor layers.17.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[164/723] Writing tensor layers.17.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[165/723] Writing tensor layers.17.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[166/723] Writing tensor layers.18.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[167/723] Writing tensor layers.18.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[168/723] Writing tensor layers.18.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[169/723] Writing tensor layers.18.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[170/723] Writing tensor layers.18.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[171/723] Writing tensor layers.18.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[172/723] Writing tensor layers.18.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[173/723] Writing tensor layers.18.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[174/723] Writing tensor layers.18.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[175/723] Writing tensor layers.19.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[176/723] Writing tensor layers.19.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[177/723] Writing tensor layers.19.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[178/723] Writing tensor layers.19.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[179/723] Writing tensor layers.19.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[180/723] Writing tensor layers.19.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[181/723] Writing tensor layers.19.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[182/723] Writing tensor layers.19.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[183/723] Writing tensor layers.19.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[184/723] Writing tensor layers.20.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[185/723] Writing tensor layers.20.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[186/723] Writing tensor layers.20.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[187/723] Writing tensor layers.20.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[188/723] Writing tensor layers.20.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[189/723] Writing tensor layers.20.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[190/723] Writing tensor layers.20.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[191/723] Writing tensor layers.20.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[192/723] Writing tensor layers.20.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[193/723] Writing tensor layers.21.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[194/723] Writing tensor layers.21.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[195/723] Writing tensor layers.21.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[196/723] Writing tensor layers.21.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[197/723] Writing tensor layers.21.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[198/723] Writing tensor layers.21.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[199/723] Writing tensor layers.21.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[200/723] Writing tensor layers.21.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[201/723] Writing tensor layers.21.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[202/723] Writing tensor layers.22.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[203/723] Writing tensor layers.22.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[204/723] Writing tensor layers.22.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[205/723] Writing tensor layers.22.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[206/723] Writing tensor layers.22.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[207/723] Writing tensor layers.22.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[208/723] Writing tensor layers.22.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[209/723] Writing tensor layers.22.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[210/723] Writing tensor layers.22.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[211/723] Writing tensor layers.23.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[212/723] Writing tensor layers.23.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[213/723] Writing tensor layers.23.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[214/723] Writing tensor layers.23.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[215/723] Writing tensor layers.23.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[216/723] Writing tensor layers.23.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[217/723] Writing tensor layers.23.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[218/723] Writing tensor layers.23.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[219/723] Writing tensor layers.23.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[220/723] Writing tensor layers.24.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[221/723] Writing tensor layers.24.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[222/723] Writing tensor layers.24.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[223/723] Writing tensor layers.24.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[224/723] Writing tensor layers.24.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[225/723] Writing tensor layers.24.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[226/723] Writing tensor layers.24.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[227/723] Writing tensor layers.24.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[228/723] Writing tensor layers.24.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[229/723] Writing tensor layers.25.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[230/723] Writing tensor layers.25.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[231/723] Writing tensor layers.25.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[232/723] Writing tensor layers.25.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[233/723] Writing tensor layers.25.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[234/723] Writing tensor layers.25.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[235/723] Writing tensor layers.25.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[236/723] Writing tensor layers.25.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[237/723] Writing tensor layers.25.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[238/723] Writing tensor layers.26.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[239/723] Writing tensor layers.26.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[240/723] Writing tensor layers.26.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[241/723] Writing tensor layers.26.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[242/723] Writing tensor layers.26.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[243/723] Writing tensor layers.26.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[244/723] Writing tensor layers.26.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[245/723] Writing tensor layers.26.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[246/723] Writing tensor layers.26.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[247/723] Writing tensor layers.27.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[248/723] Writing tensor layers.27.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[249/723] Writing tensor layers.27.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[250/723] Writing tensor layers.27.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[251/723] Writing tensor layers.27.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[252/723] Writing tensor layers.27.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[253/723] Writing tensor layers.27.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[254/723] Writing tensor layers.27.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[255/723] Writing tensor layers.27.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[256/723] Writing tensor layers.28.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[257/723] Writing tensor layers.28.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[258/723] Writing tensor layers.28.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[259/723] Writing tensor layers.28.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[260/723] Writing tensor layers.28.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[261/723] Writing tensor layers.28.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[262/723] Writing tensor layers.28.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[263/723] Writing tensor layers.28.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[264/723] Writing tensor layers.28.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[265/723] Writing tensor layers.29.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[266/723] Writing tensor layers.29.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[267/723] Writing tensor layers.29.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[268/723] Writing tensor layers.29.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[269/723] Writing tensor layers.29.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[270/723] Writing tensor layers.29.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[271/723] Writing tensor layers.29.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[272/723] Writing tensor layers.29.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[273/723] Writing tensor layers.29.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[274/723] Writing tensor layers.30.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[275/723] Writing tensor layers.30.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[276/723] Writing tensor layers.30.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[277/723] Writing tensor layers.30.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[278/723] Writing tensor layers.30.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[279/723] Writing tensor layers.30.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[280/723] Writing tensor layers.30.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[281/723] Writing tensor layers.30.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[282/723] Writing tensor layers.30.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[283/723] Writing tensor layers.31.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[284/723] Writing tensor layers.31.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[285/723] Writing tensor layers.31.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[286/723] Writing tensor layers.31.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[287/723] Writing tensor layers.31.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[288/723] Writing tensor layers.31.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[289/723] Writing tensor layers.31.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[290/723] Writing tensor layers.31.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[291/723] Writing tensor layers.31.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[292/723] Writing tensor layers.32.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[293/723] Writing tensor layers.32.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[294/723] Writing tensor layers.32.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[295/723] Writing tensor layers.32.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[296/723] Writing tensor layers.32.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[297/723] Writing tensor layers.32.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[298/723] Writing tensor layers.32.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[299/723] Writing tensor layers.32.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[300/723] Writing tensor layers.32.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[301/723] Writing tensor layers.33.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[302/723] Writing tensor layers.33.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[303/723] Writing tensor layers.33.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[304/723] Writing tensor layers.33.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[305/723] Writing tensor layers.33.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[306/723] Writing tensor layers.33.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[307/723] Writing tensor layers.33.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[308/723] Writing tensor layers.33.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[309/723] Writing tensor layers.33.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[310/723] Writing tensor layers.34.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[311/723] Writing tensor layers.34.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[312/723] Writing tensor layers.34.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[313/723] Writing tensor layers.34.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[314/723] Writing tensor layers.34.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[315/723] Writing tensor layers.34.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[316/723] Writing tensor layers.34.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[317/723] Writing tensor layers.34.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[318/723] Writing tensor layers.34.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[319/723] Writing tensor layers.35.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[320/723] Writing tensor layers.35.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[321/723] Writing tensor layers.35.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[322/723] Writing tensor layers.35.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[323/723] Writing tensor layers.35.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[324/723] Writing tensor layers.35.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[325/723] Writing tensor layers.35.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[326/723] Writing tensor layers.35.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[327/723] Writing tensor layers.35.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[328/723] Writing tensor layers.36.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[329/723] Writing tensor layers.36.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[330/723] Writing tensor layers.36.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[331/723] Writing tensor layers.36.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[332/723] Writing tensor layers.36.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[333/723] Writing tensor layers.36.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[334/723] Writing tensor layers.36.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[335/723] Writing tensor layers.36.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[336/723] Writing tensor layers.36.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[337/723] Writing tensor layers.37.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[338/723] Writing tensor layers.37.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[339/723] Writing tensor layers.37.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[340/723] Writing tensor layers.37.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[341/723] Writing tensor layers.37.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[342/723] Writing tensor layers.37.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[343/723] Writing tensor layers.37.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[344/723] Writing tensor layers.37.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[345/723] Writing tensor layers.37.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[346/723] Writing tensor layers.38.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[347/723] Writing tensor layers.38.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[348/723] Writing tensor layers.38.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[349/723] Writing tensor layers.38.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[350/723] Writing tensor layers.38.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[351/723] Writing tensor layers.38.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[352/723] Writing tensor layers.38.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[353/723] Writing tensor layers.38.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[354/723] Writing tensor layers.38.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[355/723] Writing tensor layers.39.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[356/723] Writing tensor layers.39.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[357/723] Writing tensor layers.39.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[358/723] Writing tensor layers.39.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[359/723] Writing tensor layers.39.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[360/723] Writing tensor layers.39.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[361/723] Writing tensor layers.39.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[362/723] Writing tensor layers.39.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[363/723] Writing tensor layers.39.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[364/723] Writing tensor layers.40.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[365/723] Writing tensor layers.40.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[366/723] Writing tensor layers.40.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[367/723] Writing tensor layers.40.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[368/723] Writing tensor layers.40.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[369/723] Writing tensor layers.40.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[370/723] Writing tensor layers.40.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[371/723] Writing tensor layers.40.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[372/723] Writing tensor layers.40.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[373/723] Writing tensor layers.41.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[374/723] Writing tensor layers.41.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[375/723] Writing tensor layers.41.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[376/723] Writing tensor layers.41.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[377/723] Writing tensor layers.41.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[378/723] Writing tensor layers.41.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[379/723] Writing tensor layers.41.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[380/723] Writing tensor layers.41.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[381/723] Writing tensor layers.41.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[382/723] Writing tensor layers.42.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[383/723] Writing tensor layers.42.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[384/723] Writing tensor layers.42.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[385/723] Writing tensor layers.42.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[386/723] Writing tensor layers.42.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[387/723] Writing tensor layers.42.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[388/723] Writing tensor layers.42.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[389/723] Writing tensor layers.42.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[390/723] Writing tensor layers.42.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[391/723] Writing tensor layers.43.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[392/723] Writing tensor layers.43.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[393/723] Writing tensor layers.43.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[394/723] Writing tensor layers.43.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[395/723] Writing tensor layers.43.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[396/723] Writing tensor layers.43.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[397/723] Writing tensor layers.43.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[398/723] Writing tensor layers.43.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[399/723] Writing tensor layers.43.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[400/723] Writing tensor layers.44.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[401/723] Writing tensor layers.44.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[402/723] Writing tensor layers.44.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[403/723] Writing tensor layers.44.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[404/723] Writing tensor layers.44.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[405/723] Writing tensor layers.44.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[406/723] Writing tensor layers.44.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[407/723] Writing tensor layers.44.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[408/723] Writing tensor layers.44.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[409/723] Writing tensor layers.45.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[410/723] Writing tensor layers.45.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[411/723] Writing tensor layers.45.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[412/723] Writing tensor layers.45.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[413/723] Writing tensor layers.45.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[414/723] Writing tensor layers.45.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[415/723] Writing tensor layers.45.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[416/723] Writing tensor layers.45.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[417/723] Writing tensor layers.45.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[418/723] Writing tensor layers.46.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[419/723] Writing tensor layers.46.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[420/723] Writing tensor layers.46.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[421/723] Writing tensor layers.46.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[422/723] Writing tensor layers.46.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[423/723] Writing tensor layers.46.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[424/723] Writing tensor layers.46.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[425/723] Writing tensor layers.46.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[426/723] Writing tensor layers.46.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[427/723] Writing tensor layers.47.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[428/723] Writing tensor layers.47.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[429/723] Writing tensor layers.47.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[430/723] Writing tensor layers.47.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[431/723] Writing tensor layers.47.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[432/723] Writing tensor layers.47.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[433/723] Writing tensor layers.47.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[434/723] Writing tensor layers.47.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[435/723] Writing tensor layers.47.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[436/723] Writing tensor layers.48.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[437/723] Writing tensor layers.48.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[438/723] Writing tensor layers.48.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[439/723] Writing tensor layers.48.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[440/723] Writing tensor layers.48.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[441/723] Writing tensor layers.48.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[442/723] Writing tensor layers.48.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[443/723] Writing tensor layers.48.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[444/723] Writing tensor layers.48.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[445/723] Writing tensor layers.49.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[446/723] Writing tensor layers.49.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[447/723] Writing tensor layers.49.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[448/723] Writing tensor layers.49.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[449/723] Writing tensor layers.49.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[450/723] Writing tensor layers.49.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[451/723] Writing tensor layers.49.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[452/723] Writing tensor layers.49.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[453/723] Writing tensor layers.49.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[454/723] Writing tensor layers.50.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[455/723] Writing tensor layers.50.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[456/723] Writing tensor layers.50.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[457/723] Writing tensor layers.50.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[458/723] Writing tensor layers.50.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[459/723] Writing tensor layers.50.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[460/723] Writing tensor layers.50.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[461/723] Writing tensor layers.50.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[462/723] Writing tensor layers.50.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[463/723] Writing tensor layers.51.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[464/723] Writing tensor layers.51.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[465/723] Writing tensor layers.51.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[466/723] Writing tensor layers.51.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[467/723] Writing tensor layers.51.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[468/723] Writing tensor layers.51.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[469/723] Writing tensor layers.51.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[470/723] Writing tensor layers.51.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[471/723] Writing tensor layers.51.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[472/723] Writing tensor layers.52.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[473/723] Writing tensor layers.52.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[474/723] Writing tensor layers.52.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[475/723] Writing tensor layers.52.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[476/723] Writing tensor layers.52.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[477/723] Writing tensor layers.52.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[478/723] Writing tensor layers.52.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[479/723] Writing tensor layers.52.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[480/723] Writing tensor layers.52.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[481/723] Writing tensor layers.53.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[482/723] Writing tensor layers.53.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[483/723] Writing tensor layers.53.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[484/723] Writing tensor layers.53.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[485/723] Writing tensor layers.53.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[486/723] Writing tensor layers.53.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[487/723] Writing tensor layers.53.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[488/723] Writing tensor layers.53.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[489/723] Writing tensor layers.53.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[490/723] Writing tensor layers.54.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[491/723] Writing tensor layers.54.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[492/723] Writing tensor layers.54.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[493/723] Writing tensor layers.54.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[494/723] Writing tensor layers.54.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[495/723] Writing tensor layers.54.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[496/723] Writing tensor layers.54.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[497/723] Writing tensor layers.54.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[498/723] Writing tensor layers.54.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[499/723] Writing tensor layers.55.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[500/723] Writing tensor layers.55.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[501/723] Writing tensor layers.55.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[502/723] Writing tensor layers.55.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[503/723] Writing tensor layers.55.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[504/723] Writing tensor layers.55.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[505/723] Writing tensor layers.55.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[506/723] Writing tensor layers.55.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[507/723] Writing tensor layers.55.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[508/723] Writing tensor layers.56.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[509/723] Writing tensor layers.56.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[510/723] Writing tensor layers.56.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[511/723] Writing tensor layers.56.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[512/723] Writing tensor layers.56.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[513/723] Writing tensor layers.56.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[514/723] Writing tensor layers.56.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[515/723] Writing tensor layers.56.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[516/723] Writing tensor layers.56.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[517/723] Writing tensor layers.57.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[518/723] Writing tensor layers.57.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[519/723] Writing tensor layers.57.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[520/723] Writing tensor layers.57.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[521/723] Writing tensor layers.57.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[522/723] Writing tensor layers.57.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[523/723] Writing tensor layers.57.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[524/723] Writing tensor layers.57.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[525/723] Writing tensor layers.57.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[526/723] Writing tensor layers.58.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[527/723] Writing tensor layers.58.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[528/723] Writing tensor layers.58.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[529/723] Writing tensor layers.58.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[530/723] Writing tensor layers.58.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[531/723] Writing tensor layers.58.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[532/723] Writing tensor layers.58.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[533/723] Writing tensor layers.58.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[534/723] Writing tensor layers.58.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[535/723] Writing tensor layers.59.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[536/723] Writing tensor layers.59.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[537/723] Writing tensor layers.59.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[538/723] Writing tensor layers.59.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[539/723] Writing tensor layers.59.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[540/723] Writing tensor layers.59.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[541/723] Writing tensor layers.59.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[542/723] Writing tensor layers.59.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[543/723] Writing tensor layers.59.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[544/723] Writing tensor layers.60.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[545/723] Writing tensor layers.60.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[546/723] Writing tensor layers.60.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[547/723] Writing tensor layers.60.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[548/723] Writing tensor layers.60.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[549/723] Writing tensor layers.60.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[550/723] Writing tensor layers.60.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[551/723] Writing tensor layers.60.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[552/723] Writing tensor layers.60.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[553/723] Writing tensor layers.61.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[554/723] Writing tensor layers.61.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[555/723] Writing tensor layers.61.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[556/723] Writing tensor layers.61.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[557/723] Writing tensor layers.61.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[558/723] Writing tensor layers.61.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[559/723] Writing tensor layers.61.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[560/723] Writing tensor layers.61.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[561/723] Writing tensor layers.61.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[562/723] Writing tensor layers.62.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[563/723] Writing tensor layers.62.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[564/723] Writing tensor layers.62.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[565/723] Writing tensor layers.62.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[566/723] Writing tensor layers.62.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[567/723] Writing tensor layers.62.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[568/723] Writing tensor layers.62.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[569/723] Writing tensor layers.62.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[570/723] Writing tensor layers.62.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[571/723] Writing tensor layers.63.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[572/723] Writing tensor layers.63.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[573/723] Writing tensor layers.63.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[574/723] Writing tensor layers.63.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[575/723] Writing tensor layers.63.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[576/723] Writing tensor layers.63.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[577/723] Writing tensor layers.63.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[578/723] Writing tensor layers.63.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[579/723] Writing tensor layers.63.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[580/723] Writing tensor layers.64.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[581/723] Writing tensor layers.64.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[582/723] Writing tensor layers.64.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[583/723] Writing tensor layers.64.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[584/723] Writing tensor layers.64.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[585/723] Writing tensor layers.64.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[586/723] Writing tensor layers.64.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[587/723] Writing tensor layers.64.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[588/723] Writing tensor layers.64.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[589/723] Writing tensor layers.65.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[590/723] Writing tensor layers.65.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[591/723] Writing tensor layers.65.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[592/723] Writing tensor layers.65.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[593/723] Writing tensor layers.65.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[594/723] Writing tensor layers.65.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[595/723] Writing tensor layers.65.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[596/723] Writing tensor layers.65.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[597/723] Writing tensor layers.65.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[598/723] Writing tensor layers.66.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[599/723] Writing tensor layers.66.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[600/723] Writing tensor layers.66.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[601/723] Writing tensor layers.66.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[602/723] Writing tensor layers.66.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[603/723] Writing tensor layers.66.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[604/723] Writing tensor layers.66.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[605/723] Writing tensor layers.66.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[606/723] Writing tensor layers.66.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[607/723] Writing tensor layers.67.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[608/723] Writing tensor layers.67.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[609/723] Writing tensor layers.67.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[610/723] Writing tensor layers.67.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[611/723] Writing tensor layers.67.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[612/723] Writing tensor layers.67.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[613/723] Writing tensor layers.67.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[614/723] Writing tensor layers.67.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[615/723] Writing tensor layers.67.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[616/723] Writing tensor layers.68.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[617/723] Writing tensor layers.68.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[618/723] Writing tensor layers.68.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[619/723] Writing tensor layers.68.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[620/723] Writing tensor layers.68.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[621/723] Writing tensor layers.68.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[622/723] Writing tensor layers.68.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[623/723] Writing tensor layers.68.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[624/723] Writing tensor layers.68.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[625/723] Writing tensor layers.69.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[626/723] Writing tensor layers.69.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[627/723] Writing tensor layers.69.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[628/723] Writing tensor layers.69.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[629/723] Writing tensor layers.69.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[630/723] Writing tensor layers.69.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[631/723] Writing tensor layers.69.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[632/723] Writing tensor layers.69.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[633/723] Writing tensor layers.69.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[634/723] Writing tensor layers.70.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[635/723] Writing tensor layers.70.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[636/723] Writing tensor layers.70.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[637/723] Writing tensor layers.70.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[638/723] Writing tensor layers.70.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[639/723] Writing tensor layers.70.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[640/723] Writing tensor layers.70.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[641/723] Writing tensor layers.70.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[642/723] Writing tensor layers.70.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[643/723] Writing tensor layers.71.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[644/723] Writing tensor layers.71.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[645/723] Writing tensor layers.71.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[646/723] Writing tensor layers.71.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[647/723] Writing tensor layers.71.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[648/723] Writing tensor layers.71.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[649/723] Writing tensor layers.71.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[650/723] Writing tensor layers.71.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[651/723] Writing tensor layers.71.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[652/723] Writing tensor layers.72.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[653/723] Writing tensor layers.72.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[654/723] Writing tensor layers.72.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[655/723] Writing tensor layers.72.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[656/723] Writing tensor layers.72.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[657/723] Writing tensor layers.72.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[658/723] Writing tensor layers.72.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[659/723] Writing tensor layers.72.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[660/723] Writing tensor layers.72.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[661/723] Writing tensor layers.73.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[662/723] Writing tensor layers.73.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[663/723] Writing tensor layers.73.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[664/723] Writing tensor layers.73.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[665/723] Writing tensor layers.73.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[666/723] Writing tensor layers.73.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[667/723] Writing tensor layers.73.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[668/723] Writing tensor layers.73.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[669/723] Writing tensor layers.73.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[670/723] Writing tensor layers.74.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[671/723] Writing tensor layers.74.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[672/723] Writing tensor layers.74.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[673/723] Writing tensor layers.74.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[674/723] Writing tensor layers.74.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[675/723] Writing tensor layers.74.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[676/723] Writing tensor layers.74.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[677/723] Writing tensor layers.74.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[678/723] Writing tensor layers.74.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[679/723] Writing tensor layers.75.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[680/723] Writing tensor layers.75.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[681/723] Writing tensor layers.75.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[682/723] Writing tensor layers.75.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[683/723] Writing tensor layers.75.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[684/723] Writing tensor layers.75.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[685/723] Writing tensor layers.75.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[686/723] Writing tensor layers.75.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[687/723] Writing tensor layers.75.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[688/723] Writing tensor layers.76.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[689/723] Writing tensor layers.76.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[690/723] Writing tensor layers.76.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[691/723] Writing tensor layers.76.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[692/723] Writing tensor layers.76.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[693/723] Writing tensor layers.76.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[694/723] Writing tensor layers.76.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[695/723] Writing tensor layers.76.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[696/723] Writing tensor layers.76.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[697/723] Writing tensor layers.77.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[698/723] Writing tensor layers.77.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[699/723] Writing tensor layers.77.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[700/723] Writing tensor layers.77.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[701/723] Writing tensor layers.77.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[702/723] Writing tensor layers.77.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[703/723] Writing tensor layers.77.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[704/723] Writing tensor layers.77.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[705/723] Writing tensor layers.77.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[706/723] Writing tensor layers.78.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[707/723] Writing tensor layers.78.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[708/723] Writing tensor layers.78.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[709/723] Writing tensor layers.78.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[710/723] Writing tensor layers.78.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[711/723] Writing tensor layers.78.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[712/723] Writing tensor layers.78.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[713/723] Writing tensor layers.78.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[714/723] Writing tensor layers.78.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[715/723] Writing tensor layers.79.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[716/723] Writing tensor layers.79.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[717/723] Writing tensor layers.79.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[718/723] Writing tensor layers.79.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[719/723] Writing tensor layers.79.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[720/723] Writing tensor layers.79.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[721/723] Writing tensor layers.79.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[722/723] Writing tensor layers.79.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[723/723] Writing tensor layers.79.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/65B/ggml-model-f16.bin\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/7B/ggml-model-q4_0.bin\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->   102.54 MB | hist: \n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 56352.62 ms\n",
      "main:    total time = 56352.62 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/13B/ggml-model-f16.bin' to './models/13B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/13B/ggml-model-q4_0.bin\n",
      "[   1/ 363]                tok_embeddings.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->    87.89 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                          norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->   128.17 MB | hist: \n",
      "[   4/ 363]         layers.0.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]         layers.0.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]         layers.0.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]         layers.0.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]       layers.0.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   9/ 363]      layers.0.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 363]      layers.0.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 363]      layers.0.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 363]             layers.0.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  13/ 363]         layers.1.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]         layers.1.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]         layers.1.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]         layers.1.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]       layers.1.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  18/ 363]      layers.1.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]      layers.1.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]      layers.1.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 363]             layers.1.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  22/ 363]         layers.2.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]         layers.2.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]         layers.2.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]         layers.2.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]       layers.2.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  27/ 363]      layers.2.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]      layers.2.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]      layers.2.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 363]             layers.2.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  31/ 363]         layers.3.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]         layers.3.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]         layers.3.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]         layers.3.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]       layers.3.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  36/ 363]      layers.3.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]      layers.3.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]      layers.3.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 363]             layers.3.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  40/ 363]         layers.4.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]         layers.4.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]         layers.4.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]         layers.4.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]       layers.4.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  45/ 363]      layers.4.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]      layers.4.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]      layers.4.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 363]             layers.4.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  49/ 363]         layers.5.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]         layers.5.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]         layers.5.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]         layers.5.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]       layers.5.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  54/ 363]      layers.5.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]      layers.5.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]      layers.5.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 363]             layers.5.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  58/ 363]         layers.6.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]         layers.6.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]         layers.6.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]         layers.6.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]       layers.6.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  63/ 363]      layers.6.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 363]      layers.6.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 363]      layers.6.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 363]             layers.6.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  67/ 363]         layers.7.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]         layers.7.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]         layers.7.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]         layers.7.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]       layers.7.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  72/ 363]      layers.7.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 363]      layers.7.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 363]      layers.7.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 363]             layers.7.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  76/ 363]         layers.8.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]         layers.8.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]         layers.8.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]         layers.8.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]       layers.8.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  81/ 363]      layers.8.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]      layers.8.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]      layers.8.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 363]             layers.8.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  85/ 363]         layers.9.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]         layers.9.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]         layers.9.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]         layers.9.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]       layers.9.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  90/ 363]      layers.9.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 363]      layers.9.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 363]      layers.9.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 363]             layers.9.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  94/ 363]        layers.10.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]        layers.10.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]        layers.10.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]        layers.10.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]      layers.10.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  99/ 363]     layers.10.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 363]     layers.10.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 363]     layers.10.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 363]            layers.10.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]        layers.11.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]        layers.11.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]        layers.11.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]        layers.11.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]      layers.11.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 108/ 363]     layers.11.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 363]     layers.11.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 363]     layers.11.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 363]            layers.11.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]        layers.12.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]        layers.12.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]        layers.12.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]        layers.12.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]      layers.12.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 117/ 363]     layers.12.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 363]     layers.12.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 363]     layers.12.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 363]            layers.12.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]        layers.13.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]        layers.13.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]        layers.13.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]        layers.13.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]      layers.13.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 126/ 363]     layers.13.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 363]     layers.13.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 363]     layers.13.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 363]            layers.13.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]        layers.14.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]        layers.14.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]        layers.14.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]        layers.14.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]      layers.14.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 135/ 363]     layers.14.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 363]     layers.14.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 363]     layers.14.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 363]            layers.14.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]        layers.15.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]        layers.15.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]        layers.15.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]        layers.15.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]      layers.15.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 144/ 363]     layers.15.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]     layers.15.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]     layers.15.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 363]            layers.15.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]        layers.16.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]        layers.16.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]        layers.16.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]        layers.16.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]      layers.16.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 153/ 363]     layers.16.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 363]     layers.16.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 363]     layers.16.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 363]            layers.16.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]        layers.17.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]        layers.17.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]        layers.17.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]        layers.17.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]      layers.17.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 162/ 363]     layers.17.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]     layers.17.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]     layers.17.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 363]            layers.17.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]        layers.18.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]        layers.18.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]        layers.18.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]        layers.18.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]      layers.18.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 171/ 363]     layers.18.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]     layers.18.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]     layers.18.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 363]            layers.18.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]        layers.19.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]        layers.19.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]        layers.19.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]        layers.19.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]      layers.19.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 180/ 363]     layers.19.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]     layers.19.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]     layers.19.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 363]            layers.19.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]        layers.20.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]        layers.20.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]        layers.20.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]        layers.20.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]      layers.20.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 189/ 363]     layers.20.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]     layers.20.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]     layers.20.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 363]            layers.20.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]        layers.21.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]        layers.21.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]        layers.21.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]        layers.21.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]      layers.21.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 198/ 363]     layers.21.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]     layers.21.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]     layers.21.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 363]            layers.21.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]        layers.22.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]        layers.22.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]        layers.22.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]        layers.22.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]      layers.22.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 207/ 363]     layers.22.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]     layers.22.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]     layers.22.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 363]            layers.22.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]        layers.23.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]        layers.23.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]        layers.23.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]        layers.23.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]      layers.23.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 216/ 363]     layers.23.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]     layers.23.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]     layers.23.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 363]            layers.23.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]        layers.24.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]        layers.24.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]        layers.24.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]        layers.24.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]      layers.24.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 225/ 363]     layers.24.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]     layers.24.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]     layers.24.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 363]            layers.24.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]        layers.25.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]        layers.25.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]        layers.25.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]        layers.25.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]      layers.25.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 234/ 363]     layers.25.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]     layers.25.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]     layers.25.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 363]            layers.25.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]        layers.26.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]        layers.26.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]        layers.26.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]        layers.26.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]      layers.26.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 243/ 363]     layers.26.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]     layers.26.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]     layers.26.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 363]            layers.26.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]        layers.27.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]        layers.27.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]        layers.27.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]        layers.27.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]      layers.27.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 252/ 363]     layers.27.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]     layers.27.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]     layers.27.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 363]            layers.27.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]        layers.28.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]        layers.28.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]        layers.28.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]        layers.28.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]      layers.28.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 261/ 363]     layers.28.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]     layers.28.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]     layers.28.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 363]            layers.28.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]        layers.29.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]        layers.29.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]        layers.29.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]        layers.29.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]      layers.29.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 270/ 363]     layers.29.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]     layers.29.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]     layers.29.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 363]            layers.29.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]        layers.30.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]        layers.30.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]        layers.30.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]        layers.30.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]      layers.30.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 279/ 363]     layers.30.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]     layers.30.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]     layers.30.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 363]            layers.30.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]        layers.31.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]        layers.31.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]        layers.31.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]        layers.31.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]      layers.31.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 288/ 363]     layers.31.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]     layers.31.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]     layers.31.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 363]            layers.31.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]        layers.32.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]        layers.32.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]        layers.32.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]        layers.32.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]      layers.32.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 297/ 363]     layers.32.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]     layers.32.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]     layers.32.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 363]            layers.32.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]        layers.33.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]        layers.33.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]        layers.33.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]        layers.33.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]      layers.33.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 306/ 363]     layers.33.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]     layers.33.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]     layers.33.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 363]            layers.33.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]        layers.34.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]        layers.34.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]        layers.34.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]        layers.34.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]      layers.34.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 315/ 363]     layers.34.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]     layers.34.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]     layers.34.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 363]            layers.34.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]        layers.35.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]        layers.35.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]        layers.35.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]        layers.35.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]      layers.35.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 324/ 363]     layers.35.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]     layers.35.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]     layers.35.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 363]            layers.35.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]        layers.36.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]        layers.36.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]        layers.36.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]        layers.36.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]      layers.36.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 333/ 363]     layers.36.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 363]     layers.36.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 335/ 363]     layers.36.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 363]            layers.36.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]        layers.37.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]        layers.37.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]        layers.37.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]        layers.37.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]      layers.37.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 342/ 363]     layers.37.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 363]     layers.37.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 344/ 363]     layers.37.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 363]            layers.37.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]        layers.38.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]        layers.38.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]        layers.38.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]        layers.38.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]      layers.38.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 351/ 363]     layers.38.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 363]     layers.38.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 353/ 363]     layers.38.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 363]            layers.38.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]        layers.39.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]        layers.39.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]        layers.39.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 64106.32 ms\n",
      "main:    total time = 64106.32 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/30B/ggml-model-f16.bin' to './models/30B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/30B/ggml-model-q4_0.bin\n",
      "[   1/ 543]                tok_embeddings.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   114.26 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                          norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   166.63 MB | hist: \n",
      "[   4/ 543]         layers.0.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]         layers.0.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]         layers.0.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]         layers.0.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]       layers.0.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   9/ 543]      layers.0.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 543]      layers.0.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 543]      layers.0.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 543]             layers.0.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  13/ 543]         layers.1.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]         layers.1.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]         layers.1.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]         layers.1.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]       layers.1.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  18/ 543]      layers.1.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]      layers.1.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]      layers.1.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 543]             layers.1.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  22/ 543]         layers.2.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]         layers.2.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]         layers.2.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]         layers.2.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]       layers.2.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  27/ 543]      layers.2.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]      layers.2.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]      layers.2.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 543]             layers.2.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  31/ 543]         layers.3.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]         layers.3.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]         layers.3.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]         layers.3.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]       layers.3.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  36/ 543]      layers.3.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 543]      layers.3.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  38/ 543]      layers.3.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 543]             layers.3.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  40/ 543]         layers.4.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]         layers.4.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]         layers.4.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]         layers.4.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]       layers.4.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  45/ 543]      layers.4.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]      layers.4.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]      layers.4.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 543]             layers.4.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  49/ 543]         layers.5.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]         layers.5.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]         layers.5.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]         layers.5.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]       layers.5.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  54/ 543]      layers.5.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]      layers.5.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]      layers.5.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 543]             layers.5.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  58/ 543]         layers.6.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]         layers.6.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]         layers.6.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]         layers.6.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]       layers.6.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  63/ 543]      layers.6.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]      layers.6.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]      layers.6.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 543]             layers.6.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  67/ 543]         layers.7.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]         layers.7.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]         layers.7.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]         layers.7.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]       layers.7.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  72/ 543]      layers.7.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]      layers.7.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]      layers.7.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 543]             layers.7.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  76/ 543]         layers.8.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]         layers.8.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]         layers.8.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]         layers.8.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]       layers.8.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  81/ 543]      layers.8.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]      layers.8.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]      layers.8.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 543]             layers.8.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  85/ 543]         layers.9.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]         layers.9.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]         layers.9.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]         layers.9.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]       layers.9.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  90/ 543]      layers.9.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]      layers.9.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]      layers.9.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 543]             layers.9.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  94/ 543]        layers.10.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]        layers.10.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]        layers.10.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]        layers.10.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]      layers.10.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  99/ 543]     layers.10.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]     layers.10.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]     layers.10.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 543]            layers.10.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]        layers.11.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]        layers.11.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]        layers.11.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]        layers.11.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]      layers.11.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 108/ 543]     layers.11.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]     layers.11.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]     layers.11.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 543]            layers.11.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]        layers.12.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]        layers.12.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]        layers.12.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]        layers.12.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]      layers.12.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 117/ 543]     layers.12.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]     layers.12.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]     layers.12.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 543]            layers.12.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]        layers.13.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]        layers.13.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]        layers.13.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]        layers.13.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]      layers.13.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 126/ 543]     layers.13.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]     layers.13.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]     layers.13.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 543]            layers.13.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]        layers.14.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]        layers.14.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]        layers.14.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]        layers.14.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]      layers.14.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 135/ 543]     layers.14.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 543]     layers.14.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 543]     layers.14.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 543]            layers.14.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]        layers.15.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]        layers.15.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]        layers.15.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]        layers.15.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]      layers.15.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 144/ 543]     layers.15.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]     layers.15.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]     layers.15.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 543]            layers.15.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]        layers.16.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]        layers.16.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]        layers.16.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]        layers.16.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]      layers.16.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 153/ 543]     layers.16.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]     layers.16.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]     layers.16.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 543]            layers.16.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]        layers.17.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]        layers.17.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]        layers.17.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]        layers.17.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]      layers.17.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 162/ 543]     layers.17.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]     layers.17.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]     layers.17.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 543]            layers.17.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]        layers.18.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]        layers.18.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]        layers.18.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]        layers.18.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]      layers.18.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 171/ 543]     layers.18.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]     layers.18.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]     layers.18.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 543]            layers.18.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]        layers.19.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]        layers.19.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]        layers.19.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]        layers.19.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]      layers.19.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 180/ 543]     layers.19.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 543]     layers.19.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 543]     layers.19.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 543]            layers.19.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]        layers.20.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]        layers.20.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]        layers.20.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]        layers.20.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]      layers.20.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 189/ 543]     layers.20.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 543]     layers.20.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 543]     layers.20.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 543]            layers.20.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]        layers.21.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]        layers.21.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]        layers.21.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]        layers.21.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]      layers.21.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 198/ 543]     layers.21.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]     layers.21.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]     layers.21.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 543]            layers.21.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]        layers.22.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]        layers.22.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]        layers.22.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]        layers.22.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]      layers.22.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 207/ 543]     layers.22.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]     layers.22.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]     layers.22.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 543]            layers.22.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]        layers.23.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]        layers.23.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]        layers.23.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]        layers.23.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]      layers.23.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 216/ 543]     layers.23.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]     layers.23.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]     layers.23.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 543]            layers.23.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]        layers.24.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]        layers.24.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]        layers.24.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]        layers.24.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]      layers.24.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 225/ 543]     layers.24.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]     layers.24.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]     layers.24.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 543]            layers.24.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]        layers.25.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]        layers.25.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]        layers.25.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]        layers.25.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]      layers.25.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 234/ 543]     layers.25.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]     layers.25.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]     layers.25.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 543]            layers.25.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]        layers.26.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]        layers.26.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]        layers.26.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]        layers.26.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]      layers.26.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 243/ 543]     layers.26.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]     layers.26.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]     layers.26.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 543]            layers.26.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]        layers.27.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]        layers.27.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]        layers.27.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]        layers.27.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]      layers.27.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 252/ 543]     layers.27.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]     layers.27.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]     layers.27.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 543]            layers.27.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]        layers.28.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]        layers.28.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]        layers.28.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]        layers.28.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]      layers.28.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 261/ 543]     layers.28.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]     layers.28.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]     layers.28.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 543]            layers.28.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]        layers.29.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]        layers.29.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]        layers.29.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]        layers.29.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]      layers.29.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 270/ 543]     layers.29.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]     layers.29.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]     layers.29.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 543]            layers.29.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]        layers.30.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]        layers.30.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]        layers.30.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]        layers.30.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]      layers.30.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 279/ 543]     layers.30.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]     layers.30.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]     layers.30.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 543]            layers.30.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]        layers.31.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]        layers.31.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]        layers.31.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]        layers.31.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]      layers.31.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 288/ 543]     layers.31.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]     layers.31.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]     layers.31.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 543]            layers.31.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]        layers.32.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]        layers.32.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]        layers.32.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]        layers.32.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]      layers.32.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 297/ 543]     layers.32.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]     layers.32.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]     layers.32.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 543]            layers.32.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]        layers.33.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]        layers.33.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]        layers.33.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]        layers.33.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]      layers.33.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 306/ 543]     layers.33.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]     layers.33.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]     layers.33.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 543]            layers.33.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]        layers.34.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]        layers.34.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]        layers.34.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]        layers.34.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]      layers.34.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 315/ 543]     layers.34.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]     layers.34.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]     layers.34.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 543]            layers.34.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]        layers.35.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]        layers.35.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]        layers.35.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]        layers.35.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]      layers.35.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 324/ 543]     layers.35.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]     layers.35.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]     layers.35.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 543]            layers.35.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]        layers.36.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]        layers.36.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]        layers.36.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]        layers.36.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]      layers.36.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 333/ 543]     layers.36.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]     layers.36.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]     layers.36.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 543]            layers.36.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]        layers.37.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]        layers.37.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]        layers.37.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]        layers.37.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]      layers.37.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 342/ 543]     layers.37.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]     layers.37.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]     layers.37.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 543]            layers.37.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]        layers.38.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]        layers.38.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]        layers.38.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]        layers.38.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]      layers.38.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 351/ 543]     layers.38.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]     layers.38.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]     layers.38.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 543]            layers.38.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]        layers.39.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]        layers.39.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]        layers.39.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]        layers.39.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]      layers.39.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 360/ 543]     layers.39.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]     layers.39.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]     layers.39.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 543]            layers.39.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]        layers.40.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]        layers.40.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]        layers.40.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]        layers.40.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]      layers.40.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 369/ 543]     layers.40.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]     layers.40.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]     layers.40.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 543]            layers.40.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]        layers.41.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]        layers.41.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]        layers.41.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]        layers.41.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]      layers.41.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 378/ 543]     layers.41.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]     layers.41.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]     layers.41.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 543]            layers.41.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]        layers.42.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]        layers.42.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]        layers.42.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]        layers.42.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]      layers.42.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 387/ 543]     layers.42.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]     layers.42.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]     layers.42.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 543]            layers.42.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]        layers.43.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]        layers.43.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]        layers.43.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]        layers.43.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]      layers.43.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 396/ 543]     layers.43.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]     layers.43.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]     layers.43.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 543]            layers.43.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]        layers.44.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]        layers.44.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]        layers.44.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]        layers.44.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]      layers.44.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 405/ 543]     layers.44.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]     layers.44.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]     layers.44.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 543]            layers.44.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]        layers.45.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]        layers.45.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]        layers.45.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]        layers.45.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]      layers.45.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 414/ 543]     layers.45.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]     layers.45.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]     layers.45.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 543]            layers.45.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]        layers.46.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]        layers.46.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]        layers.46.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]        layers.46.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]      layers.46.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 423/ 543]     layers.46.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]     layers.46.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]     layers.46.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 543]            layers.46.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]        layers.47.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]        layers.47.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]        layers.47.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]        layers.47.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]      layers.47.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 432/ 543]     layers.47.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]     layers.47.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]     layers.47.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 543]            layers.47.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]        layers.48.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]        layers.48.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]        layers.48.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]        layers.48.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]      layers.48.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 441/ 543]     layers.48.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]     layers.48.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]     layers.48.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 543]            layers.48.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]        layers.49.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]        layers.49.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]        layers.49.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]        layers.49.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]      layers.49.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 450/ 543]     layers.49.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]     layers.49.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]     layers.49.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 543]            layers.49.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]        layers.50.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]        layers.50.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]        layers.50.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]        layers.50.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]      layers.50.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 459/ 543]     layers.50.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]     layers.50.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]     layers.50.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 543]            layers.50.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]        layers.51.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]        layers.51.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]        layers.51.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]        layers.51.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]      layers.51.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 468/ 543]     layers.51.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]     layers.51.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]     layers.51.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 543]            layers.51.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]        layers.52.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]        layers.52.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]        layers.52.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]        layers.52.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]      layers.52.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 477/ 543]     layers.52.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]     layers.52.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]     layers.52.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 543]            layers.52.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]        layers.53.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]        layers.53.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]        layers.53.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]        layers.53.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]      layers.53.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 486/ 543]     layers.53.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]     layers.53.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]     layers.53.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 543]            layers.53.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]        layers.54.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]        layers.54.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]        layers.54.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]        layers.54.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]      layers.54.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 495/ 543]     layers.54.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]     layers.54.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]     layers.54.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 543]            layers.54.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]        layers.55.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]        layers.55.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]        layers.55.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]        layers.55.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]      layers.55.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 504/ 543]     layers.55.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]     layers.55.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]     layers.55.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 543]            layers.55.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]        layers.56.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]        layers.56.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]        layers.56.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]        layers.56.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]      layers.56.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 513/ 543]     layers.56.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 543]     layers.56.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 515/ 543]     layers.56.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 543]            layers.56.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]        layers.57.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]        layers.57.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]        layers.57.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]        layers.57.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]      layers.57.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 522/ 543]     layers.57.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 543]     layers.57.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 524/ 543]     layers.57.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 543]            layers.57.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]        layers.58.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]        layers.58.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]        layers.58.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]        layers.58.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]      layers.58.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 531/ 543]     layers.58.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 543]     layers.58.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 533/ 543]     layers.58.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 543]            layers.58.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]        layers.59.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]        layers.59.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]        layers.59.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]        layers.59.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]      layers.59.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 540/ 543]     layers.59.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 541/ 543]     layers.59.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 542/ 543]     layers.59.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 543/ 543]            layers.59.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 146967.69 ms\n",
      "main:    total time = 146967.69 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/65B/ggml-model-f16.bin' to './models/65B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/65B/ggml-model-q4_0.bin\n",
      "[   1/ 723]                tok_embeddings.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   140.62 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                          norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   205.08 MB | hist: \n",
      "[   4/ 723]         layers.0.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]         layers.0.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]         layers.0.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]         layers.0.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]       layers.0.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   9/ 723]      layers.0.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[  10/ 723]      layers.0.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  11/ 723]      layers.0.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  12/ 723]             layers.0.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  13/ 723]         layers.1.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]         layers.1.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]         layers.1.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]         layers.1.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]       layers.1.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  18/ 723]      layers.1.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]      layers.1.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]      layers.1.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 723]             layers.1.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  22/ 723]         layers.2.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]         layers.2.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]         layers.2.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]         layers.2.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]       layers.2.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  27/ 723]      layers.2.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]      layers.2.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]      layers.2.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 723]             layers.2.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  31/ 723]         layers.3.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]         layers.3.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]         layers.3.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]         layers.3.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]       layers.3.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  36/ 723]      layers.3.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]      layers.3.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]      layers.3.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 723]             layers.3.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  40/ 723]         layers.4.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]         layers.4.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]         layers.4.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]         layers.4.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]       layers.4.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  45/ 723]      layers.4.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]      layers.4.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]      layers.4.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 723]             layers.4.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  49/ 723]         layers.5.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]         layers.5.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]         layers.5.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]         layers.5.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]       layers.5.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  54/ 723]      layers.5.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]      layers.5.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]      layers.5.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 723]             layers.5.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  58/ 723]         layers.6.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]         layers.6.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]         layers.6.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]         layers.6.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]       layers.6.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  63/ 723]      layers.6.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]      layers.6.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]      layers.6.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 723]             layers.6.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  67/ 723]         layers.7.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]         layers.7.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]         layers.7.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]         layers.7.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]       layers.7.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  72/ 723]      layers.7.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]      layers.7.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]      layers.7.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 723]             layers.7.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  76/ 723]         layers.8.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]         layers.8.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]         layers.8.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]         layers.8.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]       layers.8.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  81/ 723]      layers.8.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]      layers.8.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]      layers.8.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 723]             layers.8.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  85/ 723]         layers.9.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]         layers.9.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]         layers.9.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]         layers.9.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]       layers.9.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  90/ 723]      layers.9.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 723]      layers.9.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 723]      layers.9.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 723]             layers.9.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  94/ 723]        layers.10.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]        layers.10.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]        layers.10.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]        layers.10.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]      layers.10.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  99/ 723]     layers.10.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 723]     layers.10.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 723]     layers.10.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 723]            layers.10.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]        layers.11.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]        layers.11.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]        layers.11.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]        layers.11.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]      layers.11.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 108/ 723]     layers.11.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]     layers.11.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]     layers.11.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 723]            layers.11.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]        layers.12.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]        layers.12.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]        layers.12.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]        layers.12.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]      layers.12.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 117/ 723]     layers.12.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]     layers.12.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]     layers.12.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 723]            layers.12.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]        layers.13.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]        layers.13.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]        layers.13.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]        layers.13.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]      layers.13.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 126/ 723]     layers.13.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]     layers.13.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]     layers.13.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 723]            layers.13.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]        layers.14.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]        layers.14.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]        layers.14.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]        layers.14.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]      layers.14.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 135/ 723]     layers.14.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]     layers.14.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]     layers.14.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 723]            layers.14.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]        layers.15.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]        layers.15.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]        layers.15.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]        layers.15.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]      layers.15.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 144/ 723]     layers.15.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 723]     layers.15.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 723]     layers.15.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 723]            layers.15.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]        layers.16.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]        layers.16.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]        layers.16.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]        layers.16.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]      layers.16.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 153/ 723]     layers.16.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]     layers.16.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]     layers.16.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 723]            layers.16.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]        layers.17.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]        layers.17.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]        layers.17.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]        layers.17.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]      layers.17.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 162/ 723]     layers.17.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]     layers.17.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]     layers.17.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 723]            layers.17.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]        layers.18.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]        layers.18.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]        layers.18.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]        layers.18.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]      layers.18.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 171/ 723]     layers.18.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]     layers.18.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]     layers.18.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 723]            layers.18.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]        layers.19.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]        layers.19.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]        layers.19.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]        layers.19.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]      layers.19.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 180/ 723]     layers.19.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]     layers.19.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]     layers.19.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 723]            layers.19.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]        layers.20.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]        layers.20.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]        layers.20.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]        layers.20.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]      layers.20.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 189/ 723]     layers.20.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]     layers.20.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]     layers.20.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 723]            layers.20.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]        layers.21.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]        layers.21.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]        layers.21.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]        layers.21.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]      layers.21.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 198/ 723]     layers.21.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]     layers.21.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]     layers.21.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 723]            layers.21.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]        layers.22.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]        layers.22.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]        layers.22.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]        layers.22.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]      layers.22.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 207/ 723]     layers.22.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]     layers.22.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]     layers.22.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 723]            layers.22.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]        layers.23.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]        layers.23.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]        layers.23.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]        layers.23.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]      layers.23.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 216/ 723]     layers.23.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]     layers.23.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]     layers.23.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 723]            layers.23.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]        layers.24.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]        layers.24.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]        layers.24.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]        layers.24.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]      layers.24.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 225/ 723]     layers.24.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]     layers.24.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]     layers.24.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 723]            layers.24.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]        layers.25.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]        layers.25.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]        layers.25.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]        layers.25.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]      layers.25.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 234/ 723]     layers.25.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]     layers.25.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]     layers.25.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 723]            layers.25.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]        layers.26.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]        layers.26.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]        layers.26.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]        layers.26.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]      layers.26.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 243/ 723]     layers.26.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]     layers.26.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]     layers.26.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 723]            layers.26.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]        layers.27.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]        layers.27.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]        layers.27.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]        layers.27.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]      layers.27.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 252/ 723]     layers.27.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]     layers.27.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]     layers.27.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 723]            layers.27.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]        layers.28.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]        layers.28.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]        layers.28.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]        layers.28.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]      layers.28.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 261/ 723]     layers.28.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]     layers.28.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]     layers.28.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 723]            layers.28.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]        layers.29.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]        layers.29.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]        layers.29.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]        layers.29.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]      layers.29.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 270/ 723]     layers.29.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]     layers.29.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]     layers.29.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 723]            layers.29.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]        layers.30.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]        layers.30.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]        layers.30.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]        layers.30.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]      layers.30.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 279/ 723]     layers.30.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]     layers.30.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]     layers.30.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 723]            layers.30.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]        layers.31.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]        layers.31.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]        layers.31.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]        layers.31.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]      layers.31.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 288/ 723]     layers.31.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]     layers.31.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]     layers.31.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 723]            layers.31.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]        layers.32.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]        layers.32.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]        layers.32.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]        layers.32.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]      layers.32.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 297/ 723]     layers.32.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]     layers.32.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]     layers.32.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 723]            layers.32.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]        layers.33.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]        layers.33.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]        layers.33.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]        layers.33.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]      layers.33.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 306/ 723]     layers.33.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]     layers.33.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]     layers.33.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 723]            layers.33.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]        layers.34.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]        layers.34.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]        layers.34.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]        layers.34.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]      layers.34.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 315/ 723]     layers.34.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]     layers.34.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]     layers.34.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 723]            layers.34.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]        layers.35.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]        layers.35.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]        layers.35.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]        layers.35.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]      layers.35.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 324/ 723]     layers.35.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]     layers.35.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]     layers.35.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 723]            layers.35.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]        layers.36.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]        layers.36.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]        layers.36.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]        layers.36.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]      layers.36.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 333/ 723]     layers.36.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]     layers.36.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]     layers.36.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 723]            layers.36.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]        layers.37.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]        layers.37.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]        layers.37.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]        layers.37.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]      layers.37.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 342/ 723]     layers.37.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]     layers.37.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]     layers.37.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 723]            layers.37.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]        layers.38.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]        layers.38.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]        layers.38.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]        layers.38.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]      layers.38.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 351/ 723]     layers.38.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]     layers.38.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]     layers.38.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 723]            layers.38.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]        layers.39.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]        layers.39.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]        layers.39.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]        layers.39.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]      layers.39.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 360/ 723]     layers.39.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]     layers.39.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]     layers.39.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 723]            layers.39.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]        layers.40.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]        layers.40.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]        layers.40.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]        layers.40.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]      layers.40.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 369/ 723]     layers.40.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]     layers.40.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]     layers.40.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 723]            layers.40.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]        layers.41.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]        layers.41.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]        layers.41.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]        layers.41.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]      layers.41.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 378/ 723]     layers.41.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]     layers.41.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]     layers.41.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 723]            layers.41.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]        layers.42.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]        layers.42.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]        layers.42.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]        layers.42.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]      layers.42.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 387/ 723]     layers.42.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]     layers.42.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]     layers.42.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 723]            layers.42.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]        layers.43.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]        layers.43.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]        layers.43.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]        layers.43.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]      layers.43.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 396/ 723]     layers.43.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]     layers.43.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]     layers.43.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 723]            layers.43.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]        layers.44.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]        layers.44.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]        layers.44.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]        layers.44.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]      layers.44.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 405/ 723]     layers.44.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]     layers.44.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]     layers.44.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 723]            layers.44.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]        layers.45.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]        layers.45.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]        layers.45.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]        layers.45.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]      layers.45.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 414/ 723]     layers.45.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]     layers.45.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]     layers.45.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 723]            layers.45.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]        layers.46.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]        layers.46.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]        layers.46.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]        layers.46.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]      layers.46.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 423/ 723]     layers.46.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]     layers.46.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]     layers.46.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 723]            layers.46.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]        layers.47.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]        layers.47.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]        layers.47.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]        layers.47.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]      layers.47.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 432/ 723]     layers.47.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]     layers.47.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]     layers.47.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 723]            layers.47.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]        layers.48.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]        layers.48.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]        layers.48.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]        layers.48.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]      layers.48.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 441/ 723]     layers.48.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]     layers.48.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]     layers.48.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 723]            layers.48.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]        layers.49.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]        layers.49.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]        layers.49.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]        layers.49.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]      layers.49.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 450/ 723]     layers.49.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]     layers.49.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]     layers.49.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 723]            layers.49.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]        layers.50.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]        layers.50.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]        layers.50.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]        layers.50.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]      layers.50.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 459/ 723]     layers.50.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]     layers.50.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]     layers.50.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 723]            layers.50.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]        layers.51.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]        layers.51.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]        layers.51.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]        layers.51.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]      layers.51.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 468/ 723]     layers.51.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]     layers.51.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]     layers.51.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 723]            layers.51.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]        layers.52.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]        layers.52.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]        layers.52.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]        layers.52.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]      layers.52.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 477/ 723]     layers.52.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]     layers.52.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]     layers.52.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 723]            layers.52.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]        layers.53.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]        layers.53.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]        layers.53.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]        layers.53.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]      layers.53.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 486/ 723]     layers.53.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]     layers.53.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]     layers.53.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 723]            layers.53.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]        layers.54.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]        layers.54.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]        layers.54.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]        layers.54.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]      layers.54.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 495/ 723]     layers.54.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]     layers.54.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]     layers.54.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 723]            layers.54.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]        layers.55.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]        layers.55.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]        layers.55.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]        layers.55.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]      layers.55.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 504/ 723]     layers.55.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]     layers.55.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]     layers.55.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 723]            layers.55.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]        layers.56.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]        layers.56.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]        layers.56.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]        layers.56.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]      layers.56.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 513/ 723]     layers.56.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]     layers.56.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]     layers.56.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 723]            layers.56.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]        layers.57.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]        layers.57.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]        layers.57.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]        layers.57.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]      layers.57.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 522/ 723]     layers.57.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]     layers.57.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]     layers.57.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 723]            layers.57.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]        layers.58.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]        layers.58.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]        layers.58.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]        layers.58.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]      layers.58.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 531/ 723]     layers.58.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]     layers.58.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]     layers.58.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 723]            layers.58.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]        layers.59.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]        layers.59.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]        layers.59.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]        layers.59.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]      layers.59.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 540/ 723]     layers.59.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]     layers.59.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]     layers.59.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 543/ 723]            layers.59.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]        layers.60.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]        layers.60.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]        layers.60.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]        layers.60.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]      layers.60.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 549/ 723]     layers.60.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]     layers.60.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]     layers.60.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 552/ 723]            layers.60.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]        layers.61.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]        layers.61.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]        layers.61.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]        layers.61.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]      layers.61.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 558/ 723]     layers.61.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]     layers.61.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]     layers.61.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 561/ 723]            layers.61.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]        layers.62.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]        layers.62.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]        layers.62.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]        layers.62.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]      layers.62.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 567/ 723]     layers.62.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]     layers.62.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]     layers.62.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 570/ 723]            layers.62.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]        layers.63.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]        layers.63.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]        layers.63.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]        layers.63.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]      layers.63.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 576/ 723]     layers.63.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]     layers.63.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]     layers.63.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 579/ 723]            layers.63.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]        layers.64.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]        layers.64.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]        layers.64.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]        layers.64.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]      layers.64.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 585/ 723]     layers.64.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]     layers.64.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]     layers.64.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 588/ 723]            layers.64.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]        layers.65.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]        layers.65.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]        layers.65.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]        layers.65.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]      layers.65.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 594/ 723]     layers.65.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]     layers.65.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]     layers.65.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 597/ 723]            layers.65.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]        layers.66.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]        layers.66.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]        layers.66.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]        layers.66.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]      layers.66.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 603/ 723]     layers.66.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]     layers.66.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]     layers.66.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 606/ 723]            layers.66.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]        layers.67.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]        layers.67.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]        layers.67.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]        layers.67.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]      layers.67.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 612/ 723]     layers.67.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]     layers.67.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]     layers.67.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 615/ 723]            layers.67.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]        layers.68.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]        layers.68.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]        layers.68.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]        layers.68.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]      layers.68.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 621/ 723]     layers.68.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]     layers.68.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]     layers.68.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 624/ 723]            layers.68.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]        layers.69.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]        layers.69.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]        layers.69.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]        layers.69.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]      layers.69.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 630/ 723]     layers.69.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]     layers.69.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]     layers.69.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 633/ 723]            layers.69.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]        layers.70.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]        layers.70.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]        layers.70.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]        layers.70.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]      layers.70.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 639/ 723]     layers.70.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]     layers.70.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]     layers.70.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 642/ 723]            layers.70.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]        layers.71.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]        layers.71.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]        layers.71.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]        layers.71.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]      layers.71.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 648/ 723]     layers.71.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]     layers.71.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]     layers.71.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 651/ 723]            layers.71.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]        layers.72.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]        layers.72.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]        layers.72.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]        layers.72.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]      layers.72.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 657/ 723]     layers.72.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]     layers.72.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]     layers.72.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 660/ 723]            layers.72.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]        layers.73.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]        layers.73.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]        layers.73.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]        layers.73.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]      layers.73.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 666/ 723]     layers.73.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]     layers.73.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]     layers.73.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 669/ 723]            layers.73.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]        layers.74.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]        layers.74.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]        layers.74.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]        layers.74.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]      layers.74.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 675/ 723]     layers.74.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]     layers.74.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]     layers.74.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 678/ 723]            layers.74.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]        layers.75.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]        layers.75.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]        layers.75.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]        layers.75.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]      layers.75.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 684/ 723]     layers.75.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]     layers.75.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]     layers.75.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 687/ 723]            layers.75.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]        layers.76.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]        layers.76.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]        layers.76.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]        layers.76.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]      layers.76.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 693/ 723]     layers.76.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 694/ 723]     layers.76.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 695/ 723]     layers.76.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 696/ 723]            layers.76.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]        layers.77.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]        layers.77.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]        layers.77.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]        layers.77.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]      layers.77.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 702/ 723]     layers.77.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 703/ 723]     layers.77.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 704/ 723]     layers.77.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 705/ 723]            layers.77.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]        layers.78.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]        layers.78.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]        layers.78.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]        layers.78.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]      layers.78.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 711/ 723]     layers.78.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 712/ 723]     layers.78.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 713/ 723]     layers.78.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 714/ 723]            layers.78.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]        layers.79.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]        layers.79.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]        layers.79.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]        layers.79.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]      layers.79.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 720/ 723]     layers.79.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 721/ 723]     layers.79.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 722/ 723]     layers.79.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 723/ 723]            layers.79.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 342295.48 ms\n",
      "main:    total time = 342295.48 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.bin ./models/13B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.bin ./models/30B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.bin ./models/65B/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569047\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as fully and as authentically as you can. I believe we should take risks, not be afraid to fail, and always give our best effort; that’s how we grow.\n",
      "I believe when things don’t seem to be working out for us in a relationship or career, there is probably something better on the horizon if we are willing to wait and let go of what no longer serves us. I believe life is a series of ups and downs and it’s our choice whether to get stuck in the negativity or rise up with gratitude each morning when we wake.\n",
      "I believe there is always something else to be grateful for, even if it’s just another sunrise; that’s how I have become a more positive person, by focusing on what’s good instead of all the things wrong in my life or the world.\n",
      "I believe in being true to myself and not caring about what others think because we are ultimately answerable only to ourselves. I believe it is never too late to start something new, that a fresh perspective can change everything, and that love always prevails.\n",
      "And I believe in the power of words; how they heal us or wound us, inspire or confuse us, motivate or discourage us, connect us to one another or separate us from each other, and so much more. Words can change our lives for better or worse; that’s why I always strive to say something positive when I speak to you.\n",
      "I believe we are all here together for a reason and there is an important part each of us has to play in this world. We may not realize it at first, but ultimately we will know if we keep living our lives with integrity and love. And that’s my philosophy on life: To live as fully and authentically as possible; knowing that I am here for a reason and with the intention of finding out what that is.\n",
      "I hope you are all doing well and having a wonderful summer. Enjoy!\n",
      "Tags: attitude, beliefs, Gratitude, inspiration, Kids, Life Coach, positive thinking, self-esteem, Stacy Bartlo\n",
      "One Response to “Philosophy on Life”\n",
      "Well said Stacey!! Love your philosophy and thanks for sharing.\n",
      "Stephenie August 19, 2013 at 1:17 pm #\n",
      "What an awesome article\n",
      "llama_print_timings:        load time =  3786.22 ms\n",
      "llama_print_timings:      sample time =   290.04 ms /   512 runs   (    0.57 ms per token,  1765.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2119.69 ms /   265 tokens (    8.00 ms per token,   125.02 tokens per second)\n",
      "llama_print_timings:        eval time = 10974.56 ms /   510 runs   (   21.52 ms per token,    46.47 tokens per second)\n",
      "llama_print_timings:       total time = 13477.63 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7a0912-2746-4e7d-adaf-92b60cb2ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569077\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. This includes doing everything in our power to ensure our children will have a better future than we had ourselves, and to leave this world in a better place for having lived in it.\n",
      "So, what can we do? Well, first of all, we must take care of ourselves so that we may continue to fight the good fight. It’s not easy, but I know it is possible because I’ve done it. You can learn how I did it by visiting http://www.LivingLifeWellness.com or by clicking on the link below.\n",
      "Secondly, we must get engaged in our community so that together we may take back our country from those who have stolen it (both elected and appointed officials). We need to make sure that only those who are truly qualified for public office run for public office, and that those who aren’t qualified don’t.\n",
      "We can start by electing the best possible people to serve on our local school boards, city councils, county commissions, state legislatures (including our state senators) and finally, in Congress. We must also get involved with non-profit organizations that provide critical services to the community such as food pantries, homeless shelters, addiction recovery programs, and so on. And we need to do all of this while working for a living to support ourselves and our families.\n",
      "Lastly, if you’re like me then you want to help others too. If that’s the case, I invite you to consider supporting my campaign for Congress with a financial donation, or by volunteering your time and effort on my behalf.\n",
      "I know, I know…it seems overwhelming, doesn’t it? I have good news though! If you would like me to help you figure out how you can make the biggest difference that you are capable of making (without sacrificing your own well-being), please let me know by leaving a comment below. You can also reach out directly to me at any time for guidance and support, or just to talk.\n",
      "I’m here for you, and I will be there for you until the day I die. It’s my privilege!\n",
      "P.S. If you want to learn more about how you can live your best life, I invite you to visit http://www.LivingLifeWellness.com or click on the link above. Also, if you know\n",
      "llama_print_timings:        load time =  1155.41 ms\n",
      "llama_print_timings:      sample time =   288.39 ms /   512 runs   (    0.56 ms per token,  1775.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2099.55 ms /   265 tokens (    7.92 ms per token,   126.22 tokens per second)\n",
      "llama_print_timings:        eval time = 10956.73 ms /   510 runs   (   21.48 ms per token,    46.55 tokens per second)\n",
      "llama_print_timings:       total time = 13438.14 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3324a21-7a43-4c68-83eb-52417e487c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569094\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it fully, and to make a difference.\n",
      "My name is David Lowe, I am 35 years old, and I’m from Perth, Western Australia.\n",
      "I am a husband, a father, a son, a brother, an uncle. A friend. A student of Life.\n",
      "Born with the proverbial silver spoon in my mouth; I had everything going for me: money, supportive parents, amazing teachers and mentors, opportunity. In many ways, I’ve lived a charmed life.\n",
      "But despite this, I have felt lost most of my life.\n",
      "I have always craved the freedom to do what I want, when I want, but for so long I was afraid that if I had it, I wouldn’t be able to find purpose and meaning in any of it – so I kept myself busy.\n",
      "Nowadays, though, I’ve come to understand that the only way to get to where you really want to go is by first being willing to look at what’s holding you back from getting there – because once you do, you can begin to change course and start heading in a direction that feels more aligned with your Soul.\n",
      "I know how hard it is to break out of the cycle of doing all the things that you think you should be doing, and instead step into the flow of your own life – because I’ve been there too.\n",
      "So here I am; taking my first steps towards living my life on my terms. And in this quest, I have no choice but to ask myself some big questions: What do I really want? How can I be true to myself and live a life that matters? Where is my purpose? Who am I? Why was I brought into the world?\n",
      "I’ve been searching for an answer to these questions my entire life, and now finally feel like I may have found one. But of course, this is just the beginning; it’s what comes next that really counts.\n",
      "So, in the spirit of exploration and adventure, here I am, sharing what I learn with the world through this blog, as well as through my YouTube channel (which you can find at the bottom of this page!).\n",
      "My hope is that by doing so, I might help some of you to discover your purpose, and get out there to live it. Because when we do that, all of our lives become better for it.\n",
      "I’m really looking forward\n",
      "llama_print_timings:        load time =  1152.14 ms\n",
      "llama_print_timings:      sample time =   285.46 ms /   512 runs   (    0.56 ms per token,  1793.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2098.80 ms /   265 tokens (    7.92 ms per token,   126.26 tokens per second)\n",
      "llama_print_timings:        eval time = 10985.37 ms /   510 runs   (   21.54 ms per token,    46.43 tokens per second)\n",
      "llama_print_timings:       total time = 13461.87 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98a39e0-c339-4234-b6d2-49d642ffb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569111\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\n",
      "—Pablo Picasso\n",
      "The last time I saw my dad, he was 89 years old and suffering from dementia. He had been living in assisted care for three years, but when we went to visit him, he still knew us all well enough. \"How's your boy doing?\" he asked me as I came into the room.\n",
      "I told him that my son was fine and was just about to graduate from college with a degree in architecture. Then I went on to talk about some of his grandson's accomplishments, how he had done well in school and was starting his career in the field. He smiled as we spoke, but I could see confusion in his eyes when I mentioned my son's degree.\n",
      "\"He got a job working for an architect?\" my dad asked me. \"That's great!\" I said, and then paused to let him take it all in. My brother and sister were also there at the time, and they were trying not to laugh at what was happening right before their eyes. It was clear that this was the first time he had heard about my son's degree—he had been asking us for years if my \"son\" had found a job yet!\n",
      "We didn't have long to visit that day, but it was nice having some time with him. I remember thinking how incredibly blessed we are as a family to have such a wonderful dad and grandfather in our lives.\n",
      "My mom always talked about her life as if she were talking about someone else's—as though she couldn't quite believe all the blessings that had been given to her, from her healthy childhood on a farm in Oklahoma to moving to Hawaii when she was 17 years old and then working for Pan American World Airways. She was one of the first female airline stewardesses, and she went all over the world with celebrities like Frank Sinatra and Elvis Presley.\n",
      "She retired from her job in Pan Am's public relations department after 26 years to raise my sister and me. For many years she struggled with depression, but she always found a way to persevere and move forward. She was also extremely generous—my mom gave away so much of herself that it was hard for her not to give more than she had. We had some difficult times together\n",
      "llama_print_timings:        load time =  6568.98 ms\n",
      "llama_print_timings:      sample time =   287.29 ms /   512 runs   (    0.56 ms per token,  1782.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2926.83 ms /   265 tokens (   11.04 ms per token,    90.54 tokens per second)\n",
      "llama_print_timings:        eval time = 14492.13 ms /   510 runs   (   28.42 ms per token,    35.19 tokens per second)\n",
      "llama_print_timings:       total time = 17800.20 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0296e978-7c63-4d87-a1a9-dc41b49dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569138\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give it….\n",
      "I believe the meaning of life is to give it away, but not in a self-sacrificial way or martyrdom. In a joyful way – that’s what this day is all about! It’s a day that we should be celebrating and giving thanks for every single thing we have received from our Creator above, God of the universe who loves us so much that he sent his only begotten Son to die on the cross for each one of us.\n",
      "Everything that we have comes directly from the generosity of God. Even when things are tough and seemingly hopeless in your life; you can always be thankful because there is someone who loves you more than anything else in this world! If you believe this, then it will change everything about how you think and feel on a daily basis.\n",
      "I hope everyone has an incredible Thanksgiving holiday with their family and friends! Happy thanks giving to all of you from the bottom of my heart. I am so thankful for each one of you.\n",
      "It’s time to give back in every way that we can. Let’s be generous, loving, kind and forgive each other. There is nothing but love between us if we let it show. It’s a beautiful thing!\n",
      "I believe the meaning of life is to give it away, not in a self-sacrificial way or martyrdom. In a joyful way – that’s what this day is all about! It’s a day that we should be celebrating and giving thanks for every single thing we have received from our Creator above, God of the universe who loves us so much that he sent his only begotten Son to die on the cross for each one of us.\n",
      "I am so thankful for each one of you! I hope everyone has an incredible Thanksgiving holiday with their family and friends! Happy thanks giving to all of you from the bottom of my heart.\n",
      "It’s time to give back in every way that we can. Let’s be generous, loving, kind and forgive each other. There is nothing but love between us if we let it show. It’s a beautiful thing! Amen!\n",
      "Previous Post Previous post:A Thankful Heart\n",
      "Next Post Gratitude…..\n",
      "2 Replies to “Thanksgiving”\n",
      "Well said\n",
      "llama_print_timings:        load time =  1754.66 ms\n",
      "llama_print_timings:      sample time =   291.97 ms /   512 runs   (    0.57 ms per token,  1753.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2833.90 ms /   265 tokens (   10.69 ms per token,    93.51 tokens per second)\n",
      "llama_print_timings:        eval time = 14493.40 ms /   510 runs   (   28.42 ms per token,    35.19 tokens per second)\n",
      "llama_print_timings:       total time = 17712.32 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebca2498-774d-4f51-ac56-a9da8f359523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569160\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away. PABLO PICASSO\n",
      "\"I'm not afraid of death; I just don't want to be there when it happens.\" WOODY ALLEN\n",
      "The only reason for time is so that everything doesn't happen at once. ALBERT EINSTEIN\n",
      "If you are going to sin, do it right. DANIEL PATRICK MOYNIHAN\n",
      "\"If I had nine hours to chop down a tree, I would spend the first hour sharpening my axe.\" ABRAHAM LINCOLN\n",
      "Do something every day for no other reason than you would rather not do it tomorrow. ARTHUR GOLDEN\n",
      "\"I have found out that there ain't no surer way to find out whether you like people or hate them than to travel with them.\" MARK TWAIN, Adventures of Huckleberry Finn\n",
      "\"The world is divided into two classes: the minority who make things happen and the majority who stand around and let it happen.\" ROBERT R. BRAHMS\n",
      "I've had a wonderful time but this wasn't it. WILLY CHASE\n",
      "If you can't get rid of the skeleton in your closet, you'd best teach it to dance. GURDIELLE KEITH\n",
      "\"The trouble with being punctual is that there's no one to appreciate it.\" FRANK PERSONS\n",
      "\"Love all, trust a few, do wrong to none.\" WILLIAM SHAKESPEARE\n",
      "\"Always remember you are absolutely unique. Just like everyone else.\" MICHAEL HAYES\n",
      "\"If you can't beat 'em, arrange to have them beaten.\" FRANKLIN ROOSEVELT\n",
      "\"Things turn out best for people who make the best of how things turn out.\" JOHN WOODEN\n",
      "The road is long and winding, but there are shortcuts. CAPTAIN OF THE COAST GUARD\n",
      "I don't want to achieve immortality through my work. I want to achieve it by not dying. ALBERT EINSTEIN\n",
      "\"There's no such thing as a free lunch.\" ELWOOD CASEY, UC Irvine\n",
      "\n",
      "llama_print_timings:        load time =  1693.24 ms\n",
      "llama_print_timings:      sample time =   286.11 ms /   512 runs   (    0.56 ms per token,  1789.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2903.81 ms /   265 tokens (   10.96 ms per token,    91.26 tokens per second)\n",
      "llama_print_timings:        eval time = 14509.03 ms /   510 runs   (   28.45 ms per token,    35.15 tokens per second)\n",
      "llama_print_timings:       total time = 17791.66 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d246e2-1ed4-46f7-aa3e-0d4787cc3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569182\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your bliss. And that bliss and contentment only exist inside yourself.\n",
      "There's a lot of people in this world telling you \"you can't\"...and maybe they are right, but they are not the ones who get to decide for you whether or not you CAN. You have to try first. If you fail, then those people were right. But if you succeed? Then you are the one who gets to decide how much is possible in your life!\n",
      "The trick is that there will always be someone waiting around the corner who can tell you \"no\"....but not everyone can tell you \"yes\". So what do you listen to? The people who say yes. And then you get to live your life on your terms.\n",
      "I used to have a saying I repeated over and over again...it was \"My Life, My Terms.\" It kept me going through the hard times when I needed to remember that my life was my own.\n",
      "I do believe there is a power greater than myself - it's called the universe - but I do not believe that God is sitting in heaven pulling levers and making things happen down here on earth. And if I am wrong, then so be it...because I have lived a life full of joy, love, and contentment.\n",
      "If there is a hell waiting for me when I die, I will happily go because my life was happy and full of bliss!\n",
      "In fact, it's the best gift you can give to those around you. It takes a strong person to follow their own path, but those who do leave a lasting legacy behind them that inspires others to live their dreams.\n",
      "Labels: Inspirational, Quotes\n",
      "This post is so true! Love it :)\n",
      "I'm glad you enjoyed it. And I hope you have a wonderful day filled with bliss and happiness!!\n",
      "Another great post. Very thoughtful. Your posts are always well written. I enjoy reading them.\n",
      "You are too kind! Thank you for the compliment and I am so happy to hear that you enjoy my posts...it makes this blogging journey worthwhile. :)\n",
      "I really like what you had to say here, it's all so true. Thanks for sharing. :)\n",
      "Thank you for stopping by and I am so glad you enjoyed this post! It was a good reminder for me as well. I just need to remember that\n",
      "llama_print_timings:        load time = 15249.34 ms\n",
      "llama_print_timings:      sample time =   288.56 ms /   512 runs   (    0.56 ms per token,  1774.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4550.98 ms /   265 tokens (   17.17 ms per token,    58.23 tokens per second)\n",
      "llama_print_timings:        eval time = 24543.34 ms /   510 runs   (   48.12 ms per token,    20.78 tokens per second)\n",
      "llama_print_timings:       total time = 29476.94 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce27ab44-b446-4126-8972-7472c658a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569230\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find\n",
      "your gift is happiness. I don't think it's too much to\n",
      "ask.” ~ B. Malcolm\n",
      "Labels: bart malcolm quote, inspirational quotes, lifestyle quotes, meaning of life quote, motivational quotes, quotes about gifts, quotes about success, wisdom, wise words of wisdom\n",
      "The only way to do great work is to love what you do. If you haven't found it yet, keep looking. Don't settle... ~ Steve Jobs\n",
      "Labels: inspirational quotes, leadership quotes, motivational quotes, steve jobs quote, success quotes, wisdom, wise words of wisdom\n",
      "“A person who never made a mistake never tried anything new.” ~ Albert Einstein\n",
      "Labels: albert einstein, inspirational quotes, making mistakes quotes, motivational quotes, trying something new quote, wisdom, wise words of wisdom\n",
      "Being deeply loved by someone gives you strength, while loving someone deeply gives you courage. ~ Lao Tzu\n",
      "Labels: courage quotes, inspirational quotes, lao tzu quotes, love quotes, motivational quotes, quotes about being deeply loved, quotes about giving strength, quotes about loving someone, wisdom, wise words of wisdom\n",
      "There are no traffic jams along the extra mile. ~ Roger Staubach\n",
      "Labels: inspirational quotes, leadership quotes, motivational quotes, roger staubach quote, success quotes, wisdom, wise words of wisdom\n",
      "I've learned that you shouldn't go through life with a catcher's mitt on both hands; you need to be able to throw something back. ~ Maya Angelou\n",
      "Labels: inspirational quotes, leadership quotes, maya angelou quote, motivational quotes, success quotes, wisdom, wise words of wisdom\n",
      "I do not think much of a man who is not wiser today than he was yesterday. ~ Abraham Lincoln\n",
      "Labels: abraham lincoln, inspirational quotes, leadership quotes, motivational quotes, success quotes, the meaning of life quote, wisdom, wise words of wisdom\n",
      "“The most powerful weapon on earth is the human soul on fire.” ~ Field Marshal Ferdinand Foch\n",
      "Labels: field marshall ferdinand foch, inspirational quotes, leadership quotes, motivational quotes, quotes about a soul on fire, success quotes, wisdom, wise words of wisdom\n",
      "Wisdom and truth are in\n",
      "llama_print_timings:        load time =  3509.44 ms\n",
      "llama_print_timings:      sample time =   295.98 ms /   512 runs   (    0.58 ms per token,  1729.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4655.79 ms /   265 tokens (   17.57 ms per token,    56.92 tokens per second)\n",
      "llama_print_timings:        eval time = 24455.75 ms /   510 runs   (   47.95 ms per token,    20.85 tokens per second)\n",
      "llama_print_timings:       total time = 29499.74 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eec90d1-7b7b-4071-af17-849bcb505108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569267\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find our gift. To find our personal legend and then live it out with passion, because what else are we here for?\n",
      "In his book The Alchemist, by Paulo Coehlo, he talks about finding your Personal Legend, and I love this idea! We all have a gift, a unique talent that God has placed inside us. We’re called to use our gifts in the service of others. And not just that, but we’re called to find joy in them while doing so.\n",
      "So often, especially as women, we neglect our own needs, and this is one of those areas where it happens! I don’t know about you, but I tend to put myself last on the list. If it were up to me, I would never buy another thing for myself again if I could help it! But this is unrealistic, because we actually need to take care of ourselves in order to have the energy to love and serve others.\n",
      "I’m going to share with you some things that I do on a regular basis to find joy through creativity.\n",
      "I am an artist at heart. Not only do I enjoy creating artwork myself, but I love surrounding myself with beautiful art as well. This is why my home is filled with original paintings and sculptures by local artists. It’s also why we make frequent trips to the nearby museum of fine art in our city.\n",
      "So if you find yourself enjoying the arts, there are several ways that you can support your passion. Here are some ideas for how you can find joy through creativity!\n",
      "Take a class – Whether it is music or painting or dancing, you’ll never get better at something unless you practice and learn from others who are more skilled than yourself. So if you want to get better at whatever artistic pursuit you enjoy, look into taking some classes.\n",
      "Join a group – You can also find joy through creativity by joining a group of like-minded people. I love the community that has been created around our local art museum. They organize fun social events in addition to having wonderful exhibits and educational programs. So if you want to get more involved, this is an excellent way to do it!\n",
      "Start your own group – If you can’t find a group that meets your needs, why not start one? You could create a Facebook page for your local artists or writers or musicians. Or maybe you just want to find some friends\n",
      "llama_print_timings:        load time =  3541.55 ms\n",
      "llama_print_timings:      sample time =   292.17 ms /   512 runs   (    0.57 ms per token,  1752.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4581.71 ms /   265 tokens (   17.29 ms per token,    57.84 tokens per second)\n",
      "llama_print_timings:        eval time = 24531.55 ms /   510 runs   (   48.10 ms per token,    20.79 tokens per second)\n",
      "llama_print_timings:       total time = 29499.45 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04719be2-a272-4fc9-aed8-193e4bfe9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569303\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be true to yourself and those around you. And to strive to make this world a better place for future generations.\n",
      "I am very proud of being able to provide a good home for my daughter, who has grown into a beautiful human being with lots of love to give. I am also very proud of the fact that I have been able to build a new life in South Africa after 15 years living abroad. My family and friends are still in Europe.\n",
      "I have always been curious about people who make an impact on others. Whether it is through their art, writing or just by being true to themselves and making a difference. I am inspired when I hear stories of courage, compassion and hope.\n",
      "I love the fact that we live in South Africa – in spite of all its challenges. It is such a beautiful country with so much potential. And I believe that if each one of us makes an effort to be the best version of ourselves, things can only get better from here!\n",
      "What’s your favourite thing about living in Cape Town?\n",
      "The stunning nature and the ocean which make this city unique. It is such a beautiful place to live in – even though I do miss the snow at times…\n",
      "I love all kinds of music, depending on my mood. But I would have to say Jazz and Classic are always great to listen to when you need some quiet time. In general I prefer more upbeat music. When I go out dancing it is usually to electronic music or Latin rhythms.\n",
      "What do you love most about your work? And what is the most challenging part of it?\n",
      "I love that each day at work is so different. I get to meet a lot of interesting people and see many beautiful places across our country – all in one day! The most challenging part is probably the fact that we have to take a lot of photos very quickly, no matter what circumstances or weather conditions we are facing.\n",
      "What’s been your biggest lesson learnt?\n",
      "I believe it takes courage and self-respect to follow your heart. In life you will always experience people who doubt you or want you to fail, but if you stay true to yourself, you can make anything possible!\n",
      "How has motherhood changed you?\n",
      "Before I had my daughter (who is now 18), I thought that I was pretty “together” and very independent. But when she came into the world, I realized that there\n",
      "llama_print_timings:        load time = 32412.95 ms\n",
      "llama_print_timings:      sample time =   290.06 ms /   512 runs   (    0.57 ms per token,  1765.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6600.20 ms /   265 tokens (   24.91 ms per token,    40.15 tokens per second)\n",
      "llama_print_timings:        eval time = 37064.69 ms /   510 runs   (   72.68 ms per token,    13.76 tokens per second)\n",
      "llama_print_timings:       total time = 44047.94 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1730091a-3146-476d-ab57-49f8722c351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569383\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live, love and help others.\n",
      "If you are having a bad day or feeling down please call this number 1-800-273-TALK (8255). You are not alone!\n",
      "The National Suicide Prevention Lifeline provides free and confidential emotional support to people in crisis or distress.\n",
      "Please know that we love you and we want you here with us.\n",
      "I have a 14 year old daughter, and I don't think she fully understands the ramifications of suicide, but she is well aware that there are times when she feels like there isn't hope, or that life will never be better than it is now (of course, as you know, this is not true). I always tell her to call me if things ever get too bad for her.\n",
      "I am so glad you have shared this information with your daughter and the world! If we all could just talk openly about suicide, maybe one person would think twice before taking that final step.\n",
      "Yes, let's talk about this subject more often. It's a shame that people need to suffer in silence because they feel alone or ashamed when they are thinking of ending their lives. I hope by talking about it, we can help those who need us the most.\n",
      "Thank you for sharing this post, and for sharing your story with me too!\n",
      "I was so happy to see you comment on my blog! You have a very nice site here.\n",
      "The reason that I shared my experience is to let others know they are not alone when it comes to feeling suicidal. If people could just open up and talk freely about how they feel, maybe more lives would be saved.\n",
      "Thank you for your kind words. It's very hard to put yourself out there and write such a personal story. I hope by sharing my experience I can help someone else.\n",
      "I want to thank you so much for leaving me this comment on my blog. You have no idea how much it means to me.\n",
      "Thank you for your kind words. They mean more than you know!\n",
      "I'm not sure what happened to the rest of your comment but I agree with everything you said and I thank you, too.\n",
      "I really appreciate your thoughtful post on this topic. Thank you so very much for sharing it.\n",
      "Please call me if you ever need a friend. I am here for you, always!\n",
      "Thank you so\n",
      "llama_print_timings:        load time =  6826.09 ms\n",
      "llama_print_timings:      sample time =   284.84 ms /   512 runs   (    0.56 ms per token,  1797.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6590.11 ms /   265 tokens (   24.87 ms per token,    40.21 tokens per second)\n",
      "llama_print_timings:        eval time = 37019.00 ms /   510 runs   (   72.59 ms per token,    13.78 tokens per second)\n",
      "llama_print_timings:       total time = 43987.15 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351608bd-64ae-4f26-9fb5-ccf3ee5fb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569437\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.”\n",
      "I am a 52 year old woman who has been in recovery from addiction for almost 30 years. My sobriety and my spirituality are paramount, as they were instrumental in saving my life. I was given the gift of writing poetry when I got sober, and I believe that it is one way to bring healing and hope into people’s lives. This blog is about my poetry, and about anything else I feel like sharing with you.\n",
      "Here is a poem I wrote many years ago, which gives me hope even today:\n",
      "There are days when darkness overwhelms me\n",
      "and the light seems so far away, I can no longer see it.\n",
      "I forget to look up at the stars, or into the eyes of those who love me.\n",
      "And then, there is a glimmer, and my soul wakes up.\n",
      "It begins to search for the light inside itself,\n",
      "and suddenly remembers that it has always been shining.\n",
      "Through the cracks in this broken world, the light finds its way.\n",
      "Into each of our hearts, the sun rises again.\n",
      "The darkness cannot overcome it.\n",
      "My life is a journey toward this kind of healing and wholeness. I am grateful to be able to share my experience, strength and hope with others, on this blog and in my book, The Art & Science of Addiction: A Handbook for People in Recovery. May you find your gift and give it away!\n",
      "13 thoughts on “About Tracy”\n",
      "Pingback: Poetry Prompt – Hope | Sarah Ann Hall\n",
      "I like what I’ve read here, Tracy, and I look forward to reading more. Thank you for the work that you do. ���\n",
      "Thank you so much! Much gratitude and appreciation! ����\n",
      "You have a very nice blog and I’m happy to connect with you.\n",
      "I am glad we are connected! Have a wonderful weekend! ����\n",
      "Thank you. You too.\n",
      "Wow, what an inspiring story you tell here Tracy. Congrats on the long-lasting sobriety and thank you for sharing your thoughts and beautiful words with us. It is so nice to meet you. – Paul\n",
      "So glad to connect with you in the blogosphere!\n",
      "llama_print_timings:        load time =  6822.10 ms\n",
      "llama_print_timings:      sample time =   288.80 ms /   512 runs   (    0.56 ms per token,  1772.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6634.13 ms /   265 tokens (   25.03 ms per token,    39.94 tokens per second)\n",
      "llama_print_timings:        eval time = 37148.30 ms /   510 runs   (   72.84 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time = 44165.35 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34085-ceab-48a8-96cb-f47d3f8cc5b3",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303e2727-09d0-47be-818f-2a6519aa5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569492\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live your own truth. To do this, one must learn from their past and know who they are. When you embrace your true self, all that you have experienced has helped shape that person.\n",
      "I think my greatest strength in coaching is allowing clients to understand that there is no right or wrong way of doing things; each person will view situations differently. It’s important for me to help clients find the answers within themselves and their own experiences. When we understand our personal truth, it helps us move forward with confidence and make wiser choices in life.\n",
      "In my life I have had many challenges which has made me who I am today. Some of these challenges include: having a brother die from cancer when I was 14 years old; being sexually abused at the age of 8, and going through domestic violence. My goal is to help others understand that there are ways to use your experiences to grow as individuals and become stronger. When we learn how to deal with our past, we can move forward in life without fear or regret.\n",
      "I am a Certified Professional Life Coach by the International Coaching Federation (ICF), and I have my Masters Degree in Counseling Psychology from Capella University. I also have an Associate Degree in Criminal Justice from San Joaquin Delta College in Stockton, CA.\n",
      "I am a Certified Addiction Specialist with the California Association of Alcoholism and Substance Abuse Counselors (CAASAC). Additionally, I hold certifications as a Domestic Violence Advocate, Advanced Clinical Supervisor, and Group Facilitator for San Joaquin County.\n",
      "As a person who has struggled with addiction in the past, I have been through many of the issues that I now work with clients on. My goal is to help others understand that they are not alone, and that there is always hope.\n",
      "I am married with two daughters and one son. In my spare time I enjoy playing sports, listening to music, spending time with family and friends, and traveling.\n",
      "“Sarah has been a life saver for me! She helped get me off of the addiction of alcohol through therapy and gave me the strength to be sober. I am now 8 months sober thanks to Sarah’s help!” – Jennifer J.\n",
      "“You are an amazing woman, and I know\n",
      "llama_print_timings:        load time = 12128.30 ms\n",
      "llama_print_timings:      sample time =   299.65 ms /   512 runs   (    0.59 ms per token,  1708.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2111.11 ms /   265 tokens (    7.97 ms per token,   125.53 tokens per second)\n",
      "llama_print_timings:        eval time = 13077.54 ms /   510 runs   (   25.64 ms per token,    39.00 tokens per second)\n",
      "llama_print_timings:       total time = 15581.96 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061c4fdf-de31-47b9-a6f7-55873c2b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569523\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your own meaning, and then go out there and try to live up to it.\n",
      "The thing that makes me most nervous about writing a novel is that you're trying to tell a story that you think is worth telling.\n",
      "I don't know if we'll ever see the end of terrorism in our lifetimes, but we have to be willing as a society and as a world to do everything we can in order to try and make sure it doesn't happen.\n",
      "There are some things that I would say are not really my fault, such as being born at 5:30 A.M.\n",
      "The thing about the Internet is there's a lot of noise. There's very little signal.\n",
      "People don't have enough opportunities to be around people who are different from them. And so we become more afraid of those differences than we should be, and that leads to conflict.\n",
      "I was always interested in politics. My dad is a journalist and my mom works for the United Nations, so I grew up with a lot of interesting conversations about world affairs and government and policy going on around me. But at the same time, I like to think of myself as an outsider.\n",
      "There's no reason that we should have to be afraid of people who are different from us because there's so much more in common than what separates us.\n",
      "The biggest mistake that politicians make is they forget what their priorities should be and they start trying to please everybody. I think the biggest problem with politics is that it's not about policy anymore, it's all about politics, which means you don't do the right thing because it's the right thing to do.\n",
      "I always try to avoid cliches. A good line of dialogue doesn't sound like something a person would say in real life; it sounds like someone trying to remember what a person said in real life. The more you can write things that don't sound like they came from a screenwriter, the better off you are as an artist.\n",
      "I think every writer is influenced by other writers, whether they want to admit it or not. It's impossible not to be. I think my work reflects all of my influences and experiences, good and bad.\n",
      "I always try to avoid cliches. A good line of dialogue doesn't sound like something a person would say in real life; it\n",
      "llama_print_timings:        load time =  2415.49 ms\n",
      "llama_print_timings:      sample time =   290.95 ms /   512 runs   (    0.57 ms per token,  1759.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2103.55 ms /   265 tokens (    7.94 ms per token,   125.98 tokens per second)\n",
      "llama_print_timings:        eval time = 13039.11 ms /   510 runs   (   25.57 ms per token,    39.11 tokens per second)\n",
      "llama_print_timings:       total time = 15527.68 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb37cbe3-16ab-4f63-9903-2e7951ce751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569543\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give, share and love.\n",
      "I am a very passionate person about what I do; and so I want everyone else to be as well!\n",
      "I truly love meeting new people and helping others feel more confident in their appearance. I love the thrill of putting together something for someone that they will love, or even better yet, seeing how much fun it is for them to wear it! I am always looking forward to working with you; whatever your style may be!\n",
      "I believe every woman should look and feel her best! You only get one chance at this life-and why not make the most out of it? If you are not feeling confident, then what are you doing?\n",
      "I want to help you find something that makes you happy.\n",
      "Please come in and say hello!\n",
      "*Bridal consultations by appointment only: please call 802.569.4713 or contact us for more information.\n",
      "Jennifer is an absolute joy to work with! She's extremely knowledgeable about the latest styles, trends and colors, and she truly puts her heart into everything she does. We have worked together on a variety of projects over the years, from small boutique fittings to large bridal shower events and I could not be happier that we connected! She's always so easygoing, fun and professional (and super sweet), but also knows how to get down to business when needed. I would recommend her to anyone looking for a stylist they can call their own!\n",
      "It was my great pleasure to work with Jennifer in the Bridal Shop of Saranac Lake on several occasions. She is very knowledgeable and professional, and she has an eye for detail that is not often found in today's marketplace. Her calm demeanor and warm smile always put her clients at ease - she truly is a joy to work with.\n",
      "Jennifer was an absolute pleasure to work with! I had been looking for the perfect dress for my wedding for months and after one meeting with Jennifer, I knew it would be here. She has a great eye for detail and style. Her calm demeanor put me at ease right away! My mom came along as well and she also loved her, so that definitely makes a difference when you are trying on dresses and having to share them with someone else.\n",
      "My bridesmaids were a little nervous about the whole process but Jennifer made it fun\n",
      "llama_print_timings:        load time =  2443.57 ms\n",
      "llama_print_timings:      sample time =   292.87 ms /   512 runs   (    0.57 ms per token,  1748.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2096.09 ms /   265 tokens (    7.91 ms per token,   126.43 tokens per second)\n",
      "llama_print_timings:        eval time = 13087.77 ms /   510 runs   (   25.66 ms per token,    38.97 tokens per second)\n",
      "llama_print_timings:       total time = 15569.78 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33cb1a63-f863-4a7f-b627-6c76e534f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569564\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be useful, productive and help those in need.\n",
      "I was born in a little town called Guadalajara Mexico on November 17th 1984. My father was 52 years old when he was diagnosed with Leukemia (bone marrow cancer) and was given less than 6 months to live. The doctors told my dad that the only way for him to survive was to receive a bone marrow transplant from someone who had the same blood type as him, which unfortunately is very rare. Fortunately, I was a perfect match and on December of 2001 I became his donor. My father received the bone marrow transplant and lived another 7 years until he passed away in September of 2008.\n",
      "My father's passing was devastating for me but as hard as it was, that experience made me realize how short life is so we need to make every second count. I believe that the best way to fulfill our dreams and goals is by helping others. It is my purpose in life to give back and help others achieve their goals because of what I received from my dad.\n",
      "I have been working as a real estate agent for 10 years now but at the beginning, I had no idea how I was going to be successful. All I knew was that I had a passion for sales and helping people. As I learned through trial and error on how to become a great agent, I decided to teach other agents my system which later became the foundation of this book. It has been amazing to see so many other real estate professionals grow and prosper in their careers thanks to what they have learned from me.\n",
      "In 2015 I was invited by my friend Juan Pablo Galavis as a speaker at the \"Juan Pablo Galavis Foundation\" annual conference. He is one of my favorite clients who has accomplished so much in his life and continues to inspire others every day. This foundation focuses on helping kids with cancer and their families go through this difficult time. I was very honored to be part of this amazing event and give a little speech about how important it is for us to help one another.\n",
      "I hope you are able to get inspired by reading my story, but more importantly, that you apply the lessons in this book so that your dreams can come true too!\n",
      "My name is Yohana\n",
      "llama_print_timings:        load time = 23059.74 ms\n",
      "llama_print_timings:      sample time =   285.28 ms /   512 runs   (    0.56 ms per token,  1794.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2852.48 ms /   265 tokens (   10.76 ms per token,    92.90 tokens per second)\n",
      "llama_print_timings:        eval time = 18193.93 ms /   510 runs   (   35.67 ms per token,    28.03 tokens per second)\n",
      "llama_print_timings:       total time = 21424.43 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250c511c-864d-4b20-8d87-86f2409b277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569612\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.\"\n",
      "—Pablo Picasso\n",
      "# 1\n",
      "I had no idea what my father was going to say, but I knew it would be bad news.\n",
      "\"Dad,\" I said, \"how's Mom?\"\n",
      "\"She's fine,\" he answered. Then he paused for a moment.\n",
      "\"What is it?\" I asked him.\n",
      "\"She had some tests done. They found out that she has cancer.\"\n",
      "I thought I was going to be sick.\n",
      "\"Oh no, Dad!\" I cried out. \"That's terrible news!\"\n",
      "My father looked at me and said, \"Isn't it?\"\n",
      "\"Yes,\" I replied. \"But what about the treatment? Can they do anything?\"\n",
      "\"Well, they're going to operate on her.\"\n",
      "I knew that wasn't good enough.\n",
      "\"But when? When does she have to go in for surgery?\"\n",
      "My father sighed and said, \"I don't know yet; we'll be talking about it later.\"\n",
      "\"Dad,\" I asked him again, \"when can you tell us more?\"\n",
      "\"We should all be at the hospital together. We need to talk as a family.\"\n",
      "\"Okay,\" I answered, trying not to cry.\n",
      "I knew there was something else that my father wanted to say and he finally got it out.\n",
      "\"Danny,\" he said, \"you know your mother is very sick. She may even die if we don't do this surgery right away.\"\n",
      "I looked at my father and started to cry.\n",
      "\"What should I do?\" I asked him. \"How can I help her get better?\"\n",
      "He put his arms around me. He was trying to comfort me, but all I could think about was my mother. She was the most special person in my life and now she might be dying! I knew that if there were anything I could do, I would do it—but what? What could I possibly do for her?\n",
      "\"Oh no,\" I cried out. \"No!\"\n",
      "My father held me tightly. He didn't say a word. He just sat next to me and waited until my tears stopped flowing. Then he said softly, \"Danny, your mother is very sick but there are people who can help her get better.\"\n",
      "I looked at him and asked the\n",
      "llama_print_timings:        load time =  4484.96 ms\n",
      "llama_print_timings:      sample time =   288.89 ms /   512 runs   (    0.56 ms per token,  1772.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2825.01 ms /   265 tokens (   10.66 ms per token,    93.81 tokens per second)\n",
      "llama_print_timings:        eval time = 18254.75 ms /   510 runs   (   35.79 ms per token,    27.94 tokens per second)\n",
      "llama_print_timings:       total time = 21462.28 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f792b2-0f7e-4b93-9d3e-69d4479f6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569641\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to become who you are, to share your gifts and talents with the world in a positive way. To live by example and always choose love above fear.\n",
      "I am not here to tell you how to think or what to do, but instead to guide you to the truth of your own heart and soul so that you may learn for yourself. I will show you how to connect with yourself on a deep level so that you can be at ease in this world no matter what is going on around you.\n",
      "I am a Soul Guide and Intuitive Counselor who has been teaching others how to awaken their own intuitive abilities since 2013 through my free monthly newsletter as well as classes, workshops, and private sessions. My mission is to help as many people as possible connect with themselves on a deeper level so that they can feel fulfilled in life and be the best version of themselves.\n",
      "I am passionate about helping women awaken their intuition and create a more nourishing and loving relationship with themselves through connecting with their own inner voice. This is my lifelong mission.\n",
      "When I was 19, I fell into a deep depression that lasted for many years. While it felt like the world was caving in on me, one day I made a decision to try something different and started to meditate. That simple act of taking quiet time each morning changed my life forever as it opened up a whole new world within me and allowed me to access and trust my intuition more than ever before. It also helped me heal myself and feel better in my own skin.\n",
      "As I began to awaken to my inner voice, I realized that my purpose was to help others. I started teaching meditation classes through workshops and eventually became a certified teacher of Kundalini Yoga. I began guiding others on the intuitive path as well by helping them connect with their own intuition.\n",
      "I also developed an energy healing technique called Soul Release which is a simple yet powerful process that helps people release negative experiences from their past so they can feel more free and light in their body.\n",
      "My soul’s purpose is to help other women awaken to their true inner voice and connect with themselves on a deeper level so they can be the best version of themselves and live a life full of love and joy. My mission is to empower, educate and guide as many women as I can through teaching, speaking\n",
      "llama_print_timings:        load time =  4459.61 ms\n",
      "llama_print_timings:      sample time =   297.83 ms /   512 runs   (    0.58 ms per token,  1719.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2843.14 ms /   265 tokens (   10.73 ms per token,    93.21 tokens per second)\n",
      "llama_print_timings:        eval time = 18244.74 ms /   510 runs   (   35.77 ms per token,    27.95 tokens per second)\n",
      "llama_print_timings:       total time = 21479.05 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd03d5a9-978f-454f-9dde-da67377225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569669\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away\n",
      "A new year and a time for reflection on what has gone before, what may be coming and how we should respond. I don’t think any of us would have predicted the events which occurred in 2016, nor indeed the events which are already unfolding this month. Whatever your views on Brexit and Trump, it is unlikely that anyone could have imagined these occurrences. If nothing else they demonstrate the uncertainty of life.\n",
      "The past year has seen the start of a new era at Fiduchi with our move to new offices in St Helier. This has provided us with much needed extra space and we are now able to meet the increasing demand for our services from both existing and new clients. A number of new staff have been employed over the past year, including two graduates who are now undertaking their Chartered Institute of Securities & Investment (CISI) examinations as part of their training. These changes have enabled us to provide an even better service for our clients while maintaining our personal approach to business.\n",
      "2017 is already proving to be a busy one both here at Fiduchi and in the outside world. We look forward to working with you to deliver the best possible results for your family and your company.\n",
      "Graham Montgomery, CEO, Fiduchi\n",
      "Fiduchi: Our move to St Helier\n",
      "The past year has seen a number of changes at Fiduchi, including our move from 21 Hill Street to new offices in Bath Street. The additional space this provides means we can now offer better facilities for clients and staff alike and we are able to grow the business even further. We have also been delighted with the response to our rebranding exercise which has created a fresh, modern look for Fiduchi – as well as giving us an identity to match our new home!\n",
      "A new era for Fiduchi: 2016 marks a significant year in the history of Fiduchi. Not only did we celebrate our 15th anniversary this month, but also the launch of our rebranded company identity – and our exciting move to larger offices in St Helier.\n",
      "The new logo is based on the original design by founder Graham Montgomery using the same colours and shapes, but with a more contemporary feel. The word\n",
      "llama_print_timings:        load time = 66434.96 ms\n",
      "llama_print_timings:      sample time =   296.65 ms /   512 runs   (    0.58 ms per token,  1725.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4649.87 ms /   265 tokens (   17.55 ms per token,    56.99 tokens per second)\n",
      "llama_print_timings:        eval time = 32534.72 ms /   510 runs   (   63.79 ms per token,    15.68 tokens per second)\n",
      "llama_print_timings:       total time = 37575.66 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a89a21-41b0-4075-9ed4-78c9c561bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569778\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn, grow and evolve. If we don’t continue to try new things or take on new experiences and challenges how will we ever expand?\n",
      "I have always been fascinated by the artistry involved in creating a beautiful face, and the way it can change someone's perspective not only of themselves, but also of those around them. There is an energy that comes from enhancing your natural beauty: a confidence and glow that you can’t help but shine.\n",
      "I believe the key to healthy skin is balance: balancing proper hydration with exfoliation and protection is essential for maintaining a youthful appearance. I want my clients to feel beautiful, confident and radiant both inside and out. I am committed to providing exceptional service and aesthetic results through education and customized skincare treatments tailored specifically to you and your skin care needs.\n",
      "I am the only licensed esthetician in the country trained and certified in Ecocert Organic Facials which are recognized worldwide for their high quality, scientifically proven skincare products. I have also completed advanced training at the International Dermal Institute (IDI). My experience includes working with numerous skin conditions including acne, rosacea, sun damage and aging.\n",
      "My passion is to help you feel beautiful from the inside out. I look forward to working with you!\n",
      "To book an appointment call or text (503) 863-7519. You may also reach me via email at [email protected].\n",
      "I offer free consultations so we can discuss your goals and decide on a treatment plan that is right for you! I look forward to meeting you soon. Click here for contact information.\n",
      "\"When I met Jessica, my skin was in the worst condition it had ever been in. It was red, inflamed, broken out, sensitive and flakey. She immediately sat me down and explained what she thought would help me, which was a combination of extractions, masks and a customized skincare routine for me to follow at home. After only one treatment with her, I saw great improvement in my skin! I’ve been seeing Jessica regularly ever since and I am so happy that I found her. She has helped me maintain clear, healthy skin that I know I can count on. I never have a bad day when I leave her office.\"\n",
      "\"I'\n",
      "llama_print_timings:        load time =  9847.02 ms\n",
      "llama_print_timings:      sample time =   294.48 ms /   512 runs   (    0.58 ms per token,  1738.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4653.56 ms /   265 tokens (   17.56 ms per token,    56.95 tokens per second)\n",
      "llama_print_timings:        eval time = 32502.68 ms /   510 runs   (   63.73 ms per token,    15.69 tokens per second)\n",
      "llama_print_timings:       total time = 37544.71 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dda4493-8622-4e1c-8896-a63f40cc4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569829\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 32 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find happiness in everything you do.\n",
      "My name is Chelsea, and I am a licensed Esthetician, with my degree from Douglas J Aveda Institute (2013). After graduating I worked for a few different salons, learning all I could. I have had the honor of working with some amazing people, that taught me everything they knew. From there I moved to California for the next 4 years, where I took my career even further and grew as an Esthetician. During that time is when I fell in love with waxing. The art of Brazilian waxing has always been a passion of mine. I have worked with some amazing people in the industry while working out there, but I have always had my sights set on making something for myself.\n",
      "I started this company from nothing, with an idea and a dream to be able to create a place where everyone feels like family.\n",
      "Now here I am today, 8 years later, with my own salon, full of talented Estheticians, that are truly passionate about what they do.\n",
      "We have all been working hard to make this company what it is today, and we will continue to grow as a team! We hope you enjoy our services as much as we love doing them! Welcome to the family!!\n",
      "Copyright © 2019 Skin Care by Chelsea - All Rights Reserved.\n",
      "Skin Care By Chelsea, 3851 South Ave Suite C, Yuba City, CA, 95993(530) 674-5252chelsea@skincarebychelsea.com\n",
      "website design by Linda Elyse Designs - copyright 2019 Linda Elyse Designs - all rights reserved.\n",
      "Skin Care By Chelsea, 3851 South Ave Suite C, Yuba City, CA, 95993(530) 674-5252chelsea@skincarebychelsea.com website design by Linda Elyse Designs - copyright 2019 Linda Elyse Designs - all rights reserved.\n",
      "Welcome to Skin Care By Chelsea! We are so excited to have you visit our website. If this is your first time here\n",
      "llama_print_timings:        load time =  9985.53 ms\n",
      "llama_print_timings:      sample time =   295.17 ms /   512 runs   (    0.58 ms per token,  1734.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  4571.12 ms /   265 tokens (   17.25 ms per token,    57.97 tokens per second)\n",
      "llama_print_timings:        eval time = 32578.38 ms /   510 runs   (   63.88 ms per token,    15.65 tokens per second)\n",
      "llama_print_timings:       total time = 37538.88 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a43d3544-82dc-4b04-a60a-05f82c0b8883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689569881\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "  Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
