{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ab0e6f-86d6-41f5-a0cd-aaee6226fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 02:47:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    19W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  Off |\n",
      "|  0%   29C    P8    16W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:82:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    19W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263848528 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388112b9-bf0d-4cc1-85c2-4e8e72f20df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40847d9f-7b14-44d5-b4df-8ad885a33a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19437e33-5986-4681-9cce-d497e03adb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kllama.cpp:2208:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‘\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K’ may be undefined [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsequence-point\u0007-Wsequence-point\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2208 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
      "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b76e0c-6e15-4d0b-b328-67556b448d82",
   "metadata": {},
   "source": [
    "### Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef56705-84d3-47e0-98b0-181273f6f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562050\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live life to its fullest.\n",
      "I am a woman that has grown in knowledge and wisdom over the years, I have a Masters Degree from Regent University, School of Psychology and Counseling. I have been married for 35 years and have two wonderful children that are now adults. I believe the meaning of life is to live life to its fullest.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your life as well as the things that drain you emotionally and mentally. Let's work on ways to create more of what brings us joy and less of the things that steal our joy.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your life as well as the things that drain you emotionally and mentally. Let’s work on ways to create more of what brings us joy and less of the things that steal our joy.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your life as well as the things that drain you emotionally and mentally. Let’s work on ways to create more of what brings us joy and less of the things that steal our joy.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your life as well as the things that drain you emotionally and mentally. Let’s work on ways to create more of what brings us joy and less of the things that steal our joy.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your life as well as the things that drain you emotionally and mentally. Let’s work on ways to create more of what brings us joy and less of the things that steal our joy.\n",
      "I am here to help you explore your life and how it may be affecting you in a positive or negative way, together we can look at the things that bring joy into your\n",
      "llama_print_timings:        load time =  5766.85 ms\n",
      "llama_print_timings:      sample time =   411.22 ms /   512 runs   (    0.80 ms per token,  1245.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1876.57 ms /   265 tokens (    7.08 ms per token,   141.22 tokens per second)\n",
      "llama_print_timings:        eval time =  8707.33 ms /   510 runs   (   17.07 ms per token,    58.57 tokens per second)\n",
      "llama_print_timings:       total time = 11121.92 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd4adf2-f70f-4e26-8cc6-98e57e30db26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562070\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to discover your purpose, live your potential and then give back.\n",
      "In 2016, I decided to focus more on doing this. I started a blog called “The Purpose Project” where I share stories of people who have discovered their purpose in this world. From these individuals, we can all learn how to live our purpose and achieve our potential.\n",
      "Inspired by the great quotes throughout history, I also created a series of motivational posters with some of my favorite quotes from famous people around the world. My hope is that these quotes will inspire you to find your own path in life.\n",
      "I am a firm believer that we are not here on this earth just for ourselves — but because we were meant to give back and help make our world a better place.\n",
      "This led me to create “The Purpose Project”. It’s an initiative where I will interview people who have found their purpose in life, and then share the stories with you so that we can all learn how to live our potential!\n",
      "There is no greater reward than helping others find their purpose and live their potential. And this is why I created The Purpose Project – an initiative meant to help us all discover our purpose in life and achieve our potential.\n",
      "My blog, The Purpose Project, features amazing people who have discovered their purpose in life. From these individuals, we can learn how to live our potential!\n",
      "Learn more about this project here: “The Purpose Project”.\n",
      "Inspired by the great quotes throughout history, I also created a series of motivational posters with some of my favorite quotes from famous people around the world. My hope is that these quotes will inspire you to find your own path in life!\n",
      "Learn more about this project here: “Motivation Monday”.\n",
      "My goal is to inspire people like you to help make our world a better place, by following your purpose and living your potential!\n",
      "I believe we are all meant for something greater than ourselves. We each have a purpose in life, and we each have the power to live our full potential and give back.\n",
      "But first, we must discover our own unique purpose. And once we do that, we will be able to help make our world a better place!\n",
      "I want to encourage you today to start taking action toward living out your purpose in life. You may not know what it is yet – but I believe you can discover it through lots of\n",
      "llama_print_timings:        load time =  1249.91 ms\n",
      "llama_print_timings:      sample time =   386.15 ms /   512 runs   (    0.75 ms per token,  1325.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1783.49 ms /   265 tokens (    6.73 ms per token,   148.58 tokens per second)\n",
      "llama_print_timings:        eval time =  8790.67 ms /   510 runs   (   17.24 ms per token,    58.02 tokens per second)\n",
      "llama_print_timings:       total time = 11087.51 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e31dae18-81cd-4e97-9c5d-dc830a7b7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562086\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to add value and purpose in this world.\n",
      "To be a good husband, father, brother and son. To leave this place better than when we found it. To make a difference.\n",
      "Those are my goals.\n",
      "And those are the reasons I work 70 hours a week, why I'll go out of my way to help a friend in need, or even do something crazy like climb 14ers (mountains over 14,000 feet) for charity.\n",
      "I'm a husband and father first. My wife is also one heck of an entrepreneur and she has helped me along the way. She's my inspiration to be better every day. She's taught me how to take care of business in a classy manner while staying true to myself.\n",
      "I'm a believer that the more you help others, the more it comes back around and helps you.\n",
      "And I believe we can all make an impact if we want to. We just have to be willing to give.\n",
      "So this is my story. This is what makes me tick. This is who I am.\n",
      "I hope it inspires you to make a difference in the world, too!\n",
      "The purpose of life is not simply to exist but to live meaningfully. The only way we can do that is by helping others achieve their dreams and goals.\n",
      "My name is Joshua Tongol and I'm from Albuquerque, New Mexico.\n",
      "I was born in 1986 to a Filipino mother and Mexican father.\n",
      "I grew up in the South Valley of Albuquerque (a rough part of town). When I was young my dad would take me out to do manual labor jobs like landscaping or construction work, while he did his own handyman business on the side.\n",
      "When I was 12 years old, my father passed away from a heart attack. He died before his time. He was only 47. It was a very hard day for me and my family.\n",
      "At that young age, I had to learn how to become an adult. I had to grow up fast. It was one of the most defining moments in my life.\n",
      "After his death, I didn't know what to do with myself so I decided to put all of my energy into sports and studying hard to get good grades.\n",
      "I went from playing little league\n",
      "llama_print_timings:        load time =  1236.24 ms\n",
      "llama_print_timings:      sample time =   409.01 ms /   512 runs   (    0.80 ms per token,  1251.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1801.57 ms /   265 tokens (    6.80 ms per token,   147.09 tokens per second)\n",
      "llama_print_timings:        eval time =  8751.28 ms /   510 runs   (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:       total time = 11089.03 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd12983-0e77-41ec-bcb8-ff63e18f0163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562101\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and content.\n",
      "I can’t say that I’ve always been, but I certainly am now.\n",
      "I’m a mother to three kids and wife to one man.\n",
      "I work full time as an accountant in a small town.\n",
      "I love the little things…a good meal with my family, a quiet evening at home without distractions or demands, and time alone with my husband.\n",
      "I’ve been blogging for a year now and I love it!\n",
      "Sometimes I think about getting out of bed early enough to take some “before” pictures before the kids get up so that I can show you all what our lives are like every day…..but most mornings I sleep in.\n",
      "I hope that if I’m quiet for a while and stop blogging, that doesn’t mean that something bad happened. I don’t want anyone to worry!\n",
      "My husband is awesome with kids. He is the reason we are able to stay home and homeschool.\n",
      "We have three girls, ages 5, 3, and 1. They are my whole world.\n",
      "I’m a little obsessed with Pinterest and it shows up in my blog posts quite often. I’m also a little bit of a DIY addict….whenever possible. I love to create things!\n",
      "My kids call me “Mama” but most other people call me either Kristi or Momma.\n",
      "I like to go hiking with my family and we are always looking for new trails.\n",
      "We live in the country and I LOVE it here….but I miss visiting my parents often….and my sisters and their families!\n",
      "My two youngest girls were born at home.\n",
      "I’m an avid reader and a book junkie!\n",
      "I don’t have a lot of friends, but I love the ones I do have dearly.\n",
      "If you ever want to talk about anything, please feel free to email me at kristiwilliamson@gmail.com . I’d be happy to hear from you!!\n",
      "Thank you for stopping by my blog! My husband is awesome with the children as well…I don’t know what we would do without him. He has a very hard job and doesn’t get nearly enough credit for it either!\n",
      "Thanks for introducing yourself, Kristi – I love your\n",
      "llama_print_timings:        load time =  9718.94 ms\n",
      "llama_print_timings:      sample time =   387.94 ms /   512 runs   (    0.76 ms per token,  1319.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2224.70 ms /   265 tokens (    8.40 ms per token,   119.12 tokens per second)\n",
      "llama_print_timings:        eval time = 11702.17 ms /   510 runs   (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_print_timings:       total time = 14442.58 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47b6ecb-c212-4e12-bbd7-1ec4639bfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562129\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living out the Great Commandment, which Jesus gave us as a summary of all that God requires:\n",
      "to love the Lord your God with all your heart, soul, and mind;\n",
      "and to love your neighbor as you do yourself.\n",
      "And then we have the Great Commission, given by Jesus to his disciples after he had risen from the dead and before his ascension into heaven (Matthew 28:16-20):\n",
      "I will send you what my Father promised; but stay here in the city until you have been clothed with power from on high.” When he had said this, he was taken up into a cloud while they were watching, and they could no longer see him. As they strained to see him rising into heaven, two white-robed men suddenly stood among them. “Men of Galilee,” they said, “why are you standing here staring into space? Jesus has been taken from you into heaven, but someday he will return in the same way you have seen him go!”\n",
      "When we put these together, this is what I think God is saying to us:\n",
      "God’s desire is that all people love Him with their whole being.\n",
      "It is in loving Him that we come to know our neighbors and care for them as we would ourselves.\n",
      "And, it is through the love of Christ that we are enabled to live lives of service.\n",
      "When I was a freshman at Houghton College, some friends and I came up with a little acronym, which we called “the 3 S’s”: Seeking Salvation, Serving Others, Spreading the News. This has been an ongoing theme for me all these years later.\n",
      "When I was in seminary, I was very drawn to Christian Community Development (CCD) and its vision of the Kingdom. For a time, I considered a CCD-type ministry as my vocation. But God had other plans.\n",
      "When we joined our church, the first sermon series that we heard was one called “the 3 Cs.” In this series we learned about the Church’s call to Community, Compassion, and Celebration. Once again, this theme resonated with me: God is calling his people (that would be us) to live in community for each other, with compassion, and even joyfully!\n",
      "So what does it mean\n",
      "llama_print_timings:        load time =  1774.90 ms\n",
      "llama_print_timings:      sample time =   384.71 ms /   512 runs   (    0.75 ms per token,  1330.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2327.43 ms /   265 tokens (    8.78 ms per token,   113.86 tokens per second)\n",
      "llama_print_timings:        eval time = 11693.35 ms /   510 runs   (   22.93 ms per token,    43.61 tokens per second)\n",
      "llama_print_timings:       total time = 14532.24 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49166db8-e77c-4e15-8194-9452eff672c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562149\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and useful.\n",
      "I believe in being true to one’s word.\n",
      "I believe we should not judge people by their religion or race, but by their character. We cannot change our religion or our race but we can work on our characters.\n",
      "I believe that if we are ever going to have peace on earth it will come through the teachings of the great prophets and if you look at the life of these men, they all had one thing in common – they were all persecuted. They taught love and non-violence but they themselves were not accepted in their own societies.\n",
      "I believe that we must give 10 times more than what we expect to receive. If you give a little to someone else, you will be surprised at how much you will get in return. We may not be able to help everyone, but everyone can help someone.\n",
      "I believe we should be good not because of fear of punishment or hope for reward, but just because it is the right thing to do. I believe we should try to understand others and their needs before judging them. We are all children of God and have a place in his kingdom. The differences between us are superficial; what really matters is that we care about each other.\n",
      "I believe we must remember that we were first loved by our parents so we can love the people around us, no matter who they are or what they do. I believe that we should be grateful to God for whatever he gives us. We may not have everything but we need to count our blessings and be thankful for the ones we have.\n",
      "I believe in being good and doing good to others so that we will leave this world better than how we found it.\n",
      "I believe that if you work hard, you will achieve success as God has promised us. The most important thing is not what you do but how you do it – with love for your fellow man and the desire to help him. And remember: When you get the chance to help someone, don’t ask ‘What will happen to me?’ just do it.\n",
      "I believe life is a struggle and we need to struggle harder than ever before in order to succeed. We should not give up after one failure but try again and again until we achieve our goals. We must take risks – without taking risk, there can be no progress. And remember: the bigger the risk, the bigger the reward.\n",
      "I believe that we are all different and have\n",
      "llama_print_timings:        load time =  1757.71 ms\n",
      "llama_print_timings:      sample time =   384.66 ms /   512 runs   (    0.75 ms per token,  1331.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2375.68 ms /   265 tokens (    8.96 ms per token,   111.55 tokens per second)\n",
      "llama_print_timings:        eval time = 11593.85 ms /   510 runs   (   22.73 ms per token,    43.99 tokens per second)\n",
      "llama_print_timings:       total time = 14481.23 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bd026e-0584-4617-abb5-9777259505da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562168\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life a meaning.\n",
      "I’m still trying to figure out what it means to be a woman in this world, and I will probably never figure it all out. But at least I am starting the process. The journey to self-discovery begins with the first step; it doesn’t have to be perfect, just purposeful.\n",
      "You are a strong, independent woman who doesn’t need a man? Good for you. Now please don’t go around emasculating every guy you see because they don’t fit that standard… that’s not feminism. That’s being an asshole. Feminists do not hate men. We don’t want to be men. We just want to be treated like people who can think for ourselves, make our own choices and not be objectified or sexualized because of them.\n",
      "I was born to love not hate, to save not destroy, to lift people up not put them down. I am a lover not a fighter. I have so much love in my heart but people keep trying to take it away from me. But no one can take my love away unless I let them.\n",
      "I’m the type of girl that will steal your boyfriend, not because he’s better than you, but because she thinks she deserves better than you.\n",
      "— Anonymous, 30 Quotes That Will Encourage You To Love Yourself And Be Independent\n",
      "LoveSelf-LoveIndependenceFeminismWomen's RightsActivistsStrong WomenEmasculationObjectificationSexualization\n",
      "I believe the meaning of life is to give life a meaning. I do not know what meaning it is, but as long as we are alive, we must search for this meaning and make it our own.\n",
      "— Rieko Saibara, via www.goodreads.com\n",
      "LifeMeaningPurpose\n",
      "You are my favorite hello and hardest goodbye.\n",
      "— Anonymous, 32 Quotes To Remind You Of Why You’re So Damn Great\n",
      "Quote of the MomentHelloGoodbyeLove\n",
      "I don’t want a boyfriend to hold me back from living. I want a boyfriend who will inspire me to live more.\n",
      "— Anonymous, 30 Quotes That Will Make You Feel Good About Being Single\n",
      "SingleIndependenceFreedom\n",
      "llama_print_timings:        load time = 23969.09 ms\n",
      "llama_print_timings:      sample time =   413.55 ms /   512 runs   (    0.81 ms per token,  1238.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3809.93 ms /   265 tokens (   14.38 ms per token,    69.56 tokens per second)\n",
      "llama_print_timings:        eval time = 20233.75 ms /   510 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
      "llama_print_timings:       total time = 24584.15 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710338f3-686c-4fd4-a5ca-c25984db27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562221\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning.\n",
      "I am a fan of “what if” questions. In fact, this blog was inspired by my curiosity about how we each choose to live our lives. For me, it is impossible to write about the meaning of life and not include the concept of purpose. Purpose is something that gives your life meaning—it’s why you get out of bed in the morning (other than because your alarm clock went off).\n",
      "The concept of purpose has become a popular topic in recent years. I’m sure we have all seen countless TED Talks and other lectures about finding your purpose. As I write this, I am listening to one on YouTube as background music. It is by an author named Richard Leider who wrote a book called Purpose: The Extraordinary Benefits of Living with Passion and Meaning.\n",
      "My question for you today is: Do you have a personal purpose?\n",
      "If you’re like me, then the answer is probably a bit tricky to nail down because it can change over time. I am on my third or fourth iteration of a “purpose statement” and I believe each one has been an improvement over its predecessor. In that way, I think of purpose as something to aspire to rather than something finite. As we grow and learn, our purposes should evolve as well.\n",
      "What are the things you care about?\n",
      "If you had unlimited resources, what would your life look like?\n",
      "What do people come to you for?\n",
      "For me, my most current purpose statement is: To provide guidance and support to others as they strive to achieve their personal and professional goals. And I am writing this blog in the hope that it will help me achieve that goal.\n",
      "Why is Purpose Important Anyway?\n",
      "In his TED Talk, Richard Leider says that people who have a purpose are happier, healthier, and live longer than those who do not. He also points out that we feel more connected to others when we’re living with purpose. In fact, he calls finding your purpose the “most intimate connection of all” because it is a way of connecting with yourself. So, if you are looking for a reason to find or refine your personal purpose statement, there you go!\n",
      "In my experience, writing down your purpose can be extremely helpful in clarifying what you want to do and how you will do it. I think that when we write our purpose statements down\n",
      "llama_print_timings:        load time =  3327.96 ms\n",
      "llama_print_timings:      sample time =   386.87 ms /   512 runs   (    0.76 ms per token,  1323.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3713.76 ms /   265 tokens (   14.01 ms per token,    71.36 tokens per second)\n",
      "llama_print_timings:        eval time = 20326.42 ms /   510 runs   (   39.86 ms per token,    25.09 tokens per second)\n",
      "llama_print_timings:       total time = 24554.28 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b585f81c-99a5-4330-b46d-4a05c2c555db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562252\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning. We have a choice in how we spend our time.\n",
      "We can create something beautiful, or do something that has a purpose and makes us feel good about ourselves.\n",
      "Or we can do nothing. Nothing at all. And then we don’t get to feel anything other than the emptiness of wasted time.\n",
      "I believe it is important to be in control of our own time. When our time is out of our hands, we are not in charge of how we spend it. We are not deciding if we want to do something that has a purpose or that makes us feel good about ourselves. Instead, we are doing things that we don’t necessarily choose to do and often things we really don’t want to be spending our time on.\n",
      "We make choices everyday about how we spend our time. The choice is up to you. What will you create? How will you give your life meaning? We can either spend our time in a way that has purpose, or we can waste it.\n",
      "What do you think the meaning of life is? Comment below and share your thoughts!\n",
      "Filed Under: Life Tagged With: Meaning Of Life\n",
      "I believe that the meaning of life is to live each day with a grateful heart, love others as Christ loves us, and live in such a way that we can be proud of ourselves. Thanks for sharing your post. Visiting from SITS Sharefest.\n",
      "You are welcome! I agree with what you said about living with gratitude for everyday. It is so important to remember this because when it comes down to it, no one is going to remember all the things that we did wrong or that didn’t work out in our favor. Instead they will remember how we made them feel and if we were a positive influence on their life. Being grateful for what you do have really makes a difference!\n",
      "I believe that the meaning of life is to live each day with a grateful heart, love others as Christ loves us, and live in such a way that we can be proud of ourselves. Thanks for sharing your post. Visiting from SITS Sharefest. I hope you are having a great weekend!\n",
      "Melissa @ Mommy Living the Life of Riley says\n",
      "I think the meaning of life is to leave it better than we found it. To make a positive difference in the world and hopefully inspire others to do so as well. Thanks for sharing your post\n",
      "llama_print_timings:        load time =  3352.52 ms\n",
      "llama_print_timings:      sample time =   387.23 ms /   512 runs   (    0.76 ms per token,  1322.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3758.03 ms /   265 tokens (   14.18 ms per token,    70.52 tokens per second)\n",
      "llama_print_timings:        eval time = 20284.48 ms /   510 runs   (   39.77 ms per token,    25.14 tokens per second)\n",
      "llama_print_timings:       total time = 24558.09 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d4d2d2-0271-4f74-8af2-3d1e6521707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562284\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy. Our lives are too short to not find joy in every day.\n",
      "I am a very caring, loving and kind person. I love animals especially dogs. My favorite color is purple. I love to read mystery novels. I also like to bake and cook. When I have time off I like to watch tv. I don’t like to be outside when it’s extremely hot, cold or raining.\n",
      "I am looking for someone who is kind, caring and loving. Someone that can make me laugh and smile everyday. They must love dogs because my dog would be considered part of the family. A perfect gentleman who knows how to treat a lady. Someone that enjoys life as much as I do.\n",
      "I like to watch tv especially NCIS, CSI, Law & Order SVU, Law & Order CI and The First 48. I love to read mystery novels. When it’s nice outside I enjoy walking on the beach or going for a hike but not when it’s hot, cold or raining out. My favorite holiday is Christmas because I like giving gifts to my family and friends.\n",
      "My dream vacation would be anywhere tropical such as Hawaii. Anywhere that has sandy white beaches with crystal clear water. It doesn’t matter if it’s a resort, hotel or home, as long as the view is beautiful. If I could go on vacation for two weeks at least once a year I would be very happy.\n",
      "I am looking for someone who can make me laugh and smile everyday. Someone that knows how to treat a lady. A perfect gentleman. They must love dogs because my dog will always be considered part of the family. Someone who loves life as much as I do.\n",
      "I would like you to know that I am kind, caring and very loyal. My friends tell me that I am funny and can make them laugh. I love to help people in need. I believe that everyone deserves a second chance. I have made some mistakes in my life but I have learned from them. I would also like you to know that I will always be your best friend, biggest supporter, greatest lover and number one fan.\n",
      "I want the world to know that even though I am locked up it does not define who I am. There are two sides to every story. I never had a fair trial because my attorney\n",
      "llama_print_timings:        load time = 46778.12 ms\n",
      "llama_print_timings:      sample time =   387.02 ms /   512 runs   (    0.76 ms per token,  1322.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5503.49 ms /   265 tokens (   20.77 ms per token,    48.15 tokens per second)\n",
      "llama_print_timings:        eval time = 30422.46 ms /   510 runs   (   59.65 ms per token,    16.76 tokens per second)\n",
      "llama_print_timings:       total time = 36440.49 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d79dd90-67fd-48dd-bca4-24daf152a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562373\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live, and we are given many opportunities to experience life.\n",
      "We can do this by listening to our heart instead of our head. Our minds like to keep us safe in a cage where nothing bad will happen but also nothing great happens either. We have been conditioned to be this way.\n",
      "I grew up in a small town 10 miles outside of Atlanta, GA with my mother and grandmother. My mom was a single parent for most of my childhood. She was an inspiration and continues to be. I always had a love for music. When we lived in Florida for the summer (my mother is from Miami), we would drive around listening to my mom’s favorite song “Gypsy” by Fleetwood Mac.\n",
      "I grew up knowing that I wanted to sing, just like Stevie Nicks. My mother always told me she knew I was going to be a singer because of how much I loved music and singing.\n",
      "My grandmother and I would listen to the radio in her room and dance together. She taught me so many things about life, family, love, and faith. She also encouraged my dreams. I can remember her telling me that when she was younger, she had a talent for singing too but let it go because of what others thought.\n",
      "I knew then that I didn’t want to do that. My favorite pastime in my childhood was listening to music and singing at the top of my lungs. And I don’t mean in my house or in my room; I mean anywhere—the car, the grocery store, walking around the mall, and so on.\n",
      "I would just let it out because there was no way I could keep those feelings inside. Whenever I felt down about myself, music always picked me up.\n",
      "I remember going to see my grandmother in Atlanta when she had cancer and I was only 9 or 10 years old. She was very sick and didn’t have long left on this earth. I wanted to make her happy because she made me so happy for the first ten years of my life.\n",
      "I asked if we could listen to music, which lifted up our spirits and brought us closer together. My grandmother knew she was dying, but she never showed it. She always put a smile on her face for everyone else around her.\n",
      "She told me that I would be just fine when she passed on, and I believed her\n",
      "llama_print_timings:        load time =  6302.54 ms\n",
      "llama_print_timings:      sample time =   430.43 ms /   512 runs   (    0.84 ms per token,  1189.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5357.66 ms /   265 tokens (   20.22 ms per token,    49.46 tokens per second)\n",
      "llama_print_timings:        eval time = 30487.28 ms /   510 runs   (   59.78 ms per token,    16.73 tokens per second)\n",
      "llama_print_timings:       total time = 36403.60 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138ce498-9718-44ef-a37d-9acaaa5ff0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562420\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give it meaning.\n",
      "We find meaning in our lives by investing in relationships and causes that are important to us, by building upon our strengths to achieve certain goals that we set for ourselves (although these may shift over time), and by being kind, loving and compassionate towards others.\n",
      "A lot of people ask themselves if their life is meaningful enough. They compare themselves with others, or they get caught up in the day-to-day grind and forget to stop and smell the roses. The secret is not to keep looking for happiness but to give it away by doing good deeds, being kind and giving your time and attention to other people.\n",
      "We all have something that we are good at – some form of talent or strength. I believe that we can make our lives more meaningful when we use those strengths in service of others. We can also take time throughout the week to think about what makes us happy, and then do it! This will help us discover who we really are as a person.\n",
      "I don’t have all of the answers but I know that if we want our lives to feel more meaningful, we need to put our happiness first. We also need to look for opportunities every day to make someone else happy – whether it is by calling them or spending time with them, giving them a gift, volunteering at our local charity, writing a letter of gratitude or taking the time to help out in some way.\n",
      "We can all give meaning to our lives by making life easier for others and showing that we care about the people around us.\n",
      "The best thing I have done to add more meaning to my own life is to take up volunteering at a local charity. It makes me feel good knowing that I am helping other people, plus it is something outside of myself – I don’t get caught up in my own problems and worries when I am busy helping someone else with theirs!\n",
      "We all have the power to make our lives more meaningful by giving life meaning through being kind and compassionate towards others.\n",
      "This post was written by a member of our blogging community who has chosen to remain anonymous. We are grateful for her contributions.\n",
      "Tags: happiness,meaning of life,service,volunteering\n",
      "I believe in the importance of family\n",
      "I believe in my dreams\n",
      "I believe in being a good person\n",
      "I believe that kindness is never wasted\n",
      "I believe\n",
      "llama_print_timings:        load time =  6297.52 ms\n",
      "llama_print_timings:      sample time =   429.37 ms /   512 runs   (    0.84 ms per token,  1192.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5606.41 ms /   265 tokens (   21.16 ms per token,    47.27 tokens per second)\n",
      "llama_print_timings:        eval time = 30448.09 ms /   510 runs   (   59.70 ms per token,    16.75 tokens per second)\n",
      "llama_print_timings:       total time = 36611.13 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf297fc-6ba8-4e95-9d94-679c563df6c5",
   "metadata": {},
   "source": [
    "### f16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11219943-66d6-4e82-8f62-7b2897e9e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562468\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose in everything you do. If you look at a piece of art, it has a meaning that can be interpreted by different people and every time you will see something different in that work of art. The same for music or anything else in this world.\n",
      "But what does it mean? How do we get there? And how do we stay motivated when life gets rough along the way...\n",
      "I have been through a lot with my business, but I never gave up because I knew deep inside that every experience I went through, every person I encountered were all lessons meant to make me stronger and wiser in order to fulfill the purpose God has put on my path.\n",
      "But I had to go through it...and I did. The hard way.\n",
      "I didn't know how to do it right or how to find my own path, until I met some amazing people who taught me about their journey and they showed me how I can apply their teachings in order to get where I want to be in life.\n",
      "So here are 3 pieces of advice that will help you to find your purpose:\n",
      "1) Get rid of the fears.\n",
      "Most people have a lot of fears, which cause them not to take action. They fear what other people can say or think about them, they fear rejection and failure...and a lot of other things. I know it's hard to fight all these fears, but if you really want something in life, there is no other way than to overcome your fears and take action towards that goal.\n",
      "And don’t worry, because when we start taking actions, we start believing more in ourselves and we become stronger as a person. And if the people around us are supporting our dreams then nothing can stop you.\n",
      "2) Choose the right people to surround yourself with.\n",
      "You need to identify your \"tribe\" that will help you on your journey. You see, we all want to feel loved and supported by others but sometimes we don’t know who those people are. We usually go for friends or family members because they have always been there and they've never let us down...but what happens when they don’t support our dreams?\n",
      "I think it's important to find a few good people that believe in you, that will push you forward while at the same time being understanding. They should be there for you and motivate you to take action towards your goals,\n",
      "llama_print_timings:        load time = 17203.43 ms\n",
      "llama_print_timings:      sample time =   392.00 ms /   512 runs   (    0.77 ms per token,  1306.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1770.78 ms /   265 tokens (    6.68 ms per token,   149.65 tokens per second)\n",
      "llama_print_timings:        eval time = 10734.63 ms /   510 runs   (   21.05 ms per token,    47.51 tokens per second)\n",
      "llama_print_timings:       total time = 13024.62 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4ab840-58b0-4dc5-9d40-55ae1ae5987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562502\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to serve others. My purpose in life is to help people learn, grow and become all they are meant to be while helping them achieve their goals and dreams. In my work with clients, I use a variety of techniques including hypnosis, neuro-linguistic programming (NLP), mindfulness meditation, goal setting, positive psychology, cognitive behavioural therapy (CBT) and solution focused brief therapy (SFBT). I also provide career coaching services to individuals who are looking to start a new career or change careers.\n",
      "I have worked as an addictions counselor at The Salvation Army in Calgary since 2014, where I am currently the Program Manager for Outpatient Services. In this role, I provide individual and group therapy services. Prior to joining The Salvation Army, I was a member of the team at Turning Point Psychological Services since 2013. While there, I provided counselling for clients with substance use concerns and various other mental health issues.\n",
      "I have also worked in private practice. My experience in private practice has been working with individuals who are struggling with life transitions including: depression; anxiety; relationship problems; personal growth & development; career coaching, self-esteem & assertiveness training; stress management; confidence building and weight management.\n",
      "My areas of special interest include: anxiety disorders (panic attacks, phobias, social anxiety); depression; eating disorders; substance use concerns including alcohol abuse; marijuana addiction; and gambling addictions. I also have a strong passion for working with individuals who are struggling to adjust to life transitions.\n",
      "I am committed to providing clients with an empathetic, non-judgmental approach during our time together so that you feel safe, respected and valued. I will work collaboratively with you to identify the root cause of your concerns and develop a plan to help you achieve your goals. I have experience working with individuals from all walks of life including: executives; professionals; healthcare providers; entrepreneurs; students; athletes; teachers, lawyers; accountants and many more.\n",
      "I am fluent in English, French, and Dutch.\n",
      "Addictions Counsellor - Calgary, Alberta\n",
      "Teacher & Certified Addiction Professional (CAP)\n",
      "llama_print_timings:        load time =  2395.68 ms\n",
      "llama_print_timings:      sample time =   385.16 ms /   512 runs   (    0.75 ms per token,  1329.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1724.92 ms /   265 tokens (    6.51 ms per token,   153.63 tokens per second)\n",
      "llama_print_timings:        eval time = 10745.03 ms /   510 runs   (   21.07 ms per token,    47.46 tokens per second)\n",
      "llama_print_timings:       total time = 12982.78 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846f1f9a-4fc9-48d0-8d04-d4eb7999ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562520\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live and enjoy every day. Everyone has their own way of living a happy life, but most people don't realize how lucky they are. Life isn't as bad as many people think it is.\n",
      "I'm a pretty simple person, so my way of doing this is to spend time with friends or family; eat good food and laugh until I cry. I usually like to try new things that I haven't done before, such as going out for dinner at new restaurants or watching movies in theaters (which I don't do very often).\n",
      "I'm also a huge cinephile so I love watching lots of films and TV shows. Sometimes I just spend time on my own doing nothing, but when I have no choice I read books. It's one thing I always find to be enjoyable because it takes me to another world or universe.\n",
      "I like to travel and see new places too, so that's something I make sure to do every year in order to keep myself happy. Of course, this is easier said than done since it can get quite expensive to fly around the globe. But if possible, I try my best to go somewhere far away from home each summer.\n",
      "What does your day-to-day look like?\n",
      "I usually spend most of my days at work or on campus where I study. However, this doesn't mean that life is boring for me. I know it sounds so cliché, but every day is different. Although many things are the same from one day to another, there are always small surprises in between.\n",
      "I spend most of my days at work or on campus where I study. However, this doesn't mean that life is boring for me. There is no such thing as a \"boring\" life because every day has surprises in store and things to learn from.\n",
      "I try to wake up early around 5:30am so that I can get ready before work starts at 7am. On weekdays, my daily routine mostly consists of going to work, studying for an hour or two during the evenings and going out with friends or family on the weekends.\n",
      "What are your favorite places in New York?\n",
      "I've lived in the Upper West Side for almost a year now so I feel like it's kind of my home away from home. There is so much to do here, so many different restaurants, stores\n",
      "llama_print_timings:        load time =  2403.98 ms\n",
      "llama_print_timings:      sample time =   408.91 ms /   512 runs   (    0.80 ms per token,  1252.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1802.17 ms /   265 tokens (    6.80 ms per token,   147.04 tokens per second)\n",
      "llama_print_timings:        eval time = 10729.76 ms /   510 runs   (   21.04 ms per token,    47.53 tokens per second)\n",
      "llama_print_timings:       total time = 13068.33 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b997d2b2-3217-47cc-9a70-854ee0ab4638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562539\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy.\n",
      "But what does it mean to be happy?\n",
      "The problem with the phrase “being happy” is that, like many other words in our language, we tend to assume we know precisely what it means… and we don’t really look into it as much as we should.\n",
      "The truth is, happiness is a bit of an enigma. We have a vague sense of what the word “happy” means, but we don’t have an exact definition for it — or at least a definition that we can agree on.\n",
      "In fact, this may be why there are so many books and articles out there promising to help you achieve happiness: because no one really knows how. Happiness is such a broad concept that it’s hard to define, which leads to all kinds of different definitions in the literature, from “being in a good mood” to “doing what you want when you want”. It seems like everyone has an idea of happiness, but no one can really agree on what it is.\n",
      "So maybe we should start with that question instead: what does it mean to be happy? Is there even such a thing as “being happy”?\n",
      "In my opinion, the answer is yes — and I think I finally have an idea of what happiness means. Here’s my definition:\n",
      "“Happiness is peacefulness.”\n",
      "I came up with this definition after reading a book called A Guide to the Good Life by William B. Irvine (affiliate link). This book was written in a way that helps you understand different perspectives on happiness — and I think it’s one of the best books on happiness I have ever read.\n",
      "What Does Happiness Mean?\n",
      "In this post, I’m going to talk about how we define happiness… but first, let me give you an example of what I mean by “happy”:\n",
      "Let’s say you went out to dinner with your friend and ordered a plate of nachos. You’re sitting there enjoying the food when you realize that you don’t like cheese; it turns out that your friend ordered the same thing, but she likes cheese more than you do. She offers to take a bite of your nacho plate for you… and suddenly, you find yourself in a peaceful state of mind.\n",
      "Why is this? You feel happy because your friend took the nachos away from you without making you\n",
      "llama_print_timings:        load time = 32977.66 ms\n",
      "llama_print_timings:      sample time =   385.69 ms /   512 runs   (    0.75 ms per token,  1327.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2275.38 ms /   265 tokens (    8.59 ms per token,   116.46 tokens per second)\n",
      "llama_print_timings:        eval time = 15441.93 ms /   510 runs   (   30.28 ms per token,    33.03 tokens per second)\n",
      "llama_print_timings:       total time = 18230.51 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2aedcd-60fc-4d07-8921-91c11695c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562595\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living a good and honorable life.\n",
      "I believe there are certain things all men should strive for: courage, compassion, dignity, honesty, perseverance, character, charity.\n",
      "I believe some things are right and wrong and that the distinction is clear.\n",
      "I take as my heroes those who lived these virtues in a more extreme degree than most. The Mother Theresa's and Gandhi's of the world inspire me because they took to such an extraordinary degree things I think we should all strive for.\n",
      "I believe that I am no better or worse than any other man, but I think I have a responsibility to live up to my potential and to help others realize theirs.\n",
      "I believe that I have a right to take pride in what I do, be it large or small.\n",
      "I believe that there is no greater value in this world than the love between a parent and child.\n",
      "I believe that mankind has achieved little more than the status of an interesting but flawed animal whose behavior can best be described as a combination of selfishness and ignorance.\n",
      "I think I am not alone, but I feel like I'm one in a million if not a billion at least.\n",
      "Do you share my beliefs? Have I missed any important ones you feel strongly about?\n",
      "(idea) by tnt Mon Apr 16 2001 at 14:53:58\n",
      "I believe that life is not the meaning of life, but merely one of many meanings.\n",
      "I believe that each individual has a unique purpose and calling that they must discover for themselves. I have no idea what my own is. This does not bother me; it only bothers those who wish to control my behavior based on their beliefs about mine.\n",
      "I believe that the meaning of life, if there is one, lies beyond this world.\n",
      "(idea) by Glowing Fish Fri Jun 21 2002 at 3:56:47\n",
      "In addition to the specific beliefs I listed in my original writeup, I now would like to add some additional ones that have been suggested or that have occurred to me.\n",
      "I believe that there is a certain degree of responsibility for each person to take an interest in the world and try to improve it. There are many ways to do this; some people choose charity work, or volunteering, or\n",
      "llama_print_timings:        load time =  4020.36 ms\n",
      "llama_print_timings:      sample time =   404.96 ms /   512 runs   (    0.79 ms per token,  1264.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2386.07 ms /   265 tokens (    9.00 ms per token,   111.06 tokens per second)\n",
      "llama_print_timings:        eval time = 15483.29 ms /   510 runs   (   30.36 ms per token,    32.94 tokens per second)\n",
      "llama_print_timings:       total time = 18401.24 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d26e34-4e99-4ed8-8072-8c55fe117344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562621\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live and to experience. We are all just a bunch of energy in a physical form, but our purpose here on earth is to learn and understand what it means to be human.\n",
      "I believe we should always be ourselves at any cost! If you feel good about yourself and your actions then that’s good enough for me.\n",
      "I believe the best way to make new friends is by being open-minded and accepting of everyone no matter how different they may seem from us.\n",
      "I believe that everyone has a story, so when I meet someone new, I like to get to know them better in order to understand their life experiences and viewpoints.\n",
      "I believe that if you want something bad enough, then you have to work for it! But if you truly believe in yourself and your abilities then anything is possible.\n",
      "I believe everyone has a different definition of “happiness” and we all feel happy in different ways; but I personally believe happiness is being surrounded by people who love you unconditionally with no judgment.\n",
      "I believe that we should be thankful for the things we have in our lives, as everything around us can change at any moment.\n",
      "I believe that we should never underestimate ourselves and we should all believe we are capable of anything!\n",
      "I believe that there is always room to improve and make yourself better no matter how good you may already be.\n",
      "I believe it’s never too late to follow your dreams, because if you don’t then the rest of your life will always be full of what-ifs.\n",
      "I believe being independent isn’t about living alone; but rather it means that you can live by yourself and support yourself without help from others.\n",
      "I believe there are so many different ways to express happiness with the people around us!\n",
      "I believe that family is not just the ones who share your blood, but also anyone who you share a special bond with.\n",
      "I believe that life isn’t about how hard you can hit or what you can do to other people; it’s all about how far you are willing to bend over backwards to help others!\n",
      "I believe there are no limits to what we can achieve in our lives, except for the ones we impose on ourselves.\n",
      "I believe that everyone should be treated equally regardless of their differences from us.\n",
      "I believe that it’s never too late to change something about yourself and better your life!\n",
      "I believe that no one is perfect so there isn’t a\n",
      "llama_print_timings:        load time =  4052.25 ms\n",
      "llama_print_timings:      sample time =   388.20 ms /   512 runs   (    0.76 ms per token,  1318.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2211.29 ms /   265 tokens (    8.34 ms per token,   119.84 tokens per second)\n",
      "llama_print_timings:        eval time = 15413.31 ms /   510 runs   (   30.22 ms per token,    33.09 tokens per second)\n",
      "llama_print_timings:       total time = 18139.37 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6236b49b-14c5-4502-8ddb-0af61a8eb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562648\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it as best you can, and help others do the same.\n",
      "Every day we are given the gift of a new start.\n",
      "I am so grateful for the gift of today.\n",
      "Thank you God for another opportunity to get it right.\n",
      "This entry was posted in Uncategorized on June 24, 2013 by gretchendemke.\n",
      "Today I took my first step towards my dream. A dream I’ve had since I was a little girl of having a horse ranch and helping people heal with the horses. Today that dream moved from being an idea to becoming real. My husband and I are building our own ranch on 35 acres in Pine City, Minnesota. We have plans for a house, barn, arena, pastures, stables and trails.\n",
      "We found the property after looking for about two years. It was listed as 20 acres but the owner had just purchased an additional 15 acres so it turned out to be more than we were expecting. The first thing we noticed was the beautiful land with a view of nature in all directions. There were rolling hills, fields and woods. We also saw that there is a creek running through the middle of the property which means we have water rights. This would allow us to build some ponds for our horses or even fish!\n",
      "This entry was posted in Uncategorized on June 21, 2013 by gretchendemke.\n",
      "I’ve been thinking a lot about how I can make a difference in the world. I’m at that point where many of us get to – midlife crisis! I have been reflecting on my life and my purpose. My goal is to try to live as closely to who God wants me to be. I want to live with integrity, compassion and love.\n",
      "So how can I make the most difference in this world? What does that mean exactly? And how am I doing so far? Is there something more? How can I do better?\n",
      "The answers came to me through a vision of my life as I look back on it. It has been full, rich and blessed. I have done many things that brought meaning into the lives of others. This is what I’ve come up with so far.\n",
      "This entry was posted in Uncategorized on June 9, 2013 by gretchendemke.\n",
      "\n",
      "llama_print_timings:        load time = 81536.94 ms\n",
      "llama_print_timings:      sample time =   436.74 ms /   512 runs   (    0.85 ms per token,  1172.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3795.35 ms /   265 tokens (   14.32 ms per token,    69.82 tokens per second)\n",
      "llama_print_timings:        eval time = 29100.17 ms /   510 runs   (   57.06 ms per token,    17.53 tokens per second)\n",
      "llama_print_timings:       total time = 33459.16 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c019a4d-0356-485f-a7c0-605440790a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562769\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live your life in a way that will bring you happiness, and make it worthwhile. When I think about how I want my life to be, I imagine myself as a successful doctor, giving back to the world in one of the most important ways possible.\n",
      "I believe that all human life is sacred and valuable from conception until natural death. I am pro-life because every person has inherent value regardless of their age or capabilities. Everyone deserves to be treated with respect and dignity and protected from harm.\n",
      "I believe in living a healthy, active lifestyle by doing activities that make me happy. I also believe it is important to take care of your body and mind because they are the foundation for everything you do. My favorite hobbies include playing sports like basketball and soccer, and listening to music or reading books to relax.\n",
      "I believe the meaning of life is to live your life in a way that will bring you happiness, and make it worthwhile. When I think about how I want my life to be, I imagine myself as a successful doctor, giving back to the world in one of the most important ways possible. I want to get a degree in medicine for many reasons: firstly because the field is very interesting to me; secondly because I love helping others and making them happy; and thirdly because it’s what my dad wanted me to do. He passed away when I was young, so I feel like doing this for him would make him proud of me.\n",
      "I believe that all human life is sacred and valuable from conception until natural death. I am pro-life because every person has inherent value regardless of their age or capabilities. Everyone deserves to be treated with respect and dignity and protected from harm. This means no one should ever be killed for any reason, even as punishment for a crime they committed; it also means taking care of those who are most vulnerable such as the unborn, sick, and elderly.\n",
      "I believe in living a healthy, active lifestyle by doing activities that make me happy. I also believe it is important to take care of your body and mind because they are the foundation for everything you do. My favorite hobbies include playing sports like basketball and soccer, and listening to music or reading books to relax. When you’re not feeling well physically or mentally, it can be difficult to get through the day. I believe in taking care of myself so that I can\n",
      "llama_print_timings:        load time =  8897.61 ms\n",
      "llama_print_timings:      sample time =   385.57 ms /   512 runs   (    0.75 ms per token,  1327.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3823.56 ms /   265 tokens (   14.43 ms per token,    69.31 tokens per second)\n",
      "llama_print_timings:        eval time = 29098.72 ms /   510 runs   (   57.06 ms per token,    17.53 tokens per second)\n",
      "llama_print_timings:       total time = 33435.66 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3619e5a4-ac77-4bf4-bace-ef7a991134e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562816\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to do with our journey from birth, through the struggles and challenges along the way, to death.\n",
      "I believe it is what we learn along that journey which will stay with us after we die.\n",
      "I believe we are all connected in some way, although this connection may not be obvious to everyone.\n",
      "I believe in the power of love, but I also believe that sometimes loving someone can be a difficult thing to do.\n",
      "I believe in the power of words and how they can affect us, both positively and negatively.\n",
      "I believe in the freedom and responsibility of choice.\n",
      "I believe in tolerance because we cannot control what goes on inside another person’s head or heart.\n",
      "I believe that it is never too late to make changes in our lives for ourselves and for others.\n",
      "I believe, as Nelson Mandela said, “Education is the most powerful weapon which you can use to change the world”.\n",
      "I believe that kindness matters.\n",
      "The greatest gift we have been given is this life, so let’s live it well!\n",
      "This entry was posted in Blog on November 30, 2018 by Jenny McLachlan.\n",
      "A new short story for you to enjoy. This one is set in the aftermath of a terror attack.\n",
      "This entry was posted in Blog on May 25, 2017 by Jenny McLachlan.\n",
      "The day before I had been out with friends celebrating my birthday. We’d eaten a meal at a restaurant and then gone to see a show at the theatre. I’d not got home until late – just after midnight, in fact. My husband was still up when I got back so we chatted for a while before retiring to our bedrooms for the night.\n",
      "I woke early but it was cold and I was tired so I cuddled down into my blankets and tried to go back to sleep. The sun was shining through my window, though, reminding me that I had work to do outside. I got up and put on some warm clothes before heading out into the garden.\n",
      "It was a beautiful day – one of those spring days when the sunshine makes you feel alive and everything feels possible. I had no idea then that within hours my life would be changed forever. I went about my tasks, not really aware of anything except the beauty of nature around me. It wasn’\n",
      "llama_print_timings:        load time =  8862.39 ms\n",
      "llama_print_timings:      sample time =   429.44 ms /   512 runs   (    0.84 ms per token,  1192.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3754.80 ms /   265 tokens (   14.17 ms per token,    70.58 tokens per second)\n",
      "llama_print_timings:        eval time = 29148.55 ms /   510 runs   (   57.15 ms per token,    17.50 tokens per second)\n",
      "llama_print_timings:       total time = 33460.19 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d319efa8-99e1-4534-9287-8626ee27fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689562902\n",
      "ggml_init_cublas: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 4090) as main device\n",
      "llama_model_load_internal: mem required  = 3638.20 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 130018 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5961c1-d1c7-4e90-abd4-ac0cb2c53120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
