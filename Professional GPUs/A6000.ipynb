{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e633b3d5-5a26-4769-a1b8-b9edec62f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 02:34:20 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:81:00.0 Off |                  Off |\n",
      "| 90%   40C    P8    34W / 300W |      0MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "model name\t: AMD EPYC 7443P 24-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263787824 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5db777-e04e-4c70-88af-d5226dc12432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 4916, done.\u001b[K\n",
      "remote: Counting objects: 100% (1713/1713), done.\u001b[K\n",
      "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
      "remote: Total 4916 (delta 1641), reused 1597 (delta 1580), pack-reused 3203\u001b[K\n",
      "Receiving objects: 100% (4916/4916), 4.07 MiB | 11.09 MiB/s, done.\n",
      "Resolving deltas: 100% (3365/3365), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53aa7f97-4a2e-47ab-8c66-ca34489ec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4126f1-7c42-4d66-8a7d-8e5c29c322da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eaea4f0-2d91-41ef-9f32-4103f2858796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp/models\n"
     ]
    }
   ],
   "source": [
    "%cd models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cea842-8055-4f3b-8d72-08f46dcb37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2127  100  2127    0     0   8905      0 --:--:-- --:--:-- --:--:--  8899\n",
      "Downloading tokenizer\n",
      "--2023-07-17 02:34:55--  https://agi.gpt4.org/llama/LLaMA/tokenizer.model\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer.model’\n",
      "\n",
      ".//tokenizer.model  100%[===================>] 488.01K  2.39MB/s    in 0.2s    \n",
      "\n",
      "2023-07-17 02:34:56 (2.39 MB/s) - ‘.//tokenizer.model’ saved [499723/499723]\n",
      "\n",
      "--2023-07-17 02:34:56--  https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [application/octet-stream]\n",
      "Saving to: ‘.//tokenizer_checklist.chk’\n",
      "\n",
      ".//tokenizer_checkl 100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 02:34:56 (50.2 MB/s) - ‘.//tokenizer_checklist.chk’ saved [50/50]\n",
      "\n",
      "tokenizer.model: OK\n",
      "Downloading 7B\n",
      "--2023-07-17 02:34:56--  https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13476939516 (13G) [application/octet-stream]\n",
      "Saving to: ‘.//7B/consolidated.00.pth’\n",
      "\n",
      ".//7B/consolidated. 100%[===================>]  12.55G  33.3MB/s    in 7m 53s  \n",
      "\n",
      "2023-07-17 02:42:50 (27.2 MB/s) - ‘.//7B/consolidated.00.pth’ saved [13476939516/13476939516]\n",
      "\n",
      "--2023-07-17 02:42:50--  https://agi.gpt4.org/llama/LLaMA/7B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//7B/params.json’\n",
      "\n",
      ".//7B/params.json       [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 02:42:50 (82.3 MB/s) - ‘.//7B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 02:42:50--  https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 100 [application/octet-stream]\n",
      "Saving to: ‘.//7B/checklist.chk’\n",
      "\n",
      ".//7B/checklist.chk 100%[===================>]     100  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 02:42:51 (124 MB/s) - ‘.//7B/checklist.chk’ saved [100/100]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "params.json: OK\n",
      "Downloading 13B\n",
      "--2023-07-17 02:43:09--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.00.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  58.9MB/s    in 3m 35s  \n",
      "\n",
      "2023-07-17 02:46:44 (57.8 MB/s) - ‘.//13B/consolidated.00.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 02:46:44--  https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13016334699 (12G) [application/octet-stream]\n",
      "Saving to: ‘.//13B/consolidated.01.pth’\n",
      "\n",
      ".//13B/consolidated 100%[===================>]  12.12G  25.7MB/s    in 7m 26s  \n",
      "\n",
      "2023-07-17 02:54:10 (27.8 MB/s) - ‘.//13B/consolidated.01.pth’ saved [13016334699/13016334699]\n",
      "\n",
      "--2023-07-17 02:54:10--  https://agi.gpt4.org/llama/LLaMA/13B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//13B/params.json’\n",
      "\n",
      ".//13B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 02:54:10 (13.2 MB/s) - ‘.//13B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 02:54:10--  https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154 [application/octet-stream]\n",
      "Saving to: ‘.//13B/checklist.chk’\n",
      "\n",
      ".//13B/checklist.ch 100%[===================>]     154  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 02:54:11 (397 MB/s) - ‘.//13B/checklist.chk’ saved [154/154]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "params.json: OK\n",
      "Downloading 30B\n",
      "--2023-07-17 02:54:45--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.00.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  50.3MB/s    in 4m 59s  \n",
      "\n",
      "2023-07-17 02:59:45 (51.9 MB/s) - ‘.//30B/consolidated.00.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 02:59:45--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.01.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  77.4MB/s    in 3m 56s  \n",
      "\n",
      "2023-07-17 03:03:41 (65.7 MB/s) - ‘.//30B/consolidated.01.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:03:41--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated  96%[==================> ]  14.65G  50.7MB/s    in 5m 26s  \n",
      "\n",
      "2023-07-17 03:09:08 (46.0 MB/s) - Connection closed at byte 15728377856. Retrying.\n",
      "\n",
      "--2023-07-17 03:09:09--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16265763099 (15G), 537385243 (512M) remaining [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.02.pth’\n",
      "\n",
      ".//30B/consolidated 100%[+++++++++++++++++++>]  15.15G  50.0MB/s    in 9.7s    \n",
      "\n",
      "2023-07-17 03:09:19 (52.7 MB/s) - ‘.//30B/consolidated.02.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:09:19--  https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16265763099 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//30B/consolidated.03.pth’\n",
      "\n",
      ".//30B/consolidated 100%[===================>]  15.15G  56.5MB/s    in 4m 33s  \n",
      "\n",
      "2023-07-17 03:13:53 (56.7 MB/s) - ‘.//30B/consolidated.03.pth’ saved [16265763099/16265763099]\n",
      "\n",
      "--2023-07-17 03:13:53--  https://agi.gpt4.org/llama/LLaMA/30B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//30B/params.json’\n",
      "\n",
      ".//30B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:13:53 (42.4 MB/s) - ‘.//30B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 03:13:53--  https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 262 [application/octet-stream]\n",
      "Saving to: ‘.//30B/checklist.chk’\n",
      "\n",
      ".//30B/checklist.ch 100%[===================>]     262  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:13:54 (255 MB/s) - ‘.//30B/checklist.chk’ saved [262/262]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "params.json: OK\n",
      "Downloading 65B\n",
      "--2023-07-17 03:15:20--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.00.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  55.3MB/s    in 4m 59s  \n",
      "\n",
      "2023-07-17 03:20:20 (52.1 MB/s) - ‘.//65B/consolidated.00.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:20:20--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.01.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  54.2MB/s    in 4m 38s  \n",
      "\n",
      "2023-07-17 03:24:58 (56.0 MB/s) - ‘.//65B/consolidated.01.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:24:58--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.02.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  52.2MB/s    in 4m 54s  \n",
      "\n",
      "2023-07-17 03:29:53 (52.9 MB/s) - ‘.//65B/consolidated.02.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:29:53--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.03.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  54.4MB/s    in 4m 50s  \n",
      "\n",
      "2023-07-17 03:34:45 (53.8 MB/s) - ‘.//65B/consolidated.03.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:34:45--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.04.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  57.0MB/s    in 4m 56s  \n",
      "\n",
      "2023-07-17 03:39:41 (52.7 MB/s) - ‘.//65B/consolidated.04.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:39:41--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.05.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  59.9MB/s    in 5m 8s   \n",
      "\n",
      "2023-07-17 03:44:50 (50.6 MB/s) - ‘.//65B/consolidated.05.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:44:50--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated  56%[==========>         ]   8.63G  65.9MB/s    in 2m 24s  \n",
      "\n",
      "2023-07-17 03:47:14 (61.4 MB/s) - Connection closed at byte 9270001664. Retrying.\n",
      "\n",
      "--2023-07-17 03:47:15--  (try: 2)  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 16323959449 (15G), 7053957785 (6.6G) remaining [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.06.pth’\n",
      "\n",
      ".//65B/consolidated 100%[+++++++++++========>]  15.20G  56.3MB/s    in 2m 5s   \n",
      "\n",
      "2023-07-17 03:49:20 (54.0 MB/s) - ‘.//65B/consolidated.06.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:49:20--  https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16323959449 (15G) [application/octet-stream]\n",
      "Saving to: ‘.//65B/consolidated.07.pth’\n",
      "\n",
      ".//65B/consolidated 100%[===================>]  15.20G  50.8MB/s    in 4m 18s  \n",
      "\n",
      "2023-07-17 03:53:38 (60.4 MB/s) - ‘.//65B/consolidated.07.pth’ saved [16323959449/16323959449]\n",
      "\n",
      "--2023-07-17 03:53:38--  https://agi.gpt4.org/llama/LLaMA/65B/params.json\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 104.21.94.104, 172.67.222.91, 2606:4700:3037::6815:5e68, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|104.21.94.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/json]\n",
      "Saving to: ‘.//65B/params.json’\n",
      "\n",
      ".//65B/params.json      [ <=>                ]     101  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:53:39 (38.2 MB/s) - ‘.//65B/params.json’ saved [101]\n",
      "\n",
      "--2023-07-17 03:53:39--  https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk\n",
      "Resolving agi.gpt4.org (agi.gpt4.org)... 172.67.222.91, 104.21.94.104, 2606:4700:3030::ac43:de5b, ...\n",
      "Connecting to agi.gpt4.org (agi.gpt4.org)|172.67.222.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 478 [application/octet-stream]\n",
      "Saving to: ‘.//65B/checklist.chk’\n",
      "\n",
      ".//65B/checklist.ch 100%[===================>]     478  --.-KB/s    in 0s      \n",
      "\n",
      "2023-07-17 03:53:39 (485 MB/s) - ‘.//65B/checklist.chk’ saved [478/478]\n",
      "\n",
      "Checking checksums\n",
      "consolidated.00.pth: OK\n",
      "consolidated.01.pth: OK\n",
      "consolidated.02.pth: OK\n",
      "consolidated.03.pth: OK\n",
      "consolidated.04.pth: OK\n",
      "consolidated.05.pth: OK\n",
      "consolidated.06.pth: OK\n",
      "consolidated.07.pth: OK\n",
      "params.json: OK\n"
     ]
    }
   ],
   "source": [
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f35f110-d227-42c8-a67a-d0a7a99a006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f65fdf0-dc8a-4b05-8cf0-7ab3ea75818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbee9da-c0d2-4feb-9711-c07d6bca4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install Python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ff0437-710f-4bc1-9af5-cf79b9b8d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file models/7B/consolidated.00.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:4096 n_mult:256 n_head:32 n_layer:32\n",
      "Writing vocab...\n",
      "[  1/291] Writing tensor tok_embeddings.weight                  | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  3/291] Writing tensor output.weight                          | size  32000 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
      "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type UnquantizedDataType(name='F16')\n",
      "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type UnquantizedDataType(name='F16')\n",
      "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/7B/ggml-model-f16.bin\n",
      "Loading model file models/13B/consolidated.00.pth\n",
      "Loading model file models/13B/consolidated.01.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:5120 n_mult:256 n_head:40 n_layer:40\n",
      "Writing vocab...\n",
      "[  1/363] Writing tensor tok_embeddings.weight                  | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  2/363] Writing tensor norm.weight                            | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  3/363] Writing tensor output.weight                          | size  32000 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  4/363] Writing tensor layers.0.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  5/363] Writing tensor layers.0.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  6/363] Writing tensor layers.0.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  7/363] Writing tensor layers.0.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[  8/363] Writing tensor layers.0.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[  9/363] Writing tensor layers.0.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 10/363] Writing tensor layers.0.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 11/363] Writing tensor layers.0.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 12/363] Writing tensor layers.0.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 13/363] Writing tensor layers.1.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 14/363] Writing tensor layers.1.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 15/363] Writing tensor layers.1.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 16/363] Writing tensor layers.1.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 17/363] Writing tensor layers.1.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 18/363] Writing tensor layers.1.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 19/363] Writing tensor layers.1.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 20/363] Writing tensor layers.1.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 21/363] Writing tensor layers.1.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 22/363] Writing tensor layers.2.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 23/363] Writing tensor layers.2.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 24/363] Writing tensor layers.2.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 25/363] Writing tensor layers.2.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 26/363] Writing tensor layers.2.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 27/363] Writing tensor layers.2.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 28/363] Writing tensor layers.2.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 29/363] Writing tensor layers.2.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 30/363] Writing tensor layers.2.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 31/363] Writing tensor layers.3.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 32/363] Writing tensor layers.3.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 33/363] Writing tensor layers.3.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 34/363] Writing tensor layers.3.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 35/363] Writing tensor layers.3.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 36/363] Writing tensor layers.3.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 37/363] Writing tensor layers.3.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 38/363] Writing tensor layers.3.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 39/363] Writing tensor layers.3.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 40/363] Writing tensor layers.4.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 41/363] Writing tensor layers.4.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 42/363] Writing tensor layers.4.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 43/363] Writing tensor layers.4.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 44/363] Writing tensor layers.4.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 45/363] Writing tensor layers.4.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 46/363] Writing tensor layers.4.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 47/363] Writing tensor layers.4.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 48/363] Writing tensor layers.4.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 49/363] Writing tensor layers.5.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 50/363] Writing tensor layers.5.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 51/363] Writing tensor layers.5.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 52/363] Writing tensor layers.5.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 53/363] Writing tensor layers.5.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 54/363] Writing tensor layers.5.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 55/363] Writing tensor layers.5.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 56/363] Writing tensor layers.5.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 57/363] Writing tensor layers.5.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 58/363] Writing tensor layers.6.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 59/363] Writing tensor layers.6.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 60/363] Writing tensor layers.6.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 61/363] Writing tensor layers.6.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 62/363] Writing tensor layers.6.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 63/363] Writing tensor layers.6.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 64/363] Writing tensor layers.6.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 65/363] Writing tensor layers.6.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 66/363] Writing tensor layers.6.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 67/363] Writing tensor layers.7.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 68/363] Writing tensor layers.7.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 69/363] Writing tensor layers.7.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 70/363] Writing tensor layers.7.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 71/363] Writing tensor layers.7.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 72/363] Writing tensor layers.7.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 73/363] Writing tensor layers.7.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 74/363] Writing tensor layers.7.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 75/363] Writing tensor layers.7.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 76/363] Writing tensor layers.8.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 77/363] Writing tensor layers.8.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 78/363] Writing tensor layers.8.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 79/363] Writing tensor layers.8.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 80/363] Writing tensor layers.8.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 81/363] Writing tensor layers.8.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 82/363] Writing tensor layers.8.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 83/363] Writing tensor layers.8.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 84/363] Writing tensor layers.8.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 85/363] Writing tensor layers.9.attention.wq.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 86/363] Writing tensor layers.9.attention.wk.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 87/363] Writing tensor layers.9.attention.wv.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 88/363] Writing tensor layers.9.attention.wo.weight           | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 89/363] Writing tensor layers.9.attention_norm.weight         | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 90/363] Writing tensor layers.9.feed_forward.w1.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 91/363] Writing tensor layers.9.feed_forward.w2.weight        | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[ 92/363] Writing tensor layers.9.feed_forward.w3.weight        | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 93/363] Writing tensor layers.9.ffn_norm.weight               | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 94/363] Writing tensor layers.10.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 95/363] Writing tensor layers.10.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 96/363] Writing tensor layers.10.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 97/363] Writing tensor layers.10.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[ 98/363] Writing tensor layers.10.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[ 99/363] Writing tensor layers.10.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[100/363] Writing tensor layers.10.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[101/363] Writing tensor layers.10.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[102/363] Writing tensor layers.10.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[103/363] Writing tensor layers.11.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[104/363] Writing tensor layers.11.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[105/363] Writing tensor layers.11.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[106/363] Writing tensor layers.11.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[107/363] Writing tensor layers.11.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[108/363] Writing tensor layers.11.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[109/363] Writing tensor layers.11.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[110/363] Writing tensor layers.11.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[111/363] Writing tensor layers.11.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[112/363] Writing tensor layers.12.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[113/363] Writing tensor layers.12.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[114/363] Writing tensor layers.12.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[115/363] Writing tensor layers.12.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[116/363] Writing tensor layers.12.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[117/363] Writing tensor layers.12.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[118/363] Writing tensor layers.12.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[119/363] Writing tensor layers.12.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[120/363] Writing tensor layers.12.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[121/363] Writing tensor layers.13.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[122/363] Writing tensor layers.13.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[123/363] Writing tensor layers.13.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[124/363] Writing tensor layers.13.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[125/363] Writing tensor layers.13.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[126/363] Writing tensor layers.13.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[127/363] Writing tensor layers.13.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[128/363] Writing tensor layers.13.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[129/363] Writing tensor layers.13.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[130/363] Writing tensor layers.14.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[131/363] Writing tensor layers.14.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[132/363] Writing tensor layers.14.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[133/363] Writing tensor layers.14.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[134/363] Writing tensor layers.14.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[135/363] Writing tensor layers.14.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[136/363] Writing tensor layers.14.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[137/363] Writing tensor layers.14.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[138/363] Writing tensor layers.14.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[139/363] Writing tensor layers.15.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[140/363] Writing tensor layers.15.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[141/363] Writing tensor layers.15.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[142/363] Writing tensor layers.15.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[143/363] Writing tensor layers.15.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[144/363] Writing tensor layers.15.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[145/363] Writing tensor layers.15.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[146/363] Writing tensor layers.15.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[147/363] Writing tensor layers.15.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[148/363] Writing tensor layers.16.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[149/363] Writing tensor layers.16.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[150/363] Writing tensor layers.16.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[151/363] Writing tensor layers.16.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[152/363] Writing tensor layers.16.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[153/363] Writing tensor layers.16.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[154/363] Writing tensor layers.16.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[155/363] Writing tensor layers.16.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[156/363] Writing tensor layers.16.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[157/363] Writing tensor layers.17.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[158/363] Writing tensor layers.17.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[159/363] Writing tensor layers.17.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[160/363] Writing tensor layers.17.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[161/363] Writing tensor layers.17.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[162/363] Writing tensor layers.17.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[163/363] Writing tensor layers.17.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[164/363] Writing tensor layers.17.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[165/363] Writing tensor layers.17.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[166/363] Writing tensor layers.18.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[167/363] Writing tensor layers.18.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[168/363] Writing tensor layers.18.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[169/363] Writing tensor layers.18.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[170/363] Writing tensor layers.18.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[171/363] Writing tensor layers.18.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[172/363] Writing tensor layers.18.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[173/363] Writing tensor layers.18.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[174/363] Writing tensor layers.18.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[175/363] Writing tensor layers.19.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[176/363] Writing tensor layers.19.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[177/363] Writing tensor layers.19.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[178/363] Writing tensor layers.19.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[179/363] Writing tensor layers.19.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[180/363] Writing tensor layers.19.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[181/363] Writing tensor layers.19.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[182/363] Writing tensor layers.19.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[183/363] Writing tensor layers.19.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[184/363] Writing tensor layers.20.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[185/363] Writing tensor layers.20.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[186/363] Writing tensor layers.20.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[187/363] Writing tensor layers.20.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[188/363] Writing tensor layers.20.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[189/363] Writing tensor layers.20.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[190/363] Writing tensor layers.20.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[191/363] Writing tensor layers.20.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[192/363] Writing tensor layers.20.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[193/363] Writing tensor layers.21.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[194/363] Writing tensor layers.21.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[195/363] Writing tensor layers.21.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[196/363] Writing tensor layers.21.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[197/363] Writing tensor layers.21.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[198/363] Writing tensor layers.21.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[199/363] Writing tensor layers.21.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[200/363] Writing tensor layers.21.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[201/363] Writing tensor layers.21.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[202/363] Writing tensor layers.22.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[203/363] Writing tensor layers.22.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[204/363] Writing tensor layers.22.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[205/363] Writing tensor layers.22.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[206/363] Writing tensor layers.22.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[207/363] Writing tensor layers.22.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[208/363] Writing tensor layers.22.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[209/363] Writing tensor layers.22.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[210/363] Writing tensor layers.22.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[211/363] Writing tensor layers.23.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[212/363] Writing tensor layers.23.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[213/363] Writing tensor layers.23.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[214/363] Writing tensor layers.23.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[215/363] Writing tensor layers.23.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[216/363] Writing tensor layers.23.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[217/363] Writing tensor layers.23.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[218/363] Writing tensor layers.23.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[219/363] Writing tensor layers.23.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[220/363] Writing tensor layers.24.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[221/363] Writing tensor layers.24.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[222/363] Writing tensor layers.24.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[223/363] Writing tensor layers.24.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[224/363] Writing tensor layers.24.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[225/363] Writing tensor layers.24.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[226/363] Writing tensor layers.24.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[227/363] Writing tensor layers.24.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[228/363] Writing tensor layers.24.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[229/363] Writing tensor layers.25.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[230/363] Writing tensor layers.25.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[231/363] Writing tensor layers.25.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[232/363] Writing tensor layers.25.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[233/363] Writing tensor layers.25.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[234/363] Writing tensor layers.25.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[235/363] Writing tensor layers.25.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[236/363] Writing tensor layers.25.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[237/363] Writing tensor layers.25.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[238/363] Writing tensor layers.26.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[239/363] Writing tensor layers.26.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[240/363] Writing tensor layers.26.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[241/363] Writing tensor layers.26.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[242/363] Writing tensor layers.26.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[243/363] Writing tensor layers.26.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[244/363] Writing tensor layers.26.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[245/363] Writing tensor layers.26.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[246/363] Writing tensor layers.26.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[247/363] Writing tensor layers.27.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[248/363] Writing tensor layers.27.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[249/363] Writing tensor layers.27.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[250/363] Writing tensor layers.27.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[251/363] Writing tensor layers.27.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[252/363] Writing tensor layers.27.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[253/363] Writing tensor layers.27.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[254/363] Writing tensor layers.27.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[255/363] Writing tensor layers.27.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[256/363] Writing tensor layers.28.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[257/363] Writing tensor layers.28.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[258/363] Writing tensor layers.28.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[259/363] Writing tensor layers.28.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[260/363] Writing tensor layers.28.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[261/363] Writing tensor layers.28.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[262/363] Writing tensor layers.28.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[263/363] Writing tensor layers.28.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[264/363] Writing tensor layers.28.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[265/363] Writing tensor layers.29.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[266/363] Writing tensor layers.29.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[267/363] Writing tensor layers.29.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[268/363] Writing tensor layers.29.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[269/363] Writing tensor layers.29.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[270/363] Writing tensor layers.29.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[271/363] Writing tensor layers.29.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[272/363] Writing tensor layers.29.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[273/363] Writing tensor layers.29.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[274/363] Writing tensor layers.30.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[275/363] Writing tensor layers.30.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[276/363] Writing tensor layers.30.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[277/363] Writing tensor layers.30.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[278/363] Writing tensor layers.30.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[279/363] Writing tensor layers.30.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[280/363] Writing tensor layers.30.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[281/363] Writing tensor layers.30.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[282/363] Writing tensor layers.30.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[283/363] Writing tensor layers.31.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[284/363] Writing tensor layers.31.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[285/363] Writing tensor layers.31.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[286/363] Writing tensor layers.31.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[287/363] Writing tensor layers.31.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[288/363] Writing tensor layers.31.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[289/363] Writing tensor layers.31.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[290/363] Writing tensor layers.31.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[291/363] Writing tensor layers.31.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[292/363] Writing tensor layers.32.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[293/363] Writing tensor layers.32.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[294/363] Writing tensor layers.32.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[295/363] Writing tensor layers.32.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[296/363] Writing tensor layers.32.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[297/363] Writing tensor layers.32.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[298/363] Writing tensor layers.32.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[299/363] Writing tensor layers.32.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[300/363] Writing tensor layers.32.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[301/363] Writing tensor layers.33.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[302/363] Writing tensor layers.33.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[303/363] Writing tensor layers.33.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[304/363] Writing tensor layers.33.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[305/363] Writing tensor layers.33.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[306/363] Writing tensor layers.33.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[307/363] Writing tensor layers.33.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[308/363] Writing tensor layers.33.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[309/363] Writing tensor layers.33.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[310/363] Writing tensor layers.34.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[311/363] Writing tensor layers.34.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[312/363] Writing tensor layers.34.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[313/363] Writing tensor layers.34.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[314/363] Writing tensor layers.34.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[315/363] Writing tensor layers.34.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[316/363] Writing tensor layers.34.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[317/363] Writing tensor layers.34.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[318/363] Writing tensor layers.34.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[319/363] Writing tensor layers.35.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[320/363] Writing tensor layers.35.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[321/363] Writing tensor layers.35.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[322/363] Writing tensor layers.35.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[323/363] Writing tensor layers.35.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[324/363] Writing tensor layers.35.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[325/363] Writing tensor layers.35.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[326/363] Writing tensor layers.35.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[327/363] Writing tensor layers.35.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[328/363] Writing tensor layers.36.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[329/363] Writing tensor layers.36.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[330/363] Writing tensor layers.36.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[331/363] Writing tensor layers.36.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[332/363] Writing tensor layers.36.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[333/363] Writing tensor layers.36.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[334/363] Writing tensor layers.36.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[335/363] Writing tensor layers.36.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[336/363] Writing tensor layers.36.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[337/363] Writing tensor layers.37.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[338/363] Writing tensor layers.37.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[339/363] Writing tensor layers.37.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[340/363] Writing tensor layers.37.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[341/363] Writing tensor layers.37.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[342/363] Writing tensor layers.37.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[343/363] Writing tensor layers.37.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[344/363] Writing tensor layers.37.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[345/363] Writing tensor layers.37.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[346/363] Writing tensor layers.38.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[347/363] Writing tensor layers.38.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[348/363] Writing tensor layers.38.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[349/363] Writing tensor layers.38.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[350/363] Writing tensor layers.38.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[351/363] Writing tensor layers.38.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[352/363] Writing tensor layers.38.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[353/363] Writing tensor layers.38.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[354/363] Writing tensor layers.38.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[355/363] Writing tensor layers.39.attention.wq.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[356/363] Writing tensor layers.39.attention.wk.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[357/363] Writing tensor layers.39.attention.wv.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[358/363] Writing tensor layers.39.attention.wo.weight          | size   5120 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[359/363] Writing tensor layers.39.attention_norm.weight        | size   5120           | type UnquantizedDataType(name='F32')\n",
      "[360/363] Writing tensor layers.39.feed_forward.w1.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[361/363] Writing tensor layers.39.feed_forward.w2.weight       | size   5120 x  13824  | type UnquantizedDataType(name='F16')\n",
      "[362/363] Writing tensor layers.39.feed_forward.w3.weight       | size  13824 x   5120  | type UnquantizedDataType(name='F16')\n",
      "[363/363] Writing tensor layers.39.ffn_norm.weight              | size   5120           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/13B/ggml-model-f16.bin\n",
      "Loading model file models/30B/consolidated.00.pth\n",
      "Loading model file models/30B/consolidated.01.pth\n",
      "Loading model file models/30B/consolidated.02.pth\n",
      "Loading model file models/30B/consolidated.03.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:6656 n_mult:256 n_head:52 n_layer:60\n",
      "Writing vocab...\n",
      "[  1/543] Writing tensor tok_embeddings.weight                  | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  2/543] Writing tensor norm.weight                            | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  3/543] Writing tensor output.weight                          | size  32000 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  4/543] Writing tensor layers.0.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  5/543] Writing tensor layers.0.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  6/543] Writing tensor layers.0.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  7/543] Writing tensor layers.0.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[  8/543] Writing tensor layers.0.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[  9/543] Writing tensor layers.0.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 10/543] Writing tensor layers.0.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 11/543] Writing tensor layers.0.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 12/543] Writing tensor layers.0.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 13/543] Writing tensor layers.1.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 14/543] Writing tensor layers.1.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 15/543] Writing tensor layers.1.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 16/543] Writing tensor layers.1.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 17/543] Writing tensor layers.1.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 18/543] Writing tensor layers.1.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 19/543] Writing tensor layers.1.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 20/543] Writing tensor layers.1.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 21/543] Writing tensor layers.1.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 22/543] Writing tensor layers.2.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 23/543] Writing tensor layers.2.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 24/543] Writing tensor layers.2.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 25/543] Writing tensor layers.2.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 26/543] Writing tensor layers.2.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 27/543] Writing tensor layers.2.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 28/543] Writing tensor layers.2.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 29/543] Writing tensor layers.2.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 30/543] Writing tensor layers.2.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 31/543] Writing tensor layers.3.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 32/543] Writing tensor layers.3.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 33/543] Writing tensor layers.3.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 34/543] Writing tensor layers.3.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 35/543] Writing tensor layers.3.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 36/543] Writing tensor layers.3.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 37/543] Writing tensor layers.3.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 38/543] Writing tensor layers.3.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 39/543] Writing tensor layers.3.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 40/543] Writing tensor layers.4.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 41/543] Writing tensor layers.4.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 42/543] Writing tensor layers.4.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 43/543] Writing tensor layers.4.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 44/543] Writing tensor layers.4.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 45/543] Writing tensor layers.4.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 46/543] Writing tensor layers.4.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 47/543] Writing tensor layers.4.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 48/543] Writing tensor layers.4.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 49/543] Writing tensor layers.5.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 50/543] Writing tensor layers.5.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 51/543] Writing tensor layers.5.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 52/543] Writing tensor layers.5.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 53/543] Writing tensor layers.5.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 54/543] Writing tensor layers.5.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 55/543] Writing tensor layers.5.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 56/543] Writing tensor layers.5.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 57/543] Writing tensor layers.5.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 58/543] Writing tensor layers.6.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 59/543] Writing tensor layers.6.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 60/543] Writing tensor layers.6.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 61/543] Writing tensor layers.6.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 62/543] Writing tensor layers.6.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 63/543] Writing tensor layers.6.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 64/543] Writing tensor layers.6.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 65/543] Writing tensor layers.6.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 66/543] Writing tensor layers.6.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 67/543] Writing tensor layers.7.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 68/543] Writing tensor layers.7.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 69/543] Writing tensor layers.7.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 70/543] Writing tensor layers.7.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 71/543] Writing tensor layers.7.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 72/543] Writing tensor layers.7.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 73/543] Writing tensor layers.7.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 74/543] Writing tensor layers.7.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 75/543] Writing tensor layers.7.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 76/543] Writing tensor layers.8.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 77/543] Writing tensor layers.8.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 78/543] Writing tensor layers.8.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 79/543] Writing tensor layers.8.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 80/543] Writing tensor layers.8.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 81/543] Writing tensor layers.8.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 82/543] Writing tensor layers.8.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 83/543] Writing tensor layers.8.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 84/543] Writing tensor layers.8.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 85/543] Writing tensor layers.9.attention.wq.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 86/543] Writing tensor layers.9.attention.wk.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 87/543] Writing tensor layers.9.attention.wv.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 88/543] Writing tensor layers.9.attention.wo.weight           | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 89/543] Writing tensor layers.9.attention_norm.weight         | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 90/543] Writing tensor layers.9.feed_forward.w1.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 91/543] Writing tensor layers.9.feed_forward.w2.weight        | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[ 92/543] Writing tensor layers.9.feed_forward.w3.weight        | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 93/543] Writing tensor layers.9.ffn_norm.weight               | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 94/543] Writing tensor layers.10.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 95/543] Writing tensor layers.10.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 96/543] Writing tensor layers.10.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 97/543] Writing tensor layers.10.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[ 98/543] Writing tensor layers.10.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[ 99/543] Writing tensor layers.10.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[100/543] Writing tensor layers.10.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[101/543] Writing tensor layers.10.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[102/543] Writing tensor layers.10.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[103/543] Writing tensor layers.11.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[104/543] Writing tensor layers.11.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[105/543] Writing tensor layers.11.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[106/543] Writing tensor layers.11.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[107/543] Writing tensor layers.11.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[108/543] Writing tensor layers.11.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[109/543] Writing tensor layers.11.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[110/543] Writing tensor layers.11.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[111/543] Writing tensor layers.11.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[112/543] Writing tensor layers.12.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[113/543] Writing tensor layers.12.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[114/543] Writing tensor layers.12.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[115/543] Writing tensor layers.12.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[116/543] Writing tensor layers.12.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[117/543] Writing tensor layers.12.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[118/543] Writing tensor layers.12.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[119/543] Writing tensor layers.12.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[120/543] Writing tensor layers.12.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[121/543] Writing tensor layers.13.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[122/543] Writing tensor layers.13.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[123/543] Writing tensor layers.13.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[124/543] Writing tensor layers.13.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[125/543] Writing tensor layers.13.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[126/543] Writing tensor layers.13.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[127/543] Writing tensor layers.13.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[128/543] Writing tensor layers.13.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[129/543] Writing tensor layers.13.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[130/543] Writing tensor layers.14.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[131/543] Writing tensor layers.14.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[132/543] Writing tensor layers.14.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[133/543] Writing tensor layers.14.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[134/543] Writing tensor layers.14.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[135/543] Writing tensor layers.14.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[136/543] Writing tensor layers.14.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[137/543] Writing tensor layers.14.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[138/543] Writing tensor layers.14.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[139/543] Writing tensor layers.15.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[140/543] Writing tensor layers.15.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[141/543] Writing tensor layers.15.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[142/543] Writing tensor layers.15.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[143/543] Writing tensor layers.15.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[144/543] Writing tensor layers.15.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[145/543] Writing tensor layers.15.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[146/543] Writing tensor layers.15.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[147/543] Writing tensor layers.15.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[148/543] Writing tensor layers.16.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[149/543] Writing tensor layers.16.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[150/543] Writing tensor layers.16.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[151/543] Writing tensor layers.16.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[152/543] Writing tensor layers.16.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[153/543] Writing tensor layers.16.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[154/543] Writing tensor layers.16.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[155/543] Writing tensor layers.16.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[156/543] Writing tensor layers.16.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[157/543] Writing tensor layers.17.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[158/543] Writing tensor layers.17.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[159/543] Writing tensor layers.17.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[160/543] Writing tensor layers.17.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[161/543] Writing tensor layers.17.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[162/543] Writing tensor layers.17.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[163/543] Writing tensor layers.17.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[164/543] Writing tensor layers.17.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[165/543] Writing tensor layers.17.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[166/543] Writing tensor layers.18.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[167/543] Writing tensor layers.18.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[168/543] Writing tensor layers.18.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[169/543] Writing tensor layers.18.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[170/543] Writing tensor layers.18.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[171/543] Writing tensor layers.18.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[172/543] Writing tensor layers.18.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[173/543] Writing tensor layers.18.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[174/543] Writing tensor layers.18.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[175/543] Writing tensor layers.19.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[176/543] Writing tensor layers.19.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[177/543] Writing tensor layers.19.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[178/543] Writing tensor layers.19.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[179/543] Writing tensor layers.19.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[180/543] Writing tensor layers.19.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[181/543] Writing tensor layers.19.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[182/543] Writing tensor layers.19.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[183/543] Writing tensor layers.19.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[184/543] Writing tensor layers.20.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[185/543] Writing tensor layers.20.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[186/543] Writing tensor layers.20.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[187/543] Writing tensor layers.20.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[188/543] Writing tensor layers.20.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[189/543] Writing tensor layers.20.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[190/543] Writing tensor layers.20.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[191/543] Writing tensor layers.20.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[192/543] Writing tensor layers.20.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[193/543] Writing tensor layers.21.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[194/543] Writing tensor layers.21.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[195/543] Writing tensor layers.21.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[196/543] Writing tensor layers.21.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[197/543] Writing tensor layers.21.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[198/543] Writing tensor layers.21.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[199/543] Writing tensor layers.21.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[200/543] Writing tensor layers.21.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[201/543] Writing tensor layers.21.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[202/543] Writing tensor layers.22.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[203/543] Writing tensor layers.22.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[204/543] Writing tensor layers.22.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[205/543] Writing tensor layers.22.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[206/543] Writing tensor layers.22.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[207/543] Writing tensor layers.22.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[208/543] Writing tensor layers.22.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[209/543] Writing tensor layers.22.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[210/543] Writing tensor layers.22.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[211/543] Writing tensor layers.23.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[212/543] Writing tensor layers.23.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[213/543] Writing tensor layers.23.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[214/543] Writing tensor layers.23.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[215/543] Writing tensor layers.23.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[216/543] Writing tensor layers.23.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[217/543] Writing tensor layers.23.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[218/543] Writing tensor layers.23.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[219/543] Writing tensor layers.23.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[220/543] Writing tensor layers.24.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[221/543] Writing tensor layers.24.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[222/543] Writing tensor layers.24.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[223/543] Writing tensor layers.24.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[224/543] Writing tensor layers.24.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[225/543] Writing tensor layers.24.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[226/543] Writing tensor layers.24.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[227/543] Writing tensor layers.24.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[228/543] Writing tensor layers.24.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[229/543] Writing tensor layers.25.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[230/543] Writing tensor layers.25.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[231/543] Writing tensor layers.25.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[232/543] Writing tensor layers.25.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[233/543] Writing tensor layers.25.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[234/543] Writing tensor layers.25.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[235/543] Writing tensor layers.25.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[236/543] Writing tensor layers.25.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[237/543] Writing tensor layers.25.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[238/543] Writing tensor layers.26.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[239/543] Writing tensor layers.26.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[240/543] Writing tensor layers.26.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[241/543] Writing tensor layers.26.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[242/543] Writing tensor layers.26.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[243/543] Writing tensor layers.26.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[244/543] Writing tensor layers.26.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[245/543] Writing tensor layers.26.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[246/543] Writing tensor layers.26.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[247/543] Writing tensor layers.27.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[248/543] Writing tensor layers.27.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[249/543] Writing tensor layers.27.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[250/543] Writing tensor layers.27.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[251/543] Writing tensor layers.27.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[252/543] Writing tensor layers.27.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[253/543] Writing tensor layers.27.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[254/543] Writing tensor layers.27.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[255/543] Writing tensor layers.27.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[256/543] Writing tensor layers.28.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[257/543] Writing tensor layers.28.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[258/543] Writing tensor layers.28.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[259/543] Writing tensor layers.28.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[260/543] Writing tensor layers.28.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[261/543] Writing tensor layers.28.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[262/543] Writing tensor layers.28.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[263/543] Writing tensor layers.28.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[264/543] Writing tensor layers.28.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[265/543] Writing tensor layers.29.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[266/543] Writing tensor layers.29.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[267/543] Writing tensor layers.29.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[268/543] Writing tensor layers.29.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[269/543] Writing tensor layers.29.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[270/543] Writing tensor layers.29.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[271/543] Writing tensor layers.29.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[272/543] Writing tensor layers.29.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[273/543] Writing tensor layers.29.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[274/543] Writing tensor layers.30.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[275/543] Writing tensor layers.30.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[276/543] Writing tensor layers.30.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[277/543] Writing tensor layers.30.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[278/543] Writing tensor layers.30.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[279/543] Writing tensor layers.30.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[280/543] Writing tensor layers.30.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[281/543] Writing tensor layers.30.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[282/543] Writing tensor layers.30.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[283/543] Writing tensor layers.31.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[284/543] Writing tensor layers.31.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[285/543] Writing tensor layers.31.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[286/543] Writing tensor layers.31.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[287/543] Writing tensor layers.31.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[288/543] Writing tensor layers.31.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[289/543] Writing tensor layers.31.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[290/543] Writing tensor layers.31.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[291/543] Writing tensor layers.31.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[292/543] Writing tensor layers.32.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[293/543] Writing tensor layers.32.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[294/543] Writing tensor layers.32.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[295/543] Writing tensor layers.32.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[296/543] Writing tensor layers.32.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[297/543] Writing tensor layers.32.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[298/543] Writing tensor layers.32.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[299/543] Writing tensor layers.32.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[300/543] Writing tensor layers.32.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[301/543] Writing tensor layers.33.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[302/543] Writing tensor layers.33.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[303/543] Writing tensor layers.33.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[304/543] Writing tensor layers.33.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[305/543] Writing tensor layers.33.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[306/543] Writing tensor layers.33.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[307/543] Writing tensor layers.33.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[308/543] Writing tensor layers.33.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[309/543] Writing tensor layers.33.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[310/543] Writing tensor layers.34.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[311/543] Writing tensor layers.34.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[312/543] Writing tensor layers.34.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[313/543] Writing tensor layers.34.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[314/543] Writing tensor layers.34.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[315/543] Writing tensor layers.34.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[316/543] Writing tensor layers.34.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[317/543] Writing tensor layers.34.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[318/543] Writing tensor layers.34.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[319/543] Writing tensor layers.35.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[320/543] Writing tensor layers.35.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[321/543] Writing tensor layers.35.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[322/543] Writing tensor layers.35.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[323/543] Writing tensor layers.35.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[324/543] Writing tensor layers.35.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[325/543] Writing tensor layers.35.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[326/543] Writing tensor layers.35.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[327/543] Writing tensor layers.35.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[328/543] Writing tensor layers.36.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[329/543] Writing tensor layers.36.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[330/543] Writing tensor layers.36.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[331/543] Writing tensor layers.36.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[332/543] Writing tensor layers.36.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[333/543] Writing tensor layers.36.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[334/543] Writing tensor layers.36.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[335/543] Writing tensor layers.36.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[336/543] Writing tensor layers.36.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[337/543] Writing tensor layers.37.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[338/543] Writing tensor layers.37.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[339/543] Writing tensor layers.37.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[340/543] Writing tensor layers.37.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[341/543] Writing tensor layers.37.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[342/543] Writing tensor layers.37.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[343/543] Writing tensor layers.37.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[344/543] Writing tensor layers.37.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[345/543] Writing tensor layers.37.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[346/543] Writing tensor layers.38.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[347/543] Writing tensor layers.38.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[348/543] Writing tensor layers.38.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[349/543] Writing tensor layers.38.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[350/543] Writing tensor layers.38.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[351/543] Writing tensor layers.38.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[352/543] Writing tensor layers.38.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[353/543] Writing tensor layers.38.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[354/543] Writing tensor layers.38.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[355/543] Writing tensor layers.39.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[356/543] Writing tensor layers.39.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[357/543] Writing tensor layers.39.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[358/543] Writing tensor layers.39.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[359/543] Writing tensor layers.39.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[360/543] Writing tensor layers.39.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[361/543] Writing tensor layers.39.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[362/543] Writing tensor layers.39.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[363/543] Writing tensor layers.39.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[364/543] Writing tensor layers.40.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[365/543] Writing tensor layers.40.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[366/543] Writing tensor layers.40.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[367/543] Writing tensor layers.40.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[368/543] Writing tensor layers.40.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[369/543] Writing tensor layers.40.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[370/543] Writing tensor layers.40.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[371/543] Writing tensor layers.40.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[372/543] Writing tensor layers.40.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[373/543] Writing tensor layers.41.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[374/543] Writing tensor layers.41.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[375/543] Writing tensor layers.41.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[376/543] Writing tensor layers.41.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[377/543] Writing tensor layers.41.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[378/543] Writing tensor layers.41.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[379/543] Writing tensor layers.41.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[380/543] Writing tensor layers.41.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[381/543] Writing tensor layers.41.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[382/543] Writing tensor layers.42.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[383/543] Writing tensor layers.42.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[384/543] Writing tensor layers.42.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[385/543] Writing tensor layers.42.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[386/543] Writing tensor layers.42.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[387/543] Writing tensor layers.42.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[388/543] Writing tensor layers.42.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[389/543] Writing tensor layers.42.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[390/543] Writing tensor layers.42.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[391/543] Writing tensor layers.43.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[392/543] Writing tensor layers.43.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[393/543] Writing tensor layers.43.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[394/543] Writing tensor layers.43.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[395/543] Writing tensor layers.43.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[396/543] Writing tensor layers.43.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[397/543] Writing tensor layers.43.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[398/543] Writing tensor layers.43.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[399/543] Writing tensor layers.43.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[400/543] Writing tensor layers.44.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[401/543] Writing tensor layers.44.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[402/543] Writing tensor layers.44.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[403/543] Writing tensor layers.44.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[404/543] Writing tensor layers.44.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[405/543] Writing tensor layers.44.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[406/543] Writing tensor layers.44.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[407/543] Writing tensor layers.44.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[408/543] Writing tensor layers.44.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[409/543] Writing tensor layers.45.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[410/543] Writing tensor layers.45.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[411/543] Writing tensor layers.45.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[412/543] Writing tensor layers.45.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[413/543] Writing tensor layers.45.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[414/543] Writing tensor layers.45.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[415/543] Writing tensor layers.45.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[416/543] Writing tensor layers.45.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[417/543] Writing tensor layers.45.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[418/543] Writing tensor layers.46.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[419/543] Writing tensor layers.46.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[420/543] Writing tensor layers.46.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[421/543] Writing tensor layers.46.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[422/543] Writing tensor layers.46.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[423/543] Writing tensor layers.46.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[424/543] Writing tensor layers.46.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[425/543] Writing tensor layers.46.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[426/543] Writing tensor layers.46.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[427/543] Writing tensor layers.47.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[428/543] Writing tensor layers.47.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[429/543] Writing tensor layers.47.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[430/543] Writing tensor layers.47.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[431/543] Writing tensor layers.47.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[432/543] Writing tensor layers.47.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[433/543] Writing tensor layers.47.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[434/543] Writing tensor layers.47.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[435/543] Writing tensor layers.47.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[436/543] Writing tensor layers.48.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[437/543] Writing tensor layers.48.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[438/543] Writing tensor layers.48.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[439/543] Writing tensor layers.48.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[440/543] Writing tensor layers.48.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[441/543] Writing tensor layers.48.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[442/543] Writing tensor layers.48.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[443/543] Writing tensor layers.48.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[444/543] Writing tensor layers.48.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[445/543] Writing tensor layers.49.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[446/543] Writing tensor layers.49.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[447/543] Writing tensor layers.49.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[448/543] Writing tensor layers.49.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[449/543] Writing tensor layers.49.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[450/543] Writing tensor layers.49.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[451/543] Writing tensor layers.49.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[452/543] Writing tensor layers.49.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[453/543] Writing tensor layers.49.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[454/543] Writing tensor layers.50.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[455/543] Writing tensor layers.50.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[456/543] Writing tensor layers.50.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[457/543] Writing tensor layers.50.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[458/543] Writing tensor layers.50.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[459/543] Writing tensor layers.50.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[460/543] Writing tensor layers.50.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[461/543] Writing tensor layers.50.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[462/543] Writing tensor layers.50.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[463/543] Writing tensor layers.51.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[464/543] Writing tensor layers.51.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[465/543] Writing tensor layers.51.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[466/543] Writing tensor layers.51.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[467/543] Writing tensor layers.51.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[468/543] Writing tensor layers.51.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[469/543] Writing tensor layers.51.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[470/543] Writing tensor layers.51.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[471/543] Writing tensor layers.51.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[472/543] Writing tensor layers.52.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[473/543] Writing tensor layers.52.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[474/543] Writing tensor layers.52.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[475/543] Writing tensor layers.52.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[476/543] Writing tensor layers.52.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[477/543] Writing tensor layers.52.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[478/543] Writing tensor layers.52.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[479/543] Writing tensor layers.52.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[480/543] Writing tensor layers.52.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[481/543] Writing tensor layers.53.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[482/543] Writing tensor layers.53.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[483/543] Writing tensor layers.53.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[484/543] Writing tensor layers.53.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[485/543] Writing tensor layers.53.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[486/543] Writing tensor layers.53.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[487/543] Writing tensor layers.53.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[488/543] Writing tensor layers.53.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[489/543] Writing tensor layers.53.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[490/543] Writing tensor layers.54.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[491/543] Writing tensor layers.54.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[492/543] Writing tensor layers.54.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[493/543] Writing tensor layers.54.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[494/543] Writing tensor layers.54.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[495/543] Writing tensor layers.54.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[496/543] Writing tensor layers.54.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[497/543] Writing tensor layers.54.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[498/543] Writing tensor layers.54.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[499/543] Writing tensor layers.55.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[500/543] Writing tensor layers.55.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[501/543] Writing tensor layers.55.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[502/543] Writing tensor layers.55.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[503/543] Writing tensor layers.55.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[504/543] Writing tensor layers.55.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[505/543] Writing tensor layers.55.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[506/543] Writing tensor layers.55.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[507/543] Writing tensor layers.55.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[508/543] Writing tensor layers.56.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[509/543] Writing tensor layers.56.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[510/543] Writing tensor layers.56.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[511/543] Writing tensor layers.56.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[512/543] Writing tensor layers.56.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[513/543] Writing tensor layers.56.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[514/543] Writing tensor layers.56.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[515/543] Writing tensor layers.56.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[516/543] Writing tensor layers.56.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[517/543] Writing tensor layers.57.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[518/543] Writing tensor layers.57.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[519/543] Writing tensor layers.57.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[520/543] Writing tensor layers.57.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[521/543] Writing tensor layers.57.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[522/543] Writing tensor layers.57.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[523/543] Writing tensor layers.57.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[524/543] Writing tensor layers.57.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[525/543] Writing tensor layers.57.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[526/543] Writing tensor layers.58.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[527/543] Writing tensor layers.58.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[528/543] Writing tensor layers.58.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[529/543] Writing tensor layers.58.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[530/543] Writing tensor layers.58.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[531/543] Writing tensor layers.58.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[532/543] Writing tensor layers.58.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[533/543] Writing tensor layers.58.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[534/543] Writing tensor layers.58.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[535/543] Writing tensor layers.59.attention.wq.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[536/543] Writing tensor layers.59.attention.wk.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[537/543] Writing tensor layers.59.attention.wv.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[538/543] Writing tensor layers.59.attention.wo.weight          | size   6656 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[539/543] Writing tensor layers.59.attention_norm.weight        | size   6656           | type UnquantizedDataType(name='F32')\n",
      "[540/543] Writing tensor layers.59.feed_forward.w1.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[541/543] Writing tensor layers.59.feed_forward.w2.weight       | size   6656 x  17920  | type UnquantizedDataType(name='F16')\n",
      "[542/543] Writing tensor layers.59.feed_forward.w3.weight       | size  17920 x   6656  | type UnquantizedDataType(name='F16')\n",
      "[543/543] Writing tensor layers.59.ffn_norm.weight              | size   6656           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/30B/ggml-model-f16.bin\n",
      "Loading model file models/65B/consolidated.00.pth\n",
      "Loading model file models/65B/consolidated.01.pth\n",
      "Loading model file models/65B/consolidated.02.pth\n",
      "Loading model file models/65B/consolidated.03.pth\n",
      "Loading model file models/65B/consolidated.04.pth\n",
      "Loading model file models/65B/consolidated.05.pth\n",
      "Loading model file models/65B/consolidated.06.pth\n",
      "Loading model file models/65B/consolidated.07.pth\n",
      "Loading vocab file models/tokenizer.model\n",
      "params: n_vocab:32000 n_embd:8192 n_mult:256 n_head:64 n_layer:80\n",
      "Writing vocab...\n",
      "[  1/723] Writing tensor tok_embeddings.weight                  | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  2/723] Writing tensor norm.weight                            | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  3/723] Writing tensor output.weight                          | size  32000 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  4/723] Writing tensor layers.0.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  5/723] Writing tensor layers.0.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  6/723] Writing tensor layers.0.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  7/723] Writing tensor layers.0.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[  8/723] Writing tensor layers.0.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[  9/723] Writing tensor layers.0.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 10/723] Writing tensor layers.0.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 11/723] Writing tensor layers.0.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 12/723] Writing tensor layers.0.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 13/723] Writing tensor layers.1.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 14/723] Writing tensor layers.1.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 15/723] Writing tensor layers.1.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 16/723] Writing tensor layers.1.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 17/723] Writing tensor layers.1.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 18/723] Writing tensor layers.1.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 19/723] Writing tensor layers.1.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 20/723] Writing tensor layers.1.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 21/723] Writing tensor layers.1.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 22/723] Writing tensor layers.2.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 23/723] Writing tensor layers.2.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 24/723] Writing tensor layers.2.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 25/723] Writing tensor layers.2.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 26/723] Writing tensor layers.2.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 27/723] Writing tensor layers.2.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 28/723] Writing tensor layers.2.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 29/723] Writing tensor layers.2.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 30/723] Writing tensor layers.2.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 31/723] Writing tensor layers.3.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 32/723] Writing tensor layers.3.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 33/723] Writing tensor layers.3.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 34/723] Writing tensor layers.3.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 35/723] Writing tensor layers.3.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 36/723] Writing tensor layers.3.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 37/723] Writing tensor layers.3.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 38/723] Writing tensor layers.3.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 39/723] Writing tensor layers.3.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 40/723] Writing tensor layers.4.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 41/723] Writing tensor layers.4.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 42/723] Writing tensor layers.4.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 43/723] Writing tensor layers.4.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 44/723] Writing tensor layers.4.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 45/723] Writing tensor layers.4.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 46/723] Writing tensor layers.4.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 47/723] Writing tensor layers.4.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 48/723] Writing tensor layers.4.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 49/723] Writing tensor layers.5.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 50/723] Writing tensor layers.5.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 51/723] Writing tensor layers.5.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 52/723] Writing tensor layers.5.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 53/723] Writing tensor layers.5.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 54/723] Writing tensor layers.5.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 55/723] Writing tensor layers.5.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 56/723] Writing tensor layers.5.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 57/723] Writing tensor layers.5.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 58/723] Writing tensor layers.6.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 59/723] Writing tensor layers.6.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 60/723] Writing tensor layers.6.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 61/723] Writing tensor layers.6.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 62/723] Writing tensor layers.6.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 63/723] Writing tensor layers.6.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 64/723] Writing tensor layers.6.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 65/723] Writing tensor layers.6.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 66/723] Writing tensor layers.6.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 67/723] Writing tensor layers.7.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 68/723] Writing tensor layers.7.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 69/723] Writing tensor layers.7.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 70/723] Writing tensor layers.7.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 71/723] Writing tensor layers.7.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 72/723] Writing tensor layers.7.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 73/723] Writing tensor layers.7.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 74/723] Writing tensor layers.7.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 75/723] Writing tensor layers.7.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 76/723] Writing tensor layers.8.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 77/723] Writing tensor layers.8.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 78/723] Writing tensor layers.8.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 79/723] Writing tensor layers.8.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 80/723] Writing tensor layers.8.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 81/723] Writing tensor layers.8.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 82/723] Writing tensor layers.8.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 83/723] Writing tensor layers.8.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 84/723] Writing tensor layers.8.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 85/723] Writing tensor layers.9.attention.wq.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 86/723] Writing tensor layers.9.attention.wk.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 87/723] Writing tensor layers.9.attention.wv.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 88/723] Writing tensor layers.9.attention.wo.weight           | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 89/723] Writing tensor layers.9.attention_norm.weight         | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 90/723] Writing tensor layers.9.feed_forward.w1.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 91/723] Writing tensor layers.9.feed_forward.w2.weight        | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[ 92/723] Writing tensor layers.9.feed_forward.w3.weight        | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 93/723] Writing tensor layers.9.ffn_norm.weight               | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 94/723] Writing tensor layers.10.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 95/723] Writing tensor layers.10.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 96/723] Writing tensor layers.10.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 97/723] Writing tensor layers.10.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[ 98/723] Writing tensor layers.10.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[ 99/723] Writing tensor layers.10.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[100/723] Writing tensor layers.10.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[101/723] Writing tensor layers.10.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[102/723] Writing tensor layers.10.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[103/723] Writing tensor layers.11.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[104/723] Writing tensor layers.11.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[105/723] Writing tensor layers.11.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[106/723] Writing tensor layers.11.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[107/723] Writing tensor layers.11.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[108/723] Writing tensor layers.11.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[109/723] Writing tensor layers.11.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[110/723] Writing tensor layers.11.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[111/723] Writing tensor layers.11.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[112/723] Writing tensor layers.12.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[113/723] Writing tensor layers.12.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[114/723] Writing tensor layers.12.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[115/723] Writing tensor layers.12.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[116/723] Writing tensor layers.12.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[117/723] Writing tensor layers.12.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[118/723] Writing tensor layers.12.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[119/723] Writing tensor layers.12.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[120/723] Writing tensor layers.12.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[121/723] Writing tensor layers.13.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[122/723] Writing tensor layers.13.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[123/723] Writing tensor layers.13.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[124/723] Writing tensor layers.13.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[125/723] Writing tensor layers.13.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[126/723] Writing tensor layers.13.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[127/723] Writing tensor layers.13.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[128/723] Writing tensor layers.13.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[129/723] Writing tensor layers.13.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[130/723] Writing tensor layers.14.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[131/723] Writing tensor layers.14.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[132/723] Writing tensor layers.14.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[133/723] Writing tensor layers.14.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[134/723] Writing tensor layers.14.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[135/723] Writing tensor layers.14.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[136/723] Writing tensor layers.14.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[137/723] Writing tensor layers.14.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[138/723] Writing tensor layers.14.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[139/723] Writing tensor layers.15.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[140/723] Writing tensor layers.15.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[141/723] Writing tensor layers.15.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[142/723] Writing tensor layers.15.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[143/723] Writing tensor layers.15.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[144/723] Writing tensor layers.15.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[145/723] Writing tensor layers.15.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[146/723] Writing tensor layers.15.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[147/723] Writing tensor layers.15.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[148/723] Writing tensor layers.16.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[149/723] Writing tensor layers.16.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[150/723] Writing tensor layers.16.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[151/723] Writing tensor layers.16.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[152/723] Writing tensor layers.16.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[153/723] Writing tensor layers.16.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[154/723] Writing tensor layers.16.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[155/723] Writing tensor layers.16.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[156/723] Writing tensor layers.16.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[157/723] Writing tensor layers.17.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[158/723] Writing tensor layers.17.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[159/723] Writing tensor layers.17.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[160/723] Writing tensor layers.17.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[161/723] Writing tensor layers.17.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[162/723] Writing tensor layers.17.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[163/723] Writing tensor layers.17.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[164/723] Writing tensor layers.17.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[165/723] Writing tensor layers.17.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[166/723] Writing tensor layers.18.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[167/723] Writing tensor layers.18.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[168/723] Writing tensor layers.18.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[169/723] Writing tensor layers.18.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[170/723] Writing tensor layers.18.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[171/723] Writing tensor layers.18.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[172/723] Writing tensor layers.18.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[173/723] Writing tensor layers.18.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[174/723] Writing tensor layers.18.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[175/723] Writing tensor layers.19.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[176/723] Writing tensor layers.19.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[177/723] Writing tensor layers.19.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[178/723] Writing tensor layers.19.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[179/723] Writing tensor layers.19.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[180/723] Writing tensor layers.19.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[181/723] Writing tensor layers.19.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[182/723] Writing tensor layers.19.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[183/723] Writing tensor layers.19.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[184/723] Writing tensor layers.20.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[185/723] Writing tensor layers.20.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[186/723] Writing tensor layers.20.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[187/723] Writing tensor layers.20.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[188/723] Writing tensor layers.20.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[189/723] Writing tensor layers.20.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[190/723] Writing tensor layers.20.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[191/723] Writing tensor layers.20.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[192/723] Writing tensor layers.20.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[193/723] Writing tensor layers.21.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[194/723] Writing tensor layers.21.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[195/723] Writing tensor layers.21.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[196/723] Writing tensor layers.21.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[197/723] Writing tensor layers.21.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[198/723] Writing tensor layers.21.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[199/723] Writing tensor layers.21.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[200/723] Writing tensor layers.21.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[201/723] Writing tensor layers.21.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[202/723] Writing tensor layers.22.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[203/723] Writing tensor layers.22.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[204/723] Writing tensor layers.22.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[205/723] Writing tensor layers.22.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[206/723] Writing tensor layers.22.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[207/723] Writing tensor layers.22.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[208/723] Writing tensor layers.22.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[209/723] Writing tensor layers.22.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[210/723] Writing tensor layers.22.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[211/723] Writing tensor layers.23.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[212/723] Writing tensor layers.23.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[213/723] Writing tensor layers.23.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[214/723] Writing tensor layers.23.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[215/723] Writing tensor layers.23.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[216/723] Writing tensor layers.23.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[217/723] Writing tensor layers.23.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[218/723] Writing tensor layers.23.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[219/723] Writing tensor layers.23.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[220/723] Writing tensor layers.24.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[221/723] Writing tensor layers.24.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[222/723] Writing tensor layers.24.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[223/723] Writing tensor layers.24.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[224/723] Writing tensor layers.24.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[225/723] Writing tensor layers.24.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[226/723] Writing tensor layers.24.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[227/723] Writing tensor layers.24.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[228/723] Writing tensor layers.24.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[229/723] Writing tensor layers.25.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[230/723] Writing tensor layers.25.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[231/723] Writing tensor layers.25.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[232/723] Writing tensor layers.25.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[233/723] Writing tensor layers.25.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[234/723] Writing tensor layers.25.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[235/723] Writing tensor layers.25.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[236/723] Writing tensor layers.25.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[237/723] Writing tensor layers.25.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[238/723] Writing tensor layers.26.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[239/723] Writing tensor layers.26.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[240/723] Writing tensor layers.26.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[241/723] Writing tensor layers.26.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[242/723] Writing tensor layers.26.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[243/723] Writing tensor layers.26.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[244/723] Writing tensor layers.26.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[245/723] Writing tensor layers.26.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[246/723] Writing tensor layers.26.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[247/723] Writing tensor layers.27.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[248/723] Writing tensor layers.27.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[249/723] Writing tensor layers.27.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[250/723] Writing tensor layers.27.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[251/723] Writing tensor layers.27.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[252/723] Writing tensor layers.27.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[253/723] Writing tensor layers.27.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[254/723] Writing tensor layers.27.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[255/723] Writing tensor layers.27.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[256/723] Writing tensor layers.28.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[257/723] Writing tensor layers.28.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[258/723] Writing tensor layers.28.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[259/723] Writing tensor layers.28.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[260/723] Writing tensor layers.28.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[261/723] Writing tensor layers.28.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[262/723] Writing tensor layers.28.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[263/723] Writing tensor layers.28.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[264/723] Writing tensor layers.28.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[265/723] Writing tensor layers.29.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[266/723] Writing tensor layers.29.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[267/723] Writing tensor layers.29.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[268/723] Writing tensor layers.29.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[269/723] Writing tensor layers.29.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[270/723] Writing tensor layers.29.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[271/723] Writing tensor layers.29.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[272/723] Writing tensor layers.29.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[273/723] Writing tensor layers.29.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[274/723] Writing tensor layers.30.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[275/723] Writing tensor layers.30.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[276/723] Writing tensor layers.30.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[277/723] Writing tensor layers.30.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[278/723] Writing tensor layers.30.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[279/723] Writing tensor layers.30.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[280/723] Writing tensor layers.30.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[281/723] Writing tensor layers.30.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[282/723] Writing tensor layers.30.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[283/723] Writing tensor layers.31.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[284/723] Writing tensor layers.31.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[285/723] Writing tensor layers.31.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[286/723] Writing tensor layers.31.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[287/723] Writing tensor layers.31.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[288/723] Writing tensor layers.31.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[289/723] Writing tensor layers.31.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[290/723] Writing tensor layers.31.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[291/723] Writing tensor layers.31.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[292/723] Writing tensor layers.32.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[293/723] Writing tensor layers.32.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[294/723] Writing tensor layers.32.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[295/723] Writing tensor layers.32.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[296/723] Writing tensor layers.32.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[297/723] Writing tensor layers.32.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[298/723] Writing tensor layers.32.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[299/723] Writing tensor layers.32.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[300/723] Writing tensor layers.32.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[301/723] Writing tensor layers.33.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[302/723] Writing tensor layers.33.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[303/723] Writing tensor layers.33.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[304/723] Writing tensor layers.33.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[305/723] Writing tensor layers.33.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[306/723] Writing tensor layers.33.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[307/723] Writing tensor layers.33.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[308/723] Writing tensor layers.33.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[309/723] Writing tensor layers.33.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[310/723] Writing tensor layers.34.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[311/723] Writing tensor layers.34.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[312/723] Writing tensor layers.34.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[313/723] Writing tensor layers.34.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[314/723] Writing tensor layers.34.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[315/723] Writing tensor layers.34.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[316/723] Writing tensor layers.34.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[317/723] Writing tensor layers.34.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[318/723] Writing tensor layers.34.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[319/723] Writing tensor layers.35.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[320/723] Writing tensor layers.35.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[321/723] Writing tensor layers.35.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[322/723] Writing tensor layers.35.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[323/723] Writing tensor layers.35.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[324/723] Writing tensor layers.35.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[325/723] Writing tensor layers.35.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[326/723] Writing tensor layers.35.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[327/723] Writing tensor layers.35.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[328/723] Writing tensor layers.36.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[329/723] Writing tensor layers.36.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[330/723] Writing tensor layers.36.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[331/723] Writing tensor layers.36.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[332/723] Writing tensor layers.36.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[333/723] Writing tensor layers.36.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[334/723] Writing tensor layers.36.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[335/723] Writing tensor layers.36.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[336/723] Writing tensor layers.36.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[337/723] Writing tensor layers.37.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[338/723] Writing tensor layers.37.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[339/723] Writing tensor layers.37.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[340/723] Writing tensor layers.37.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[341/723] Writing tensor layers.37.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[342/723] Writing tensor layers.37.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[343/723] Writing tensor layers.37.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[344/723] Writing tensor layers.37.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[345/723] Writing tensor layers.37.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[346/723] Writing tensor layers.38.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[347/723] Writing tensor layers.38.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[348/723] Writing tensor layers.38.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[349/723] Writing tensor layers.38.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[350/723] Writing tensor layers.38.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[351/723] Writing tensor layers.38.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[352/723] Writing tensor layers.38.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[353/723] Writing tensor layers.38.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[354/723] Writing tensor layers.38.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[355/723] Writing tensor layers.39.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[356/723] Writing tensor layers.39.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[357/723] Writing tensor layers.39.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[358/723] Writing tensor layers.39.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[359/723] Writing tensor layers.39.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[360/723] Writing tensor layers.39.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[361/723] Writing tensor layers.39.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[362/723] Writing tensor layers.39.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[363/723] Writing tensor layers.39.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[364/723] Writing tensor layers.40.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[365/723] Writing tensor layers.40.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[366/723] Writing tensor layers.40.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[367/723] Writing tensor layers.40.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[368/723] Writing tensor layers.40.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[369/723] Writing tensor layers.40.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[370/723] Writing tensor layers.40.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[371/723] Writing tensor layers.40.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[372/723] Writing tensor layers.40.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[373/723] Writing tensor layers.41.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[374/723] Writing tensor layers.41.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[375/723] Writing tensor layers.41.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[376/723] Writing tensor layers.41.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[377/723] Writing tensor layers.41.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[378/723] Writing tensor layers.41.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[379/723] Writing tensor layers.41.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[380/723] Writing tensor layers.41.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[381/723] Writing tensor layers.41.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[382/723] Writing tensor layers.42.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[383/723] Writing tensor layers.42.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[384/723] Writing tensor layers.42.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[385/723] Writing tensor layers.42.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[386/723] Writing tensor layers.42.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[387/723] Writing tensor layers.42.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[388/723] Writing tensor layers.42.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[389/723] Writing tensor layers.42.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[390/723] Writing tensor layers.42.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[391/723] Writing tensor layers.43.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[392/723] Writing tensor layers.43.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[393/723] Writing tensor layers.43.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[394/723] Writing tensor layers.43.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[395/723] Writing tensor layers.43.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[396/723] Writing tensor layers.43.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[397/723] Writing tensor layers.43.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[398/723] Writing tensor layers.43.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[399/723] Writing tensor layers.43.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[400/723] Writing tensor layers.44.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[401/723] Writing tensor layers.44.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[402/723] Writing tensor layers.44.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[403/723] Writing tensor layers.44.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[404/723] Writing tensor layers.44.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[405/723] Writing tensor layers.44.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[406/723] Writing tensor layers.44.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[407/723] Writing tensor layers.44.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[408/723] Writing tensor layers.44.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[409/723] Writing tensor layers.45.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[410/723] Writing tensor layers.45.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[411/723] Writing tensor layers.45.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[412/723] Writing tensor layers.45.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[413/723] Writing tensor layers.45.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[414/723] Writing tensor layers.45.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[415/723] Writing tensor layers.45.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[416/723] Writing tensor layers.45.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[417/723] Writing tensor layers.45.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[418/723] Writing tensor layers.46.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[419/723] Writing tensor layers.46.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[420/723] Writing tensor layers.46.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[421/723] Writing tensor layers.46.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[422/723] Writing tensor layers.46.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[423/723] Writing tensor layers.46.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[424/723] Writing tensor layers.46.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[425/723] Writing tensor layers.46.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[426/723] Writing tensor layers.46.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[427/723] Writing tensor layers.47.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[428/723] Writing tensor layers.47.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[429/723] Writing tensor layers.47.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[430/723] Writing tensor layers.47.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[431/723] Writing tensor layers.47.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[432/723] Writing tensor layers.47.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[433/723] Writing tensor layers.47.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[434/723] Writing tensor layers.47.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[435/723] Writing tensor layers.47.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[436/723] Writing tensor layers.48.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[437/723] Writing tensor layers.48.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[438/723] Writing tensor layers.48.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[439/723] Writing tensor layers.48.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[440/723] Writing tensor layers.48.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[441/723] Writing tensor layers.48.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[442/723] Writing tensor layers.48.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[443/723] Writing tensor layers.48.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[444/723] Writing tensor layers.48.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[445/723] Writing tensor layers.49.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[446/723] Writing tensor layers.49.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[447/723] Writing tensor layers.49.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[448/723] Writing tensor layers.49.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[449/723] Writing tensor layers.49.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[450/723] Writing tensor layers.49.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[451/723] Writing tensor layers.49.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[452/723] Writing tensor layers.49.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[453/723] Writing tensor layers.49.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[454/723] Writing tensor layers.50.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[455/723] Writing tensor layers.50.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[456/723] Writing tensor layers.50.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[457/723] Writing tensor layers.50.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[458/723] Writing tensor layers.50.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[459/723] Writing tensor layers.50.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[460/723] Writing tensor layers.50.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[461/723] Writing tensor layers.50.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[462/723] Writing tensor layers.50.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[463/723] Writing tensor layers.51.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[464/723] Writing tensor layers.51.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[465/723] Writing tensor layers.51.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[466/723] Writing tensor layers.51.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[467/723] Writing tensor layers.51.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[468/723] Writing tensor layers.51.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[469/723] Writing tensor layers.51.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[470/723] Writing tensor layers.51.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[471/723] Writing tensor layers.51.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[472/723] Writing tensor layers.52.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[473/723] Writing tensor layers.52.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[474/723] Writing tensor layers.52.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[475/723] Writing tensor layers.52.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[476/723] Writing tensor layers.52.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[477/723] Writing tensor layers.52.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[478/723] Writing tensor layers.52.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[479/723] Writing tensor layers.52.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[480/723] Writing tensor layers.52.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[481/723] Writing tensor layers.53.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[482/723] Writing tensor layers.53.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[483/723] Writing tensor layers.53.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[484/723] Writing tensor layers.53.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[485/723] Writing tensor layers.53.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[486/723] Writing tensor layers.53.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[487/723] Writing tensor layers.53.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[488/723] Writing tensor layers.53.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[489/723] Writing tensor layers.53.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[490/723] Writing tensor layers.54.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[491/723] Writing tensor layers.54.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[492/723] Writing tensor layers.54.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[493/723] Writing tensor layers.54.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[494/723] Writing tensor layers.54.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[495/723] Writing tensor layers.54.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[496/723] Writing tensor layers.54.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[497/723] Writing tensor layers.54.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[498/723] Writing tensor layers.54.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[499/723] Writing tensor layers.55.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[500/723] Writing tensor layers.55.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[501/723] Writing tensor layers.55.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[502/723] Writing tensor layers.55.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[503/723] Writing tensor layers.55.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[504/723] Writing tensor layers.55.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[505/723] Writing tensor layers.55.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[506/723] Writing tensor layers.55.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[507/723] Writing tensor layers.55.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[508/723] Writing tensor layers.56.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[509/723] Writing tensor layers.56.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[510/723] Writing tensor layers.56.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[511/723] Writing tensor layers.56.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[512/723] Writing tensor layers.56.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[513/723] Writing tensor layers.56.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[514/723] Writing tensor layers.56.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[515/723] Writing tensor layers.56.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[516/723] Writing tensor layers.56.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[517/723] Writing tensor layers.57.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[518/723] Writing tensor layers.57.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[519/723] Writing tensor layers.57.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[520/723] Writing tensor layers.57.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[521/723] Writing tensor layers.57.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[522/723] Writing tensor layers.57.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[523/723] Writing tensor layers.57.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[524/723] Writing tensor layers.57.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[525/723] Writing tensor layers.57.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[526/723] Writing tensor layers.58.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[527/723] Writing tensor layers.58.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[528/723] Writing tensor layers.58.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[529/723] Writing tensor layers.58.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[530/723] Writing tensor layers.58.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[531/723] Writing tensor layers.58.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[532/723] Writing tensor layers.58.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[533/723] Writing tensor layers.58.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[534/723] Writing tensor layers.58.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[535/723] Writing tensor layers.59.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[536/723] Writing tensor layers.59.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[537/723] Writing tensor layers.59.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[538/723] Writing tensor layers.59.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[539/723] Writing tensor layers.59.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[540/723] Writing tensor layers.59.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[541/723] Writing tensor layers.59.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[542/723] Writing tensor layers.59.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[543/723] Writing tensor layers.59.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[544/723] Writing tensor layers.60.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[545/723] Writing tensor layers.60.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[546/723] Writing tensor layers.60.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[547/723] Writing tensor layers.60.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[548/723] Writing tensor layers.60.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[549/723] Writing tensor layers.60.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[550/723] Writing tensor layers.60.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[551/723] Writing tensor layers.60.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[552/723] Writing tensor layers.60.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[553/723] Writing tensor layers.61.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[554/723] Writing tensor layers.61.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[555/723] Writing tensor layers.61.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[556/723] Writing tensor layers.61.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[557/723] Writing tensor layers.61.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[558/723] Writing tensor layers.61.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[559/723] Writing tensor layers.61.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[560/723] Writing tensor layers.61.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[561/723] Writing tensor layers.61.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[562/723] Writing tensor layers.62.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[563/723] Writing tensor layers.62.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[564/723] Writing tensor layers.62.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[565/723] Writing tensor layers.62.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[566/723] Writing tensor layers.62.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[567/723] Writing tensor layers.62.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[568/723] Writing tensor layers.62.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[569/723] Writing tensor layers.62.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[570/723] Writing tensor layers.62.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[571/723] Writing tensor layers.63.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[572/723] Writing tensor layers.63.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[573/723] Writing tensor layers.63.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[574/723] Writing tensor layers.63.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[575/723] Writing tensor layers.63.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[576/723] Writing tensor layers.63.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[577/723] Writing tensor layers.63.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[578/723] Writing tensor layers.63.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[579/723] Writing tensor layers.63.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[580/723] Writing tensor layers.64.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[581/723] Writing tensor layers.64.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[582/723] Writing tensor layers.64.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[583/723] Writing tensor layers.64.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[584/723] Writing tensor layers.64.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[585/723] Writing tensor layers.64.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[586/723] Writing tensor layers.64.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[587/723] Writing tensor layers.64.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[588/723] Writing tensor layers.64.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[589/723] Writing tensor layers.65.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[590/723] Writing tensor layers.65.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[591/723] Writing tensor layers.65.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[592/723] Writing tensor layers.65.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[593/723] Writing tensor layers.65.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[594/723] Writing tensor layers.65.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[595/723] Writing tensor layers.65.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[596/723] Writing tensor layers.65.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[597/723] Writing tensor layers.65.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[598/723] Writing tensor layers.66.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[599/723] Writing tensor layers.66.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[600/723] Writing tensor layers.66.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[601/723] Writing tensor layers.66.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[602/723] Writing tensor layers.66.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[603/723] Writing tensor layers.66.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[604/723] Writing tensor layers.66.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[605/723] Writing tensor layers.66.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[606/723] Writing tensor layers.66.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[607/723] Writing tensor layers.67.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[608/723] Writing tensor layers.67.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[609/723] Writing tensor layers.67.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[610/723] Writing tensor layers.67.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[611/723] Writing tensor layers.67.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[612/723] Writing tensor layers.67.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[613/723] Writing tensor layers.67.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[614/723] Writing tensor layers.67.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[615/723] Writing tensor layers.67.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[616/723] Writing tensor layers.68.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[617/723] Writing tensor layers.68.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[618/723] Writing tensor layers.68.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[619/723] Writing tensor layers.68.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[620/723] Writing tensor layers.68.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[621/723] Writing tensor layers.68.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[622/723] Writing tensor layers.68.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[623/723] Writing tensor layers.68.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[624/723] Writing tensor layers.68.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[625/723] Writing tensor layers.69.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[626/723] Writing tensor layers.69.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[627/723] Writing tensor layers.69.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[628/723] Writing tensor layers.69.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[629/723] Writing tensor layers.69.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[630/723] Writing tensor layers.69.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[631/723] Writing tensor layers.69.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[632/723] Writing tensor layers.69.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[633/723] Writing tensor layers.69.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[634/723] Writing tensor layers.70.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[635/723] Writing tensor layers.70.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[636/723] Writing tensor layers.70.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[637/723] Writing tensor layers.70.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[638/723] Writing tensor layers.70.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[639/723] Writing tensor layers.70.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[640/723] Writing tensor layers.70.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[641/723] Writing tensor layers.70.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[642/723] Writing tensor layers.70.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[643/723] Writing tensor layers.71.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[644/723] Writing tensor layers.71.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[645/723] Writing tensor layers.71.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[646/723] Writing tensor layers.71.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[647/723] Writing tensor layers.71.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[648/723] Writing tensor layers.71.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[649/723] Writing tensor layers.71.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[650/723] Writing tensor layers.71.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[651/723] Writing tensor layers.71.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[652/723] Writing tensor layers.72.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[653/723] Writing tensor layers.72.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[654/723] Writing tensor layers.72.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[655/723] Writing tensor layers.72.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[656/723] Writing tensor layers.72.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[657/723] Writing tensor layers.72.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[658/723] Writing tensor layers.72.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[659/723] Writing tensor layers.72.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[660/723] Writing tensor layers.72.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[661/723] Writing tensor layers.73.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[662/723] Writing tensor layers.73.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[663/723] Writing tensor layers.73.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[664/723] Writing tensor layers.73.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[665/723] Writing tensor layers.73.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[666/723] Writing tensor layers.73.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[667/723] Writing tensor layers.73.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[668/723] Writing tensor layers.73.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[669/723] Writing tensor layers.73.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[670/723] Writing tensor layers.74.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[671/723] Writing tensor layers.74.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[672/723] Writing tensor layers.74.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[673/723] Writing tensor layers.74.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[674/723] Writing tensor layers.74.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[675/723] Writing tensor layers.74.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[676/723] Writing tensor layers.74.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[677/723] Writing tensor layers.74.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[678/723] Writing tensor layers.74.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[679/723] Writing tensor layers.75.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[680/723] Writing tensor layers.75.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[681/723] Writing tensor layers.75.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[682/723] Writing tensor layers.75.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[683/723] Writing tensor layers.75.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[684/723] Writing tensor layers.75.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[685/723] Writing tensor layers.75.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[686/723] Writing tensor layers.75.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[687/723] Writing tensor layers.75.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[688/723] Writing tensor layers.76.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[689/723] Writing tensor layers.76.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[690/723] Writing tensor layers.76.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[691/723] Writing tensor layers.76.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[692/723] Writing tensor layers.76.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[693/723] Writing tensor layers.76.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[694/723] Writing tensor layers.76.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[695/723] Writing tensor layers.76.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[696/723] Writing tensor layers.76.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[697/723] Writing tensor layers.77.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[698/723] Writing tensor layers.77.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[699/723] Writing tensor layers.77.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[700/723] Writing tensor layers.77.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[701/723] Writing tensor layers.77.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[702/723] Writing tensor layers.77.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[703/723] Writing tensor layers.77.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[704/723] Writing tensor layers.77.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[705/723] Writing tensor layers.77.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[706/723] Writing tensor layers.78.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[707/723] Writing tensor layers.78.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[708/723] Writing tensor layers.78.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[709/723] Writing tensor layers.78.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[710/723] Writing tensor layers.78.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[711/723] Writing tensor layers.78.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[712/723] Writing tensor layers.78.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[713/723] Writing tensor layers.78.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[714/723] Writing tensor layers.78.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[715/723] Writing tensor layers.79.attention.wq.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[716/723] Writing tensor layers.79.attention.wk.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[717/723] Writing tensor layers.79.attention.wv.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[718/723] Writing tensor layers.79.attention.wo.weight          | size   8192 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[719/723] Writing tensor layers.79.attention_norm.weight        | size   8192           | type UnquantizedDataType(name='F32')\n",
      "[720/723] Writing tensor layers.79.feed_forward.w1.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[721/723] Writing tensor layers.79.feed_forward.w2.weight       | size   8192 x  22016  | type UnquantizedDataType(name='F16')\n",
      "[722/723] Writing tensor layers.79.feed_forward.w3.weight       | size  22016 x   8192  | type UnquantizedDataType(name='F16')\n",
      "[723/723] Writing tensor layers.79.ffn_norm.weight              | size   8192           | type UnquantizedDataType(name='F32')\n",
      "Wrote models/65B/ggml-model-f16.bin\n"
     ]
    }
   ],
   "source": [
    "# convert the models to ggml FP16 format\n",
    "!python3 convert.py models/7B/\n",
    "!python3 convert.py models/13B/\n",
    "!python3 convert.py models/30B/\n",
    "!python3 convert.py models/65B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5820952c-5e39-47e6-a2ec-8f06307b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/7B/ggml-model-f16.bin' to './models/7B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/7B/ggml-model-q4_0.bin\n",
      "[   1/ 291]                tok_embeddings.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->    70.31 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 291]                          norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                        output.weight -     4096 x 32000, type =    f16, quantizing .. size =   250.00 MB ->   102.54 MB | hist: \n",
      "[   4/ 291]         layers.0.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.019 0.030 0.047 0.069 0.097 0.129 0.152 0.129 0.098 0.070 0.047 0.031 0.019 0.016 \n",
      "[   5/ 291]         layers.0.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.035 0.012 0.020 0.032 0.049 0.072 0.098 0.125 0.139 0.125 0.099 0.072 0.050 0.033 0.021 0.017 \n",
      "[   6/ 291]         layers.0.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 291]         layers.0.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.073 0.099 0.123 0.133 0.123 0.099 0.073 0.051 0.033 0.021 0.018 \n",
      "[   8/ 291]       layers.0.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[   9/ 291]      layers.0.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  10/ 291]      layers.0.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 291]      layers.0.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  12/ 291]             layers.0.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  13/ 291]         layers.1.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 291]         layers.1.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 291]         layers.1.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  16/ 291]         layers.1.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.013 0.021 0.033 0.050 0.072 0.098 0.124 0.136 0.124 0.098 0.072 0.050 0.033 0.021 0.018 \n",
      "[  17/ 291]       layers.1.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  18/ 291]      layers.1.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 291]      layers.1.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 291]      layers.1.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 291]             layers.1.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  22/ 291]         layers.2.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  23/ 291]         layers.2.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 291]         layers.2.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 291]         layers.2.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  26/ 291]       layers.2.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  27/ 291]      layers.2.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 291]      layers.2.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 291]      layers.2.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 291]             layers.2.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  31/ 291]         layers.3.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[  32/ 291]         layers.3.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 291]         layers.3.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 291]         layers.3.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 291]       layers.3.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  36/ 291]      layers.3.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 291]      layers.3.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 291]      layers.3.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 291]             layers.3.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  40/ 291]         layers.4.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 291]         layers.4.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  42/ 291]         layers.4.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 291]         layers.4.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  44/ 291]       layers.4.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  45/ 291]      layers.4.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 291]      layers.4.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 291]      layers.4.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 291]             layers.4.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  49/ 291]         layers.5.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 291]         layers.5.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 291]         layers.5.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 291]         layers.5.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 291]       layers.5.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  54/ 291]      layers.5.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 291]      layers.5.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 291]      layers.5.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 291]             layers.5.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  58/ 291]         layers.6.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 291]         layers.6.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 291]         layers.6.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 291]         layers.6.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 291]       layers.6.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  63/ 291]      layers.6.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 291]      layers.6.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 291]      layers.6.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 291]             layers.6.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  67/ 291]         layers.7.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 291]         layers.7.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 291]         layers.7.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 291]         layers.7.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 291]       layers.7.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  72/ 291]      layers.7.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 291]      layers.7.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 291]      layers.7.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 291]             layers.7.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  76/ 291]         layers.8.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 291]         layers.8.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 291]         layers.8.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 291]         layers.8.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 291]       layers.8.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  81/ 291]      layers.8.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 291]      layers.8.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  83/ 291]      layers.8.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 291]             layers.8.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  85/ 291]         layers.9.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 291]         layers.9.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  87/ 291]         layers.9.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 291]         layers.9.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 291]       layers.9.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  90/ 291]      layers.9.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 291]      layers.9.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 291]      layers.9.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 291]             layers.9.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  94/ 291]        layers.10.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 291]        layers.10.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 291]        layers.10.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  97/ 291]        layers.10.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 291]      layers.10.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[  99/ 291]     layers.10.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 291]     layers.10.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 291]     layers.10.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 291]            layers.10.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 103/ 291]        layers.11.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 291]        layers.11.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 291]        layers.11.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 291]        layers.11.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 291]      layers.11.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 108/ 291]     layers.11.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 291]     layers.11.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 110/ 291]     layers.11.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 291]            layers.11.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 112/ 291]        layers.12.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 291]        layers.12.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 114/ 291]        layers.12.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 291]        layers.12.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 291]      layers.12.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 117/ 291]     layers.12.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 291]     layers.12.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 291]     layers.12.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 291]            layers.12.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 121/ 291]        layers.13.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 291]        layers.13.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 291]        layers.13.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 291]        layers.13.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 291]      layers.13.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 126/ 291]     layers.13.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 291]     layers.13.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 291]     layers.13.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 291]            layers.13.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 130/ 291]        layers.14.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 291]        layers.14.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 132/ 291]        layers.14.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 291]        layers.14.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 291]      layers.14.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]     layers.14.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 291]     layers.14.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 291]     layers.14.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 291]            layers.14.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]        layers.15.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 291]        layers.15.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 291]        layers.15.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 291]        layers.15.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 291]      layers.15.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 144/ 291]     layers.15.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 291]     layers.15.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 291]     layers.15.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 291]            layers.15.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 148/ 291]        layers.16.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 291]        layers.16.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 291]        layers.16.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 291]        layers.16.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 291]      layers.16.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 153/ 291]     layers.16.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 291]     layers.16.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 291]     layers.16.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 291]            layers.16.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 157/ 291]        layers.17.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 291]        layers.17.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 159/ 291]        layers.17.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 291]        layers.17.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 291]      layers.17.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 162/ 291]     layers.17.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 291]     layers.17.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 291]     layers.17.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 291]            layers.17.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 166/ 291]        layers.18.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 291]        layers.18.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 168/ 291]        layers.18.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 291]        layers.18.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 291]      layers.18.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 171/ 291]     layers.18.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 291]     layers.18.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 291]     layers.18.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 291]            layers.18.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 175/ 291]        layers.19.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 291]        layers.19.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 177/ 291]        layers.19.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 291]        layers.19.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 291]      layers.19.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 180/ 291]     layers.19.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 291]     layers.19.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 291]     layers.19.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 291]            layers.19.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 184/ 291]        layers.20.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 291]        layers.20.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 186/ 291]        layers.20.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 291]        layers.20.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 291]      layers.20.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]     layers.20.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 291]     layers.20.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 291]     layers.20.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 291]            layers.20.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]        layers.21.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 291]        layers.21.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 291]        layers.21.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 291]        layers.21.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 291]      layers.21.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 198/ 291]     layers.21.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 291]     layers.21.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 291]     layers.21.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 291]            layers.21.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 202/ 291]        layers.22.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 291]        layers.22.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 204/ 291]        layers.22.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 291]        layers.22.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 291]      layers.22.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 207/ 291]     layers.22.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 291]     layers.22.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 291]     layers.22.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 291]            layers.22.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 211/ 291]        layers.23.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 291]        layers.23.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 291]        layers.23.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 291]        layers.23.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 291]      layers.23.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 216/ 291]     layers.23.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 291]     layers.23.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 291]     layers.23.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 291]            layers.23.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 220/ 291]        layers.24.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 291]        layers.24.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 291]        layers.24.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 291]        layers.24.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 291]      layers.24.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 225/ 291]     layers.24.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 291]     layers.24.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 291]     layers.24.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 291]            layers.24.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 229/ 291]        layers.25.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 291]        layers.25.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 231/ 291]        layers.25.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 291]        layers.25.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 291]      layers.25.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 234/ 291]     layers.25.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 291]     layers.25.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 291]     layers.25.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 291]            layers.25.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]        layers.26.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 291]        layers.26.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 291]        layers.26.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 291]        layers.26.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 291]      layers.26.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]     layers.26.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 291]     layers.26.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 291]     layers.26.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 291]            layers.26.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]        layers.27.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 291]        layers.27.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 249/ 291]        layers.27.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 291]        layers.27.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 291]      layers.27.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 252/ 291]     layers.27.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 291]     layers.27.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 291]     layers.27.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 291]            layers.27.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 256/ 291]        layers.28.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 291]        layers.28.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 291]        layers.28.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 291]        layers.28.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 291]      layers.28.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 261/ 291]     layers.28.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 291]     layers.28.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 263/ 291]     layers.28.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 291]            layers.28.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 265/ 291]        layers.29.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 291]        layers.29.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 267/ 291]        layers.29.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 268/ 291]        layers.29.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 291]      layers.29.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 270/ 291]     layers.29.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 291]     layers.29.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 272/ 291]     layers.29.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 291]            layers.29.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]        layers.30.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 291]        layers.30.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 276/ 291]        layers.30.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 291]        layers.30.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 291]      layers.30.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 279/ 291]     layers.30.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 291]     layers.30.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 281/ 291]     layers.30.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 291]            layers.30.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]        layers.31.attention.wq.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[ 284/ 291]        layers.31.attention.wk.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 291]        layers.31.attention.wv.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 286/ 291]        layers.31.attention.wo.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MB ->     9.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 291]      layers.31.attention_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]     layers.31.feed_forward.w1.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 289/ 291]     layers.31.feed_forward.w2.weight -    11008 x  4096, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.098 0.116 0.123 0.116 0.098 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 290/ 291]     layers.31.feed_forward.w3.weight -     4096 x 11008, type =    f16, quantizing .. size =    86.00 MB ->    24.19 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 291/ 291]            layers.31.ffn_norm.weight -             4096, type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 12853.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3647.87 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 17969.84 ms\n",
      "main:    total time = 17969.84 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/13B/ggml-model-f16.bin' to './models/13B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/13B/ggml-model-q4_0.bin\n",
      "[   1/ 363]                tok_embeddings.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->    87.89 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[   2/ 363]                          norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   3/ 363]                        output.weight -     5120 x 32000, type =    f16, quantizing .. size =   312.50 MB ->   128.17 MB | hist: \n",
      "[   4/ 363]         layers.0.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.018 0.029 0.045 0.068 0.097 0.132 0.158 0.132 0.097 0.068 0.045 0.029 0.018 0.015 \n",
      "[   5/ 363]         layers.0.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.098 0.132 0.152 0.132 0.098 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 363]         layers.0.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.075 0.096 0.114 0.124 0.114 0.096 0.075 0.055 0.038 0.024 0.020 \n",
      "[   7/ 363]         layers.0.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.132 0.122 0.099 0.073 0.051 0.034 0.021 0.018 \n",
      "[   8/ 363]       layers.0.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[   9/ 363]      layers.0.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 363]      layers.0.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 363]      layers.0.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 363]             layers.0.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  13/ 363]         layers.1.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 363]         layers.1.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 363]         layers.1.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  16/ 363]         layers.1.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.099 0.122 0.133 0.122 0.099 0.073 0.051 0.034 0.022 0.018 \n",
      "[  17/ 363]       layers.1.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  18/ 363]      layers.1.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 363]      layers.1.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 363]      layers.1.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 363]             layers.1.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  22/ 363]         layers.2.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 363]         layers.2.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 363]         layers.2.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  25/ 363]         layers.2.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  26/ 363]       layers.2.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  27/ 363]      layers.2.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 363]      layers.2.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 363]      layers.2.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 363]             layers.2.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  31/ 363]         layers.3.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 363]         layers.3.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[  33/ 363]         layers.3.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  34/ 363]         layers.3.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 363]       layers.3.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  36/ 363]      layers.3.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 363]      layers.3.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 363]      layers.3.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 363]             layers.3.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  40/ 363]         layers.4.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  41/ 363]         layers.4.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 363]         layers.4.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  43/ 363]         layers.4.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 363]       layers.4.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  45/ 363]      layers.4.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 363]      layers.4.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 363]      layers.4.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 363]             layers.4.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  49/ 363]         layers.5.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  50/ 363]         layers.5.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 363]         layers.5.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  52/ 363]         layers.5.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 363]       layers.5.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  54/ 363]      layers.5.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 363]      layers.5.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 363]      layers.5.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 363]             layers.5.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  58/ 363]         layers.6.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  59/ 363]         layers.6.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 363]         layers.6.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 363]         layers.6.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 363]       layers.6.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  63/ 363]      layers.6.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 363]      layers.6.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  65/ 363]      layers.6.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 363]             layers.6.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  67/ 363]         layers.7.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  68/ 363]         layers.7.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  69/ 363]         layers.7.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 363]         layers.7.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 363]       layers.7.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  72/ 363]      layers.7.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 363]      layers.7.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  74/ 363]      layers.7.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 363]             layers.7.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  76/ 363]         layers.8.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 363]         layers.8.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 363]         layers.8.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 363]         layers.8.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 363]       layers.8.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  81/ 363]      layers.8.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 363]      layers.8.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 363]      layers.8.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 363]             layers.8.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  85/ 363]         layers.9.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 363]         layers.9.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 363]         layers.9.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 363]         layers.9.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 363]       layers.9.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  90/ 363]      layers.9.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 363]      layers.9.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 363]      layers.9.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 363]             layers.9.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  94/ 363]        layers.10.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  95/ 363]        layers.10.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 363]        layers.10.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 363]        layers.10.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 363]      layers.10.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[  99/ 363]     layers.10.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 363]     layers.10.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 363]     layers.10.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 363]            layers.10.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 103/ 363]        layers.11.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 363]        layers.11.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 363]        layers.11.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 363]        layers.11.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 363]      layers.11.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 108/ 363]     layers.11.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 363]     layers.11.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 110/ 363]     layers.11.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 363]            layers.11.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 112/ 363]        layers.12.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 363]        layers.12.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 363]        layers.12.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 115/ 363]        layers.12.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 363]      layers.12.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 117/ 363]     layers.12.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 363]     layers.12.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 119/ 363]     layers.12.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 363]            layers.12.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 121/ 363]        layers.13.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 122/ 363]        layers.13.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 363]        layers.13.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 363]        layers.13.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 363]      layers.13.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 126/ 363]     layers.13.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 363]     layers.13.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 128/ 363]     layers.13.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 363]            layers.13.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 130/ 363]        layers.14.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 363]        layers.14.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 363]        layers.14.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 363]        layers.14.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 363]      layers.14.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 135/ 363]     layers.14.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 363]     layers.14.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 363]     layers.14.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 363]            layers.14.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 139/ 363]        layers.15.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 363]        layers.15.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 363]        layers.15.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 363]        layers.15.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 363]      layers.15.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 144/ 363]     layers.15.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 363]     layers.15.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 363]     layers.15.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 363]            layers.15.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 148/ 363]        layers.16.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 149/ 363]        layers.16.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 363]        layers.16.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 151/ 363]        layers.16.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 363]      layers.16.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 153/ 363]     layers.16.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 363]     layers.16.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 155/ 363]     layers.16.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 363]            layers.16.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 157/ 363]        layers.17.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 363]        layers.17.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 363]        layers.17.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 160/ 363]        layers.17.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 363]      layers.17.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 162/ 363]     layers.17.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 363]     layers.17.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 363]     layers.17.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 363]            layers.17.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 166/ 363]        layers.18.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 363]        layers.18.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 363]        layers.18.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 363]        layers.18.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 363]      layers.18.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 171/ 363]     layers.18.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 363]     layers.18.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 363]     layers.18.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 363]            layers.18.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 175/ 363]        layers.19.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 363]        layers.19.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 363]        layers.19.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 178/ 363]        layers.19.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 363]      layers.19.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 180/ 363]     layers.19.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 363]     layers.19.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 363]     layers.19.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 363]            layers.19.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 184/ 363]        layers.20.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 363]        layers.20.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 363]        layers.20.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 363]        layers.20.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 188/ 363]      layers.20.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 189/ 363]     layers.20.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 363]     layers.20.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 363]     layers.20.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 363]            layers.20.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 193/ 363]        layers.21.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 363]        layers.21.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 195/ 363]        layers.21.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 363]        layers.21.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 363]      layers.21.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 198/ 363]     layers.21.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 363]     layers.21.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 363]     layers.21.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 363]            layers.21.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 202/ 363]        layers.22.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 363]        layers.22.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 363]        layers.22.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 363]        layers.22.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 363]      layers.22.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 207/ 363]     layers.22.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 363]     layers.22.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 363]     layers.22.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 363]            layers.22.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 211/ 363]        layers.23.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 363]        layers.23.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 363]        layers.23.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 363]        layers.23.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 363]      layers.23.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 216/ 363]     layers.23.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 363]     layers.23.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 363]     layers.23.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 363]            layers.23.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 220/ 363]        layers.24.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 363]        layers.24.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 363]        layers.24.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 223/ 363]        layers.24.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 363]      layers.24.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 225/ 363]     layers.24.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 363]     layers.24.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 363]     layers.24.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 363]            layers.24.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 229/ 363]        layers.25.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.057 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 230/ 363]        layers.25.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 363]        layers.25.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 232/ 363]        layers.25.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 363]      layers.25.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 234/ 363]     layers.25.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 363]     layers.25.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 363]     layers.25.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 363]            layers.25.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 238/ 363]        layers.26.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 363]        layers.26.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 363]        layers.26.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 363]        layers.26.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 363]      layers.26.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 243/ 363]     layers.26.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 363]     layers.26.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 363]     layers.26.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 363]            layers.26.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 247/ 363]        layers.27.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 363]        layers.27.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 363]        layers.27.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 363]        layers.27.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 363]      layers.27.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 252/ 363]     layers.27.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 363]     layers.27.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 363]     layers.27.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 363]            layers.27.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 256/ 363]        layers.28.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 363]        layers.28.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 363]        layers.28.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 363]        layers.28.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 363]      layers.28.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 261/ 363]     layers.28.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 363]     layers.28.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 363]     layers.28.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 363]            layers.28.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 265/ 363]        layers.29.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 363]        layers.29.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 363]        layers.29.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 363]        layers.29.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 363]      layers.29.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 270/ 363]     layers.29.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 363]     layers.29.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 363]     layers.29.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 363]            layers.29.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 274/ 363]        layers.30.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 275/ 363]        layers.30.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 363]        layers.30.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 277/ 363]        layers.30.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 363]      layers.30.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 279/ 363]     layers.30.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 363]     layers.30.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 363]     layers.30.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 363]            layers.30.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 283/ 363]        layers.31.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 363]        layers.31.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 285/ 363]        layers.31.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 363]        layers.31.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 363]      layers.31.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 288/ 363]     layers.31.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 363]     layers.31.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 363]     layers.31.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 363]            layers.31.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 292/ 363]        layers.32.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 293/ 363]        layers.32.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 294/ 363]        layers.32.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 295/ 363]        layers.32.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 296/ 363]      layers.32.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 297/ 363]     layers.32.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 363]     layers.32.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 363]     layers.32.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 363]            layers.32.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 301/ 363]        layers.33.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 363]        layers.33.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 303/ 363]        layers.33.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 363]        layers.33.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 363]      layers.33.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 306/ 363]     layers.33.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 363]     layers.33.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 363]     layers.33.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 363]            layers.33.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 310/ 363]        layers.34.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 311/ 363]        layers.34.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 363]        layers.34.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 313/ 363]        layers.34.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 363]      layers.34.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 315/ 363]     layers.34.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 363]     layers.34.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 363]     layers.34.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 363]            layers.34.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 319/ 363]        layers.35.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 363]        layers.35.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 363]        layers.35.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 363]        layers.35.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 363]      layers.35.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 324/ 363]     layers.35.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 363]     layers.35.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 363]     layers.35.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 363]            layers.35.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 328/ 363]        layers.36.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 363]        layers.36.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 363]        layers.36.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 331/ 363]        layers.36.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 363]      layers.36.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 333/ 363]     layers.36.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 363]     layers.36.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 335/ 363]     layers.36.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 363]            layers.36.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 337/ 363]        layers.37.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 363]        layers.37.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 363]        layers.37.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 340/ 363]        layers.37.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 363]      layers.37.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 342/ 363]     layers.37.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 363]     layers.37.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 344/ 363]     layers.37.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 363]            layers.37.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 346/ 363]        layers.38.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 363]        layers.38.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 363]        layers.38.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 349/ 363]        layers.38.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 350/ 363]      layers.38.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 351/ 363]     layers.38.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 363]     layers.38.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 353/ 363]     layers.38.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 363]            layers.38.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 355/ 363]        layers.39.attention.wq.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 356/ 363]        layers.39.attention.wk.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 357/ 363]        layers.39.attention.wv.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 358/ 363]        layers.39.attention.wo.weight -     5120 x  5120, type =    f16, quantizing .. size =    50.00 MB ->    14.06 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 363]      layers.39.attention_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "[ 360/ 363]     layers.39.feed_forward.w1.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 361/ 363]     layers.39.feed_forward.w2.weight -    13824 x  5120, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 362/ 363]     layers.39.feed_forward.w3.weight -     5120 x 13824, type =    f16, quantizing .. size =   135.00 MB ->    37.97 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 363/ 363]            layers.39.ffn_norm.weight -             5120, type =    f32, size =    0.020 MB\n",
      "llama_model_quantize_internal: model size  = 24826.58 MB\n",
      "llama_model_quantize_internal: quant size  =  7023.90 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 37533.24 ms\n",
      "main:    total time = 37533.24 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/30B/ggml-model-f16.bin' to './models/30B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/30B/ggml-model-q4_0.bin\n",
      "[   1/ 543]                tok_embeddings.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   114.26 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[   2/ 543]                          norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   3/ 543]                        output.weight -     6656 x 32000, type =    f16, quantizing .. size =   406.25 MB ->   166.63 MB | hist: \n",
      "[   4/ 543]         layers.0.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.048 0.070 0.098 0.128 0.146 0.128 0.098 0.070 0.048 0.031 0.020 0.016 \n",
      "[   5/ 543]         layers.0.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.011 0.019 0.030 0.046 0.069 0.099 0.132 0.149 0.132 0.099 0.069 0.046 0.030 0.019 0.015 \n",
      "[   6/ 543]         layers.0.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[   7/ 543]         layers.0.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.035 0.012 0.020 0.031 0.049 0.072 0.100 0.126 0.138 0.126 0.101 0.072 0.049 0.032 0.020 0.016 \n",
      "[   8/ 543]       layers.0.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[   9/ 543]      layers.0.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.113 0.118 0.113 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  10/ 543]      layers.0.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  11/ 543]      layers.0.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  12/ 543]             layers.0.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  13/ 543]         layers.1.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  14/ 543]         layers.1.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  15/ 543]         layers.1.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[  16/ 543]         layers.1.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.098 0.117 0.125 0.117 0.098 0.075 0.054 0.036 0.023 0.019 \n",
      "[  17/ 543]       layers.1.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  18/ 543]      layers.1.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 543]      layers.1.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 543]      layers.1.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 543]             layers.1.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  22/ 543]         layers.2.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  23/ 543]         layers.2.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  24/ 543]         layers.2.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 543]         layers.2.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 543]       layers.2.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  27/ 543]      layers.2.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 543]      layers.2.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 543]      layers.2.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 543]             layers.2.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  31/ 543]         layers.3.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 543]         layers.3.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  33/ 543]         layers.3.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 543]         layers.3.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  35/ 543]       layers.3.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  36/ 543]      layers.3.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 543]      layers.3.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  38/ 543]      layers.3.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 543]             layers.3.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  40/ 543]         layers.4.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 543]         layers.4.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 543]         layers.4.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 543]         layers.4.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 543]       layers.4.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  45/ 543]      layers.4.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 543]      layers.4.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 543]      layers.4.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 543]             layers.4.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  49/ 543]         layers.5.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 543]         layers.5.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 543]         layers.5.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 543]         layers.5.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 543]       layers.5.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  54/ 543]      layers.5.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 543]      layers.5.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 543]      layers.5.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 543]             layers.5.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  58/ 543]         layers.6.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 543]         layers.6.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  60/ 543]         layers.6.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 543]         layers.6.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 543]       layers.6.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  63/ 543]      layers.6.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 543]      layers.6.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 543]      layers.6.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 543]             layers.6.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  67/ 543]         layers.7.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 543]         layers.7.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 543]         layers.7.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 543]         layers.7.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 543]       layers.7.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  72/ 543]      layers.7.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 543]      layers.7.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 543]      layers.7.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 543]             layers.7.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  76/ 543]         layers.8.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  77/ 543]         layers.8.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  78/ 543]         layers.8.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  79/ 543]         layers.8.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 543]       layers.8.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  81/ 543]      layers.8.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 543]      layers.8.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 543]      layers.8.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 543]             layers.8.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  85/ 543]         layers.9.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  86/ 543]         layers.9.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 543]         layers.9.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  88/ 543]         layers.9.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 543]       layers.9.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  90/ 543]      layers.9.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 543]      layers.9.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  92/ 543]      layers.9.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 543]             layers.9.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  94/ 543]        layers.10.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 543]        layers.10.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 543]        layers.10.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 543]        layers.10.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 543]      layers.10.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[  99/ 543]     layers.10.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 543]     layers.10.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 101/ 543]     layers.10.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 543]            layers.10.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 103/ 543]        layers.11.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 104/ 543]        layers.11.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 105/ 543]        layers.11.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 106/ 543]        layers.11.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 543]      layers.11.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 108/ 543]     layers.11.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 543]     layers.11.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 543]     layers.11.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 543]            layers.11.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 112/ 543]        layers.12.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 543]        layers.12.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 543]        layers.12.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 543]        layers.12.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 543]      layers.12.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 117/ 543]     layers.12.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 543]     layers.12.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 543]     layers.12.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 543]            layers.12.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 121/ 543]        layers.13.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 543]        layers.13.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 543]        layers.13.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 543]        layers.13.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 543]      layers.13.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 126/ 543]     layers.13.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 543]     layers.13.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 543]     layers.13.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 543]            layers.13.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 130/ 543]        layers.14.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 543]        layers.14.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 543]        layers.14.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 133/ 543]        layers.14.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 543]      layers.14.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 135/ 543]     layers.14.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 543]     layers.14.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 137/ 543]     layers.14.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 543]            layers.14.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 139/ 543]        layers.15.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 543]        layers.15.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 141/ 543]        layers.15.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 142/ 543]        layers.15.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 543]      layers.15.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 144/ 543]     layers.15.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 543]     layers.15.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 146/ 543]     layers.15.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 543]            layers.15.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 148/ 543]        layers.16.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 543]        layers.16.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 150/ 543]        layers.16.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 543]        layers.16.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 543]      layers.16.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 153/ 543]     layers.16.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 543]     layers.16.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 543]     layers.16.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 543]            layers.16.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 157/ 543]        layers.17.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 158/ 543]        layers.17.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 543]        layers.17.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 543]        layers.17.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 543]      layers.17.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 162/ 543]     layers.17.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 543]     layers.17.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 543]     layers.17.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 543]            layers.17.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 166/ 543]        layers.18.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 167/ 543]        layers.18.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 543]        layers.18.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 169/ 543]        layers.18.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 543]      layers.18.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 171/ 543]     layers.18.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 543]     layers.18.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 543]     layers.18.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 543]            layers.18.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 175/ 543]        layers.19.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 176/ 543]        layers.19.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 543]        layers.19.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 543]        layers.19.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 543]      layers.19.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 180/ 543]     layers.19.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 543]     layers.19.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 182/ 543]     layers.19.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 543]            layers.19.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 184/ 543]        layers.20.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 185/ 543]        layers.20.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 543]        layers.20.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 187/ 543]        layers.20.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 543]      layers.20.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 189/ 543]     layers.20.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 543]     layers.20.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 191/ 543]     layers.20.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 543]            layers.20.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 193/ 543]        layers.21.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 194/ 543]        layers.21.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 195/ 543]        layers.21.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 543]        layers.21.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 543]      layers.21.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 198/ 543]     layers.21.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 543]     layers.21.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 543]     layers.21.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 543]            layers.21.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 202/ 543]        layers.22.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 203/ 543]        layers.22.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 543]        layers.22.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 205/ 543]        layers.22.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 206/ 543]      layers.22.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 207/ 543]     layers.22.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 543]     layers.22.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 543]     layers.22.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 543]            layers.22.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 211/ 543]        layers.23.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 212/ 543]        layers.23.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 213/ 543]        layers.23.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 214/ 543]        layers.23.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 543]      layers.23.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 216/ 543]     layers.23.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 543]     layers.23.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 543]     layers.23.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 543]            layers.23.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 220/ 543]        layers.24.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 221/ 543]        layers.24.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 222/ 543]        layers.24.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 543]        layers.24.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 224/ 543]      layers.24.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 225/ 543]     layers.24.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 543]     layers.24.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 543]     layers.24.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 543]            layers.24.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 229/ 543]        layers.25.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 230/ 543]        layers.25.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 231/ 543]        layers.25.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 543]        layers.25.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 543]      layers.25.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 234/ 543]     layers.25.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 543]     layers.25.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 543]     layers.25.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 543]            layers.25.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 238/ 543]        layers.26.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 239/ 543]        layers.26.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 240/ 543]        layers.26.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 543]        layers.26.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 543]      layers.26.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 243/ 543]     layers.26.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 543]     layers.26.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 543]     layers.26.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 543]            layers.26.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 247/ 543]        layers.27.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 248/ 543]        layers.27.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 249/ 543]        layers.27.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 543]        layers.27.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 251/ 543]      layers.27.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 252/ 543]     layers.27.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 543]     layers.27.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 543]     layers.27.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 543]            layers.27.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 256/ 543]        layers.28.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 257/ 543]        layers.28.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 258/ 543]        layers.28.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 543]        layers.28.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 260/ 543]      layers.28.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 261/ 543]     layers.28.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 543]     layers.28.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 543]     layers.28.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 543]            layers.28.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 265/ 543]        layers.29.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 266/ 543]        layers.29.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 267/ 543]        layers.29.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 543]        layers.29.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 269/ 543]      layers.29.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 270/ 543]     layers.29.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 543]     layers.29.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 543]     layers.29.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 543]            layers.29.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 274/ 543]        layers.30.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 543]        layers.30.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 276/ 543]        layers.30.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 543]        layers.30.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 543]      layers.30.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 279/ 543]     layers.30.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 543]     layers.30.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 543]     layers.30.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 543]            layers.30.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 283/ 543]        layers.31.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 543]        layers.31.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 285/ 543]        layers.31.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 543]        layers.31.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 543]      layers.31.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 288/ 543]     layers.31.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 543]     layers.31.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 543]     layers.31.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 543]            layers.31.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 292/ 543]        layers.32.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 543]        layers.32.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 294/ 543]        layers.32.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 543]        layers.32.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 543]      layers.32.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 297/ 543]     layers.32.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 543]     layers.32.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 543]     layers.32.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 543]            layers.32.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 301/ 543]        layers.33.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 543]        layers.33.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 303/ 543]        layers.33.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 543]        layers.33.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 543]      layers.33.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 306/ 543]     layers.33.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 543]     layers.33.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 543]     layers.33.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 543]            layers.33.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 310/ 543]        layers.34.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 543]        layers.34.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 312/ 543]        layers.34.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 543]        layers.34.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 543]      layers.34.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 315/ 543]     layers.34.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 543]     layers.34.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 543]     layers.34.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 543]            layers.34.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 319/ 543]        layers.35.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 543]        layers.35.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 543]        layers.35.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 543]        layers.35.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 543]      layers.35.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 324/ 543]     layers.35.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 543]     layers.35.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 543]     layers.35.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 543]            layers.35.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 328/ 543]        layers.36.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 543]        layers.36.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 330/ 543]        layers.36.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 543]        layers.36.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 543]      layers.36.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 333/ 543]     layers.36.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 543]     layers.36.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 543]     layers.36.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 543]            layers.36.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 337/ 543]        layers.37.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 543]        layers.37.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 339/ 543]        layers.37.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 543]        layers.37.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 341/ 543]      layers.37.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 342/ 543]     layers.37.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 543]     layers.37.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 543]     layers.37.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 543]            layers.37.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 346/ 543]        layers.38.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 543]        layers.38.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 543]        layers.38.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 543]        layers.38.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 543]      layers.38.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 351/ 543]     layers.38.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 543]     layers.38.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 543]     layers.38.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 543]            layers.38.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 355/ 543]        layers.39.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 543]        layers.39.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 357/ 543]        layers.39.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 543]        layers.39.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 359/ 543]      layers.39.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 360/ 543]     layers.39.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 543]     layers.39.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 543]     layers.39.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 543]            layers.39.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 364/ 543]        layers.40.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 543]        layers.40.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 366/ 543]        layers.40.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 543]        layers.40.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 543]      layers.40.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 369/ 543]     layers.40.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 543]     layers.40.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 543]     layers.40.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 543]            layers.40.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 373/ 543]        layers.41.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 543]        layers.41.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 375/ 543]        layers.41.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 543]        layers.41.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 377/ 543]      layers.41.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 378/ 543]     layers.41.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 543]     layers.41.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 543]     layers.41.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 543]            layers.41.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 382/ 543]        layers.42.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 383/ 543]        layers.42.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 384/ 543]        layers.42.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 543]        layers.42.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 386/ 543]      layers.42.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 387/ 543]     layers.42.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 543]     layers.42.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 543]     layers.42.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 543]            layers.42.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 391/ 543]        layers.43.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 392/ 543]        layers.43.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 393/ 543]        layers.43.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 543]        layers.43.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 395/ 543]      layers.43.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 396/ 543]     layers.43.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 543]     layers.43.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 543]     layers.43.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 543]            layers.43.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 400/ 543]        layers.44.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 401/ 543]        layers.44.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 402/ 543]        layers.44.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 543]        layers.44.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 543]      layers.44.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 405/ 543]     layers.44.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 543]     layers.44.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 543]     layers.44.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 543]            layers.44.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 409/ 543]        layers.45.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 410/ 543]        layers.45.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 411/ 543]        layers.45.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 543]        layers.45.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 543]      layers.45.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 414/ 543]     layers.45.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 543]     layers.45.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 543]     layers.45.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 543]            layers.45.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 418/ 543]        layers.46.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 419/ 543]        layers.46.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 420/ 543]        layers.46.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 543]        layers.46.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 543]      layers.46.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 423/ 543]     layers.46.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 543]     layers.46.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 543]     layers.46.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 543]            layers.46.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 427/ 543]        layers.47.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 428/ 543]        layers.47.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 429/ 543]        layers.47.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 543]        layers.47.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 431/ 543]      layers.47.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 432/ 543]     layers.47.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 543]     layers.47.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 543]     layers.47.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 543]            layers.47.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 436/ 543]        layers.48.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 437/ 543]        layers.48.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 438/ 543]        layers.48.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 543]        layers.48.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 440/ 543]      layers.48.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 441/ 543]     layers.48.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 543]     layers.48.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 543]     layers.48.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 543]            layers.48.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 445/ 543]        layers.49.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 446/ 543]        layers.49.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 447/ 543]        layers.49.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 543]        layers.49.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 543]      layers.49.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 450/ 543]     layers.49.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 543]     layers.49.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 543]     layers.49.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 543]            layers.49.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 454/ 543]        layers.50.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 455/ 543]        layers.50.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 456/ 543]        layers.50.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 543]        layers.50.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 543]      layers.50.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 459/ 543]     layers.50.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 543]     layers.50.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 543]     layers.50.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 543]            layers.50.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 463/ 543]        layers.51.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 464/ 543]        layers.51.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 465/ 543]        layers.51.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 543]        layers.51.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 467/ 543]      layers.51.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 468/ 543]     layers.51.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 543]     layers.51.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 543]     layers.51.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 543]            layers.51.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 472/ 543]        layers.52.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 473/ 543]        layers.52.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 474/ 543]        layers.52.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 543]        layers.52.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 543]      layers.52.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 477/ 543]     layers.52.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 543]     layers.52.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 543]     layers.52.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 543]            layers.52.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 481/ 543]        layers.53.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 482/ 543]        layers.53.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 483/ 543]        layers.53.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 543]        layers.53.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 543]      layers.53.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 486/ 543]     layers.53.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 543]     layers.53.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 543]     layers.53.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 543]            layers.53.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 490/ 543]        layers.54.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 491/ 543]        layers.54.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 492/ 543]        layers.54.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 493/ 543]        layers.54.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 543]      layers.54.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 495/ 543]     layers.54.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 543]     layers.54.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 543]     layers.54.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 543]            layers.54.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 499/ 543]        layers.55.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 500/ 543]        layers.55.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 501/ 543]        layers.55.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 543]        layers.55.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 503/ 543]      layers.55.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 504/ 543]     layers.55.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 543]     layers.55.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 543]     layers.55.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 543]            layers.55.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 508/ 543]        layers.56.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 509/ 543]        layers.56.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 510/ 543]        layers.56.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 543]        layers.56.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 543]      layers.56.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 513/ 543]     layers.56.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 543]     layers.56.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 515/ 543]     layers.56.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 543]            layers.56.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 517/ 543]        layers.57.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
      "[ 518/ 543]        layers.57.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 519/ 543]        layers.57.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 520/ 543]        layers.57.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 521/ 543]      layers.57.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 522/ 543]     layers.57.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 543]     layers.57.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 524/ 543]     layers.57.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 543]            layers.57.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 526/ 543]        layers.58.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 527/ 543]        layers.58.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 528/ 543]        layers.58.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 529/ 543]        layers.58.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 543]      layers.58.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 531/ 543]     layers.58.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 543]     layers.58.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 533/ 543]     layers.58.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 543]            layers.58.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 535/ 543]        layers.59.attention.wq.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 536/ 543]        layers.59.attention.wk.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 537/ 543]        layers.59.attention.wv.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 538/ 543]        layers.59.attention.wo.weight -     6656 x  6656, type =    f16, quantizing .. size =    84.50 MB ->    23.77 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 543]      layers.59.attention_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "[ 540/ 543]     layers.59.feed_forward.w1.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 541/ 543]     layers.59.feed_forward.w2.weight -    17920 x  6656, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 542/ 543]     layers.59.feed_forward.w3.weight -     6656 x 17920, type =    f16, quantizing .. size =   227.50 MB ->    63.98 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 543/ 543]            layers.59.ffn_norm.weight -             6656, type =    f32, size =    0.025 MB\n",
      "llama_model_quantize_internal: model size  = 62045.57 MB\n",
      "llama_model_quantize_internal: quant size  = 17504.89 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 99814.83 ms\n",
      "main:    total time = 99814.83 ms\n",
      "main: build = 846 (b764743)\n",
      "main: quantizing './models/65B/ggml-model-f16.bin' to './models/65B/ggml-model-q4_0.bin' as Q4_0\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-f16.bin\n",
      "llama.cpp: saving model to ./models/65B/ggml-model-q4_0.bin\n",
      "[   1/ 723]                tok_embeddings.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   140.62 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[   2/ 723]                          norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   3/ 723]                        output.weight -     8192 x 32000, type =    f16, quantizing .. size =   500.00 MB ->   205.08 MB | hist: \n",
      "[   4/ 723]         layers.0.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.011 0.018 0.028 0.044 0.067 0.098 0.135 0.158 0.135 0.098 0.067 0.044 0.028 0.018 0.015 \n",
      "[   5/ 723]         layers.0.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.035 0.010 0.016 0.025 0.041 0.064 0.098 0.142 0.171 0.142 0.098 0.064 0.041 0.025 0.016 0.013 \n",
      "[   6/ 723]         layers.0.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[   7/ 723]         layers.0.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.013 0.021 0.033 0.051 0.074 0.100 0.123 0.133 0.123 0.100 0.074 0.051 0.033 0.021 0.017 \n",
      "[   8/ 723]       layers.0.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[   9/ 723]      layers.0.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
      "[  10/ 723]      layers.0.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.020 \n",
      "[  11/ 723]      layers.0.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[  12/ 723]             layers.0.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  13/ 723]         layers.1.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  14/ 723]         layers.1.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.115 0.098 0.076 0.055 0.037 0.024 0.020 \n",
      "[  15/ 723]         layers.1.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[  16/ 723]         layers.1.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.024 0.037 0.054 0.076 0.098 0.116 0.123 0.116 0.098 0.076 0.054 0.037 0.024 0.019 \n",
      "[  17/ 723]       layers.1.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  18/ 723]      layers.1.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  19/ 723]      layers.1.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  20/ 723]      layers.1.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  21/ 723]             layers.1.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  22/ 723]         layers.2.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[  23/ 723]         layers.2.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[  24/ 723]         layers.2.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  25/ 723]         layers.2.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  26/ 723]       layers.2.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  27/ 723]      layers.2.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  28/ 723]      layers.2.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  29/ 723]      layers.2.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  30/ 723]             layers.2.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  31/ 723]         layers.3.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  32/ 723]         layers.3.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  33/ 723]         layers.3.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  34/ 723]         layers.3.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  35/ 723]       layers.3.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  36/ 723]      layers.3.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  37/ 723]      layers.3.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  38/ 723]      layers.3.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  39/ 723]             layers.3.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  40/ 723]         layers.4.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  41/ 723]         layers.4.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  42/ 723]         layers.4.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  43/ 723]         layers.4.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  44/ 723]       layers.4.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  45/ 723]      layers.4.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  46/ 723]      layers.4.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  47/ 723]      layers.4.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  48/ 723]             layers.4.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  49/ 723]         layers.5.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  50/ 723]         layers.5.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  51/ 723]         layers.5.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  52/ 723]         layers.5.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  53/ 723]       layers.5.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  54/ 723]      layers.5.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  55/ 723]      layers.5.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  56/ 723]      layers.5.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  57/ 723]             layers.5.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  58/ 723]         layers.6.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  59/ 723]         layers.6.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  60/ 723]         layers.6.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  61/ 723]         layers.6.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  62/ 723]       layers.6.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  63/ 723]      layers.6.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  64/ 723]      layers.6.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  65/ 723]      layers.6.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  66/ 723]             layers.6.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  67/ 723]         layers.7.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  68/ 723]         layers.7.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  69/ 723]         layers.7.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  70/ 723]         layers.7.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  71/ 723]       layers.7.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  72/ 723]      layers.7.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  73/ 723]      layers.7.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  74/ 723]      layers.7.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  75/ 723]             layers.7.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  76/ 723]         layers.8.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  77/ 723]         layers.8.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
      "[  78/ 723]         layers.8.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  79/ 723]         layers.8.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  80/ 723]       layers.8.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  81/ 723]      layers.8.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  82/ 723]      layers.8.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[  83/ 723]      layers.8.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  84/ 723]             layers.8.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  85/ 723]         layers.9.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  86/ 723]         layers.9.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  87/ 723]         layers.9.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  88/ 723]         layers.9.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  89/ 723]       layers.9.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  90/ 723]      layers.9.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  91/ 723]      layers.9.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  92/ 723]      layers.9.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  93/ 723]             layers.9.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  94/ 723]        layers.10.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  95/ 723]        layers.10.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[  96/ 723]        layers.10.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[  97/ 723]        layers.10.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[  98/ 723]      layers.10.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[  99/ 723]     layers.10.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 100/ 723]     layers.10.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 101/ 723]     layers.10.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 102/ 723]            layers.10.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 103/ 723]        layers.11.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 104/ 723]        layers.11.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 105/ 723]        layers.11.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 106/ 723]        layers.11.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 107/ 723]      layers.11.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 108/ 723]     layers.11.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 109/ 723]     layers.11.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 110/ 723]     layers.11.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 111/ 723]            layers.11.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 112/ 723]        layers.12.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 113/ 723]        layers.12.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 114/ 723]        layers.12.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 115/ 723]        layers.12.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 116/ 723]      layers.12.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 117/ 723]     layers.12.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 118/ 723]     layers.12.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 119/ 723]     layers.12.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 120/ 723]            layers.12.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 121/ 723]        layers.13.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 122/ 723]        layers.13.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 123/ 723]        layers.13.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 124/ 723]        layers.13.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 125/ 723]      layers.13.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 126/ 723]     layers.13.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 127/ 723]     layers.13.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 128/ 723]     layers.13.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 129/ 723]            layers.13.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 130/ 723]        layers.14.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 131/ 723]        layers.14.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 132/ 723]        layers.14.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 133/ 723]        layers.14.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 134/ 723]      layers.14.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 135/ 723]     layers.14.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 136/ 723]     layers.14.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 137/ 723]     layers.14.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 138/ 723]            layers.14.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 139/ 723]        layers.15.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 140/ 723]        layers.15.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 141/ 723]        layers.15.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 142/ 723]        layers.15.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 143/ 723]      layers.15.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 144/ 723]     layers.15.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 145/ 723]     layers.15.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 146/ 723]     layers.15.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 147/ 723]            layers.15.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 148/ 723]        layers.16.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 149/ 723]        layers.16.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 150/ 723]        layers.16.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 151/ 723]        layers.16.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 152/ 723]      layers.16.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 153/ 723]     layers.16.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 154/ 723]     layers.16.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 155/ 723]     layers.16.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 156/ 723]            layers.16.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 157/ 723]        layers.17.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 158/ 723]        layers.17.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 159/ 723]        layers.17.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 160/ 723]        layers.17.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 161/ 723]      layers.17.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 162/ 723]     layers.17.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 163/ 723]     layers.17.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 164/ 723]     layers.17.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 165/ 723]            layers.17.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 166/ 723]        layers.18.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 167/ 723]        layers.18.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 168/ 723]        layers.18.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 169/ 723]        layers.18.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 170/ 723]      layers.18.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 171/ 723]     layers.18.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 172/ 723]     layers.18.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 173/ 723]     layers.18.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 174/ 723]            layers.18.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 175/ 723]        layers.19.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 176/ 723]        layers.19.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 177/ 723]        layers.19.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 178/ 723]        layers.19.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 179/ 723]      layers.19.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 180/ 723]     layers.19.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 181/ 723]     layers.19.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 182/ 723]     layers.19.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 183/ 723]            layers.19.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 184/ 723]        layers.20.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 185/ 723]        layers.20.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 186/ 723]        layers.20.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 187/ 723]        layers.20.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 188/ 723]      layers.20.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 189/ 723]     layers.20.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 190/ 723]     layers.20.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 191/ 723]     layers.20.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 192/ 723]            layers.20.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 193/ 723]        layers.21.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 194/ 723]        layers.21.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 195/ 723]        layers.21.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 196/ 723]        layers.21.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 197/ 723]      layers.21.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 198/ 723]     layers.21.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 199/ 723]     layers.21.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 200/ 723]     layers.21.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 201/ 723]            layers.21.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 202/ 723]        layers.22.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 203/ 723]        layers.22.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 204/ 723]        layers.22.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 205/ 723]        layers.22.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 206/ 723]      layers.22.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 207/ 723]     layers.22.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 208/ 723]     layers.22.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 209/ 723]     layers.22.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 210/ 723]            layers.22.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 211/ 723]        layers.23.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 212/ 723]        layers.23.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 213/ 723]        layers.23.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 214/ 723]        layers.23.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 215/ 723]      layers.23.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 216/ 723]     layers.23.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 217/ 723]     layers.23.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 218/ 723]     layers.23.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 219/ 723]            layers.23.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 220/ 723]        layers.24.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 221/ 723]        layers.24.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 222/ 723]        layers.24.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 223/ 723]        layers.24.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 224/ 723]      layers.24.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 225/ 723]     layers.24.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 226/ 723]     layers.24.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 227/ 723]     layers.24.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 228/ 723]            layers.24.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 229/ 723]        layers.25.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 230/ 723]        layers.25.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 231/ 723]        layers.25.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 232/ 723]        layers.25.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 233/ 723]      layers.25.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 234/ 723]     layers.25.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 235/ 723]     layers.25.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 236/ 723]     layers.25.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 237/ 723]            layers.25.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 238/ 723]        layers.26.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 239/ 723]        layers.26.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 240/ 723]        layers.26.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 241/ 723]        layers.26.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 242/ 723]      layers.26.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 243/ 723]     layers.26.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 244/ 723]     layers.26.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 245/ 723]     layers.26.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 246/ 723]            layers.26.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 247/ 723]        layers.27.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 248/ 723]        layers.27.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 249/ 723]        layers.27.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 250/ 723]        layers.27.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 251/ 723]      layers.27.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 252/ 723]     layers.27.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 253/ 723]     layers.27.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 254/ 723]     layers.27.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 255/ 723]            layers.27.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 256/ 723]        layers.28.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 257/ 723]        layers.28.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 258/ 723]        layers.28.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 259/ 723]        layers.28.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 260/ 723]      layers.28.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 261/ 723]     layers.28.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 262/ 723]     layers.28.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 263/ 723]     layers.28.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 264/ 723]            layers.28.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 265/ 723]        layers.29.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 266/ 723]        layers.29.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 267/ 723]        layers.29.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 268/ 723]        layers.29.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 269/ 723]      layers.29.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 270/ 723]     layers.29.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 271/ 723]     layers.29.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 272/ 723]     layers.29.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 273/ 723]            layers.29.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 274/ 723]        layers.30.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 275/ 723]        layers.30.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 276/ 723]        layers.30.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 277/ 723]        layers.30.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 278/ 723]      layers.30.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 279/ 723]     layers.30.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 280/ 723]     layers.30.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 281/ 723]     layers.30.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 282/ 723]            layers.30.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 283/ 723]        layers.31.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 284/ 723]        layers.31.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 285/ 723]        layers.31.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 286/ 723]        layers.31.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 287/ 723]      layers.31.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 288/ 723]     layers.31.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 289/ 723]     layers.31.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 290/ 723]     layers.31.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 291/ 723]            layers.31.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 292/ 723]        layers.32.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 293/ 723]        layers.32.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 294/ 723]        layers.32.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 295/ 723]        layers.32.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 296/ 723]      layers.32.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 297/ 723]     layers.32.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 298/ 723]     layers.32.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 299/ 723]     layers.32.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 300/ 723]            layers.32.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 301/ 723]        layers.33.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 302/ 723]        layers.33.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 303/ 723]        layers.33.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 304/ 723]        layers.33.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 305/ 723]      layers.33.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 306/ 723]     layers.33.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 307/ 723]     layers.33.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 308/ 723]     layers.33.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 309/ 723]            layers.33.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 310/ 723]        layers.34.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 311/ 723]        layers.34.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 312/ 723]        layers.34.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 313/ 723]        layers.34.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 314/ 723]      layers.34.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 315/ 723]     layers.34.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 316/ 723]     layers.34.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 317/ 723]     layers.34.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 318/ 723]            layers.34.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 319/ 723]        layers.35.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 320/ 723]        layers.35.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 321/ 723]        layers.35.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 322/ 723]        layers.35.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 323/ 723]      layers.35.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 324/ 723]     layers.35.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 325/ 723]     layers.35.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 326/ 723]     layers.35.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 327/ 723]            layers.35.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 328/ 723]        layers.36.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 329/ 723]        layers.36.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 330/ 723]        layers.36.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 331/ 723]        layers.36.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 332/ 723]      layers.36.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 333/ 723]     layers.36.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 334/ 723]     layers.36.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 335/ 723]     layers.36.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 336/ 723]            layers.36.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 337/ 723]        layers.37.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 338/ 723]        layers.37.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 339/ 723]        layers.37.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 340/ 723]        layers.37.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 341/ 723]      layers.37.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 342/ 723]     layers.37.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 343/ 723]     layers.37.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 344/ 723]     layers.37.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 345/ 723]            layers.37.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 346/ 723]        layers.38.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 347/ 723]        layers.38.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 348/ 723]        layers.38.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 349/ 723]        layers.38.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 350/ 723]      layers.38.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 351/ 723]     layers.38.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 352/ 723]     layers.38.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 353/ 723]     layers.38.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 354/ 723]            layers.38.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 355/ 723]        layers.39.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 356/ 723]        layers.39.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
      "[ 357/ 723]        layers.39.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 358/ 723]        layers.39.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 359/ 723]      layers.39.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 360/ 723]     layers.39.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 361/ 723]     layers.39.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 362/ 723]     layers.39.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 363/ 723]            layers.39.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 364/ 723]        layers.40.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 365/ 723]        layers.40.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 366/ 723]        layers.40.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 367/ 723]        layers.40.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 368/ 723]      layers.40.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 369/ 723]     layers.40.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 370/ 723]     layers.40.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 371/ 723]     layers.40.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 372/ 723]            layers.40.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 373/ 723]        layers.41.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 374/ 723]        layers.41.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 375/ 723]        layers.41.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 376/ 723]        layers.41.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 377/ 723]      layers.41.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 378/ 723]     layers.41.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 379/ 723]     layers.41.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 380/ 723]     layers.41.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 381/ 723]            layers.41.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 382/ 723]        layers.42.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 383/ 723]        layers.42.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 384/ 723]        layers.42.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 385/ 723]        layers.42.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 386/ 723]      layers.42.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 387/ 723]     layers.42.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 388/ 723]     layers.42.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 389/ 723]     layers.42.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 390/ 723]            layers.42.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 391/ 723]        layers.43.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 392/ 723]        layers.43.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 393/ 723]        layers.43.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 394/ 723]        layers.43.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 395/ 723]      layers.43.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 396/ 723]     layers.43.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 397/ 723]     layers.43.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 398/ 723]     layers.43.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 399/ 723]            layers.43.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 400/ 723]        layers.44.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 401/ 723]        layers.44.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 402/ 723]        layers.44.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 403/ 723]        layers.44.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 404/ 723]      layers.44.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 405/ 723]     layers.44.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 406/ 723]     layers.44.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 407/ 723]     layers.44.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 408/ 723]            layers.44.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 409/ 723]        layers.45.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 410/ 723]        layers.45.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 411/ 723]        layers.45.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 412/ 723]        layers.45.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 413/ 723]      layers.45.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 414/ 723]     layers.45.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 415/ 723]     layers.45.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 416/ 723]     layers.45.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 417/ 723]            layers.45.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 418/ 723]        layers.46.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 419/ 723]        layers.46.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 420/ 723]        layers.46.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 421/ 723]        layers.46.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 422/ 723]      layers.46.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 423/ 723]     layers.46.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 424/ 723]     layers.46.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 425/ 723]     layers.46.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 426/ 723]            layers.46.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 427/ 723]        layers.47.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.025 0.020 \n",
      "[ 428/ 723]        layers.47.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.115 0.096 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 429/ 723]        layers.47.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 430/ 723]        layers.47.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 431/ 723]      layers.47.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 432/ 723]     layers.47.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 433/ 723]     layers.47.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 434/ 723]     layers.47.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 435/ 723]            layers.47.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 436/ 723]        layers.48.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 437/ 723]        layers.48.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 438/ 723]        layers.48.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 439/ 723]        layers.48.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 440/ 723]      layers.48.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 441/ 723]     layers.48.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 442/ 723]     layers.48.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 443/ 723]     layers.48.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 444/ 723]            layers.48.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 445/ 723]        layers.49.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 446/ 723]        layers.49.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 447/ 723]        layers.49.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 448/ 723]        layers.49.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 449/ 723]      layers.49.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 450/ 723]     layers.49.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 451/ 723]     layers.49.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 452/ 723]     layers.49.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 453/ 723]            layers.49.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 454/ 723]        layers.50.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 455/ 723]        layers.50.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 456/ 723]        layers.50.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 457/ 723]        layers.50.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 458/ 723]      layers.50.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 459/ 723]     layers.50.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 460/ 723]     layers.50.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 461/ 723]     layers.50.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 462/ 723]            layers.50.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 463/ 723]        layers.51.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 464/ 723]        layers.51.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.053 0.074 0.097 0.117 0.130 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 465/ 723]        layers.51.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 466/ 723]        layers.51.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 467/ 723]      layers.51.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 468/ 723]     layers.51.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 469/ 723]     layers.51.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 470/ 723]     layers.51.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 471/ 723]            layers.51.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 472/ 723]        layers.52.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.123 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 473/ 723]        layers.52.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.127 0.116 0.097 0.075 0.054 0.037 0.023 0.019 \n",
      "[ 474/ 723]        layers.52.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 475/ 723]        layers.52.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 476/ 723]      layers.52.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 477/ 723]     layers.52.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 478/ 723]     layers.52.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 479/ 723]     layers.52.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 480/ 723]            layers.52.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 481/ 723]        layers.53.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 482/ 723]        layers.53.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.096 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 483/ 723]        layers.53.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 484/ 723]        layers.53.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 485/ 723]      layers.53.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 486/ 723]     layers.53.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 487/ 723]     layers.53.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 488/ 723]     layers.53.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 489/ 723]            layers.53.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 490/ 723]        layers.54.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.056 0.076 0.097 0.114 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 491/ 723]        layers.54.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 492/ 723]        layers.54.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 493/ 723]        layers.54.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 494/ 723]      layers.54.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 495/ 723]     layers.54.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 496/ 723]     layers.54.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 497/ 723]     layers.54.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 498/ 723]            layers.54.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 499/ 723]        layers.55.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 500/ 723]        layers.55.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 501/ 723]        layers.55.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 502/ 723]        layers.55.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 503/ 723]      layers.55.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 504/ 723]     layers.55.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 505/ 723]     layers.55.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 506/ 723]     layers.55.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 507/ 723]            layers.55.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 508/ 723]        layers.56.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.123 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 509/ 723]        layers.56.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.075 0.054 0.036 0.023 0.019 \n",
      "[ 510/ 723]        layers.56.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 511/ 723]        layers.56.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 512/ 723]      layers.56.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 513/ 723]     layers.56.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 514/ 723]     layers.56.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 515/ 723]     layers.56.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 516/ 723]            layers.56.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 517/ 723]        layers.57.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.038 0.024 0.020 \n",
      "[ 518/ 723]        layers.57.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.075 0.097 0.117 0.127 0.117 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 519/ 723]        layers.57.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 520/ 723]        layers.57.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 521/ 723]      layers.57.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 522/ 723]     layers.57.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 523/ 723]     layers.57.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 524/ 723]     layers.57.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 525/ 723]            layers.57.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 526/ 723]        layers.58.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 527/ 723]        layers.58.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.036 0.054 0.074 0.097 0.117 0.129 0.117 0.096 0.074 0.053 0.036 0.023 0.019 \n",
      "[ 528/ 723]        layers.58.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 529/ 723]        layers.58.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 530/ 723]      layers.58.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 531/ 723]     layers.58.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 532/ 723]     layers.58.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 533/ 723]     layers.58.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 534/ 723]            layers.58.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 535/ 723]        layers.59.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 536/ 723]        layers.59.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.126 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 537/ 723]        layers.59.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 538/ 723]        layers.59.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.110 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 539/ 723]      layers.59.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 540/ 723]     layers.59.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 541/ 723]     layers.59.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 542/ 723]     layers.59.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 543/ 723]            layers.59.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 544/ 723]        layers.60.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 545/ 723]        layers.60.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.116 0.125 0.116 0.097 0.075 0.054 0.037 0.024 0.019 \n",
      "[ 546/ 723]        layers.60.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 547/ 723]        layers.60.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 548/ 723]      layers.60.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 549/ 723]     layers.60.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 550/ 723]     layers.60.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 551/ 723]     layers.60.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 552/ 723]            layers.60.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 553/ 723]        layers.61.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 554/ 723]        layers.61.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 555/ 723]        layers.61.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 556/ 723]        layers.61.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 557/ 723]      layers.61.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 558/ 723]     layers.61.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 559/ 723]     layers.61.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 560/ 723]     layers.61.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 561/ 723]            layers.61.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 562/ 723]        layers.62.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 563/ 723]        layers.62.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.124 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 564/ 723]        layers.62.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 565/ 723]        layers.62.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 566/ 723]      layers.62.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 567/ 723]     layers.62.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 568/ 723]     layers.62.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 569/ 723]     layers.62.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 570/ 723]            layers.62.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 571/ 723]        layers.63.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 572/ 723]        layers.63.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 573/ 723]        layers.63.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 574/ 723]        layers.63.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 575/ 723]      layers.63.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 576/ 723]     layers.63.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 577/ 723]     layers.63.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 578/ 723]     layers.63.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 579/ 723]            layers.63.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 580/ 723]        layers.64.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.121 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 581/ 723]        layers.64.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.115 0.122 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 582/ 723]        layers.64.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 583/ 723]        layers.64.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 584/ 723]      layers.64.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 585/ 723]     layers.64.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 586/ 723]     layers.64.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 587/ 723]     layers.64.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 588/ 723]            layers.64.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 589/ 723]        layers.65.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 590/ 723]        layers.65.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.014 0.023 0.037 0.054 0.075 0.097 0.117 0.127 0.116 0.097 0.074 0.054 0.036 0.023 0.019 \n",
      "[ 591/ 723]        layers.65.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 592/ 723]        layers.65.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 593/ 723]      layers.65.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 594/ 723]     layers.65.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 595/ 723]     layers.65.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 596/ 723]     layers.65.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 597/ 723]            layers.65.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 598/ 723]        layers.66.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.055 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 599/ 723]        layers.66.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.122 0.115 0.097 0.076 0.055 0.037 0.024 0.020 \n",
      "[ 600/ 723]        layers.66.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 601/ 723]        layers.66.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 602/ 723]      layers.66.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 603/ 723]     layers.66.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 604/ 723]     layers.66.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 605/ 723]     layers.66.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 606/ 723]            layers.66.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 607/ 723]        layers.67.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 608/ 723]        layers.67.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 609/ 723]        layers.67.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 610/ 723]        layers.67.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 611/ 723]      layers.67.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 612/ 723]     layers.67.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 613/ 723]     layers.67.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 614/ 723]     layers.67.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 615/ 723]            layers.67.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 616/ 723]        layers.68.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 617/ 723]        layers.68.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 618/ 723]        layers.68.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 619/ 723]        layers.68.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 620/ 723]      layers.68.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 621/ 723]     layers.68.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 622/ 723]     layers.68.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 623/ 723]     layers.68.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 624/ 723]            layers.68.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 625/ 723]        layers.69.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 626/ 723]        layers.69.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 627/ 723]        layers.69.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 628/ 723]        layers.69.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 629/ 723]      layers.69.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 630/ 723]     layers.69.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 631/ 723]     layers.69.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 632/ 723]     layers.69.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 633/ 723]            layers.69.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 634/ 723]        layers.70.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 635/ 723]        layers.70.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.122 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 636/ 723]        layers.70.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 637/ 723]        layers.70.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 638/ 723]      layers.70.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 639/ 723]     layers.70.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 640/ 723]     layers.70.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 641/ 723]     layers.70.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 642/ 723]            layers.70.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 643/ 723]        layers.71.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 644/ 723]        layers.71.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.113 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
      "[ 645/ 723]        layers.71.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 646/ 723]        layers.71.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 647/ 723]      layers.71.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 648/ 723]     layers.71.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 649/ 723]     layers.71.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 650/ 723]     layers.71.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 651/ 723]            layers.71.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 652/ 723]        layers.72.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 653/ 723]        layers.72.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 654/ 723]        layers.72.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 655/ 723]        layers.72.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 656/ 723]      layers.72.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 657/ 723]     layers.72.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 658/ 723]     layers.72.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 659/ 723]     layers.72.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 660/ 723]            layers.72.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 661/ 723]        layers.73.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 662/ 723]        layers.73.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 663/ 723]        layers.73.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 664/ 723]        layers.73.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 665/ 723]      layers.73.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 666/ 723]     layers.73.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 667/ 723]     layers.73.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 668/ 723]     layers.73.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 669/ 723]            layers.73.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 670/ 723]        layers.74.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 671/ 723]        layers.74.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 672/ 723]        layers.74.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 673/ 723]        layers.74.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 674/ 723]      layers.74.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 675/ 723]     layers.74.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 676/ 723]     layers.74.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 677/ 723]     layers.74.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 678/ 723]            layers.74.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 679/ 723]        layers.75.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 680/ 723]        layers.75.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 681/ 723]        layers.75.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 682/ 723]        layers.75.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 683/ 723]      layers.75.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 684/ 723]     layers.75.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 685/ 723]     layers.75.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 686/ 723]     layers.75.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 687/ 723]            layers.75.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 688/ 723]        layers.76.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
      "[ 689/ 723]        layers.76.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 690/ 723]        layers.76.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 691/ 723]        layers.76.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 692/ 723]      layers.76.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 693/ 723]     layers.76.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 694/ 723]     layers.76.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 695/ 723]     layers.76.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 696/ 723]            layers.76.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 697/ 723]        layers.77.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 698/ 723]        layers.77.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 699/ 723]        layers.77.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 700/ 723]        layers.77.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 701/ 723]      layers.77.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 702/ 723]     layers.77.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 703/ 723]     layers.77.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 704/ 723]     layers.77.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 705/ 723]            layers.77.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 706/ 723]        layers.78.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.096 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 707/ 723]        layers.78.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.120 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 708/ 723]        layers.78.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 709/ 723]        layers.78.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.110 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
      "[ 710/ 723]      layers.78.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 711/ 723]     layers.78.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 712/ 723]     layers.78.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 713/ 723]     layers.78.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 714/ 723]            layers.78.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 715/ 723]        layers.79.attention.wq.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.125 0.115 0.097 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 716/ 723]        layers.79.attention.wk.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.125 0.115 0.096 0.075 0.054 0.037 0.024 0.020 \n",
      "[ 717/ 723]        layers.79.attention.wv.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.119 0.112 0.097 0.076 0.056 0.038 0.025 0.020 \n",
      "[ 718/ 723]        layers.79.attention.wo.weight -     8192 x  8192, type =    f16, quantizing .. size =   128.00 MB ->    36.00 MB | hist: 0.037 0.017 0.027 0.041 0.058 0.077 0.095 0.108 0.113 0.108 0.095 0.077 0.058 0.041 0.027 0.022 \n",
      "[ 719/ 723]      layers.79.attention_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "[ 720/ 723]     layers.79.feed_forward.w1.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
      "[ 721/ 723]     layers.79.feed_forward.w2.weight -    22016 x  8192, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.097 0.115 0.123 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
      "[ 722/ 723]     layers.79.feed_forward.w3.weight -     8192 x 22016, type =    f16, quantizing .. size =   344.00 MB ->    96.75 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
      "[ 723/ 723]            layers.79.ffn_norm.weight -             8192, type =    f32, size =    0.031 MB\n",
      "llama_model_quantize_internal: model size  = 124525.03 MB\n",
      "llama_model_quantize_internal: quant size  = 35090.73 MB\n",
      "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
      "\n",
      "main: quantize time = 204146.64 ms\n",
      "main:    total time = 204146.64 ms\n"
     ]
    }
   ],
   "source": [
    "# quantize the model to 4-bits (using q4_0 method)\n",
    "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/13B/ggml-model-f16.bin ./models/13B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/30B/ggml-model-f16.bin ./models/30B/ggml-model-q4_0.bin q4_0\n",
    "!./quantize ./models/65B/ggml-model-f16.bin ./models/65B/ggml-model-q4_0.bin q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f2aec9-559c-43b0-b579-1f425532f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki1\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dadb227-432e-44e5-8f21-d6338a2f4bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566833\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your purpose, live it out and in doing so, make a positive difference to the world.\n",
      "I was born and raised in London, England (not the UK). I am married with 3 children, all girls aged 25, 21 & 16. My first degree is in Mathematics and Statistics from King’s College, University of London, followed by an MBA at Henley Management College.\n",
      "I have a wide range of interests including sport (rugby, running, swimming, cycling), food, travel, technology, photography, the environment and sustainability, music & art. I am also interested in personal development, philosophy, ethics & spirituality.\n",
      "You can read more about my background here.\n",
      "My purpose is to help people live a happy, healthy, meaningful life by enabling them to learn how to manage their minds effectively so that they can develop the mental strengths and skills needed to achieve whatever it is they want in their lives. To do this I use positive psychology as well as other proven scientifically validated tools from mind management, cognitive behavioural therapy (CBT), resilience training & emotional intelligence.\n",
      "I am a Chartered Member of the British Psychological Society and member of the UK’s leading coaching body – The Association for Coaching. I also have a number of other professional qualifications including a certificate in psychotherapeutic counselling skills from The Institute Of Counselling & Psychotherapy, an NLP Practitioner course & Master Practitioner, Hypnosis Diploma and various certificates in coaching skills from the Academy of Executive Coaching.\n",
      "I trained as a coach with the prestigious International Coach Federation (ICF) where I also became a Professional Certified Coach (PCC). This is one of only two international coaching accreditations, the other being Master Certified Coach (MCC). It means that I have met all their requirements to be considered at expert level.\n",
      "I am also an Associate Professor in Positive Psychology at The International University for Graduates.\n",
      "I started my career as a Management Consultant with PricewaterhouseCoopers, where I became a Partner specialising in Strategic Planning & Change Management. My clients included many of the UK’s leading organisations including BP, British Airways, AstraZeneca and HSB\n",
      "llama_print_timings:        load time =  2399.01 ms\n",
      "llama_print_timings:      sample time =   242.33 ms /   512 runs   (    0.47 ms per token,  2112.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   335.12 ms /   265 tokens (    1.26 ms per token,   790.77 tokens per second)\n",
      "llama_print_timings:        eval time =  4908.00 ms /   510 runs   (    9.62 ms per token,   103.91 tokens per second)\n",
      "llama_print_timings:       total time =  5569.40 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7a0912-2746-4e7d-adaf-92b60cb2ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566843\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it, and while we are here, we should try to make a difference in the lives of others.\n",
      "I have been involved with several charities over the years including The Royal Marines Charity, Royal British Legion Poppy Appeal, Great Ormond Street Hospital Children’s Charity and now I am delighted to say that I will be supporting Alzheimer’s Research UK.\n",
      "Alzheimer’s is currently the most common form of dementia, with over 850,000 people in the UK living with it today. A cure or treatment for dementia does not seem like an unrealistic dream, however, and we have seen a huge amount of progress in dementia research in recent years. Alzheimer’s Research UK are committed to supporting this vital work by funding world-class scientific studies into the prevention, cause, care, treatment and cure of dementia.\n",
      "I am delighted to be working with them, and I know that together we can make a difference!\n",
      "As part of my partnership with Alzheimer’s Research UK, I will also be taking part in Race for Life 2017 on Sunday July 3rd, along with my team. The race will take place at Victoria Park, Peterborough and will raise money for the charity.\n",
      "I would love it if you could join us on the day; please email me to register your interest. I can’t wait to get started!\n",
      "http://www.raceforlife.org/race-for-life-2017/peterborough/?utm_source=Alzheimers+Research+UK&utm_campaign=16d8e539f8-EMAIL_CAMPAIGN_2017_04_07&utm_medium=email&utm_term=0_c2b70eeba3-16d8e539f8-41461191\n",
      "If you’re unable to join us, please consider donating to the charity to help fund this vital research. You can do so here: https://www.alzheimersresearchuk.org/donate/fundraiser/charlotte-williams/?utm_source=Alzheimers+Research+UK&utm_\n",
      "llama_print_timings:        load time =   951.41 ms\n",
      "llama_print_timings:      sample time =   242.24 ms /   512 runs   (    0.47 ms per token,  2113.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   330.73 ms /   265 tokens (    1.25 ms per token,   801.27 tokens per second)\n",
      "llama_print_timings:        eval time =  4896.59 ms /   510 runs   (    9.60 ms per token,   104.15 tokens per second)\n",
      "llama_print_timings:       total time =  5553.36 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3324a21-7a43-4c68-83eb-52417e487c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566851\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to go out there and make your mark on this world, to leave something behind that will last longer than you do.\n",
      "I'm a big believer in dreaming big because if you don't, you'll never reach those goals.\n",
      "My goal in life has always been to work hard and be the best I can at everything I put my mind to. Whether it was basketball, cheerleading, school or anything else that came up, I went for it.\n",
      "I also believe in being a good person who is kind, compassionate and loving towards others.\n",
      "Growing up I was always made aware of my responsibilities as a role model to younger kids. From the time I started playing youth basketball at age 7, to when I took my first trip overseas for charity work at age 15, it has been important to me that I set an example for people who look up to me.\n",
      "I'm always willing to take a step back and listen to others so that I can learn as much as possible about the world around me and how my actions will affect those around me. If I can make even one person smile or be inspired by something I do, then I'll consider myself to have done what I set out to achieve in life.\n",
      "\"I have no idea where I want to end up, but I know I am going somewhere\"\n",
      "In 2014, my family and I moved from Perth to Melbourne to be closer to our extended family there. It was around this time that I also started thinking about how my future could be impacted by a career in basketball.\n",
      "My dad had always joked that if I didn't go into the NAB League (young player development league for Australia's national league, Basketball Australia) then it would just mean he could spend more time with me than if I did play! It was only when I started to think about what I wanted out of life and my future career options that I started thinking about playing overseas.\n",
      "I believe the meaning of life is to go out there and make your mark on this world, to leave something behind that will last longer than you do. I'm a big believer in dreaming big because if you don't, you'll never reach those goals. My goal in life has always been to work hard and be the best I can at everything I put my mind to.\n",
      "I hope\n",
      "llama_print_timings:        load time =   942.12 ms\n",
      "llama_print_timings:      sample time =   240.80 ms /   512 runs   (    0.47 ms per token,  2126.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   331.73 ms /   265 tokens (    1.25 ms per token,   798.85 tokens per second)\n",
      "llama_print_timings:        eval time =  4901.35 ms /   510 runs   (    9.61 ms per token,   104.05 tokens per second)\n",
      "llama_print_timings:       total time =  5558.34 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98a39e0-c339-4234-b6d2-49d642ffb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566859\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and fulfilled in whatever you do.\n",
      "Being a student means being able to choose your future, but it also means that you have to fight for what you want. It’s hard work, but I’ve always believed the harder you work, the luckier you get.\n",
      "In life, we don’t get what we deserve or what we want, we just get what we need.\n",
      "Life is an echo. What you send out comes back. What you sow you reap. The truth you live by comes home to you.\n",
      "Life has two rules: Number one, never quit. Number two, always remember Rule number one.\n",
      "I love life because what more is there?\n",
      "It is the greatest of all advantages to enjoy no advantage at all.\n",
      "The only thing that separates a winner from a loser is time.\n",
      "Life is the art of drawing without an eraser.\n",
      "Live fast, die young and leave a good-looking corpse.\n",
      "We are not what we do or what we have done—we are what we have overcome.\n",
      "I am going to live; I want to live! And no one on earth will stop me.\n",
      "Life has its own hidden forces which you can only discover by living.\n",
      "I hate life, and I’m sticking with it.\n",
      "The best thing about the future is that it comes one day at a time.\n",
      "If you don’t know where you are going, any road will get you there.\n",
      "Life is a series of problems which we must not take too seriously.\n",
      "Every man has his own life to live; to be true to it is the only thing that matters. The world is full of people living a lie.\n",
      "I like living. I have some things still to do.\n",
      "The world goes on for those who only stand and wait.\n",
      "Life is worth living if we can sing and laugh.\n",
      "Life is not always beautiful; sometimes life is more about learning to deal with ugly things.\n",
      "I’ve learned that you can’t have everything and do everything at the same time.\n",
      "We cannot become what we need by remaining what we are.\n",
      "It’s a hard thing to be happy for no reason; it’s harder still to learn how to do it.\n",
      "Life is something that happens when you can’t get any sleep.\n",
      "When one door closes another opens, but often we look so long at the closed door that we see only\n",
      "llama_print_timings:        load time =  4122.26 ms\n",
      "llama_print_timings:      sample time =   241.43 ms /   512 runs   (    0.47 ms per token,  2120.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   557.10 ms /   265 tokens (    2.10 ms per token,   475.68 tokens per second)\n",
      "llama_print_timings:        eval time =  8139.16 ms /   510 runs   (   15.96 ms per token,    62.66 tokens per second)\n",
      "llama_print_timings:       total time =  9022.22 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0296e978-7c63-4d87-a1a9-dc41b49dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566874\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living.\n",
      "> \n",
      "> —FRANÇOIS TRUFFAUT, director\n",
      "We've all heard that one of the best ways to learn a foreign language is to make love while speaking it. While this has often proven true for me, I have also learned my lessons about life and filmmaking in bed. This book is the story of some of those lessons as well as how I applied them to making movies. It's not the typical tell-all Hollywood memoir that focuses on celebrity gossip or scandals. What you will find here are some of my favorite moments from my life behind and in front of the camera—some funny, some sexy, some inspirational.\n",
      "Making movies is an exhilarating experience, but it's not just about the filmmaking process. It's about the people you meet along the way, like the beautiful woman I met when I was nineteen. She took me to bed and taught me that life can be a wonderful adventure if you take chances. My mom also taught me this, but in a different way—by showing me how she lived her life with passion, grace, and dignity.\n",
      "I've had the good fortune of working with some of the world's most famous (and talented) actors on films like _Girlfriends_ , _The Big Lebowski_ , _Talk to Her_ , _Vicky Cristina Barcelona_ , and _To Rome with Love_. They helped me grow as a director, while I taught them some things about filmmaking.\n",
      "I was also fortunate to meet some of my heroes, like Federico Fellini and Marlon Brando. I wanted their secrets for a successful life and they shared what they knew—sometimes in bed.\n",
      "I wasn't always this lucky with women or in the movies. I often felt like Charlie Brown as a young boy growing up in Los Angeles. I remember thinking how unfair it was that every girl in my class had a crush on _The Fonz_ from TV's _Happy Days_ , while no one even knew who I was.\n",
      "I wasn't always this lucky with women or in the movies.\n",
      "To get me through the pain of adolescence, I turned to the movies for inspiration and guidance. The characters I fell in love with\n",
      "llama_print_timings:        load time =  1485.19 ms\n",
      "llama_print_timings:      sample time =   241.85 ms /   512 runs   (    0.47 ms per token,  2117.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   558.02 ms /   265 tokens (    2.11 ms per token,   474.89 tokens per second)\n",
      "llama_print_timings:        eval time =  8162.11 ms /   510 runs   (   16.00 ms per token,    62.48 tokens per second)\n",
      "llama_print_timings:       total time =  9046.06 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebca2498-774d-4f51-ac56-a9da8f359523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566887\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find something that you love, and then make it your mission to do that thing.\n",
      "My job as a photographer is not just about capturing moments for clients, but also about helping them see their wedding day through fresh eyes. My clients trust me with all of their memories on the biggest day of their lives. They don’t want just anyone; they want someone who will do everything in their power to give them the best photos possible and create a unique experience for their wedding day.\n",
      "I strive to go above and beyond, giving my clients more than expected. I like to think of myself as an extension of my client’s big day. I want it to be something that they look back on with pride and joy because the images are so beautiful. I love to get to know my couples, and become part of their lives during this time.\n",
      "I believe in taking chances, being genuine, living your passion and never settling for less than you deserve. I believe in getting dirty, dancing wildly, and crying tears of joy. I believe in sharing a moment, telling a story, and creating memories that will last forever.\n",
      "The thing about life is that it’s always changing; we are always growing and evolving. We have to adapt and be flexible with our lives because they are never perfect. But that doesn’t mean we can’t do the best we can while we are here on earth, soaking up every moment of this one wild ride.\n",
      "I love being a photographer for these reasons and more. I truly believe life is about finding something you love to do, and then doing it with everything in your power. I also understand that not everyone loves photography like I do, but we all have passions. The key is to find yours, and make it your mission.\n",
      "I am a wedding photographer based out of Indianapolis, Indiana. My passion for photography started in high school when I took my first photojournalism class with Mrs. Gunterman (who was awesome!). After that, I fell in love with the art of photography and knew it was something I wanted to pursue as a career.\n",
      "I earned an Associate’s Degree from Vincennes University in Commercial Photography, where my mentor was Jerry Greenspoon. After graduation, I worked for several years at Sears Portrait Studios, where I learned the ins\n",
      "llama_print_timings:        load time =  1462.15 ms\n",
      "llama_print_timings:      sample time =   260.04 ms /   512 runs   (    0.51 ms per token,  1968.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   558.70 ms /   265 tokens (    2.11 ms per token,   474.32 tokens per second)\n",
      "llama_print_timings:        eval time =  8159.68 ms /   510 runs   (   16.00 ms per token,    62.50 tokens per second)\n",
      "llama_print_timings:       total time =  9062.44 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d246e2-1ed4-46f7-aa3e-0d4787cc3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566899\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn, grow and develop into an extraordinary human being.\n",
      "In order to do so, we must constantly challenge ourselves both physically and mentally. This means putting yourself in uncomfortable situations. This may be a new workout at the gym, or starting up that business that you have always dreamed about. Either way, it will be tough. But if it was easy then everyone would be doing it.\n",
      "This is why I started this blog. I want to help people by sharing my knowledge and experiences with you. So I can hopefully inspire you to step out of your comfort zone, challenge yourself and become an extraordinary human being.\n",
      "The best way to learn is by making mistakes and failing. I have made plenty of both over the years. But I have also learned a lot of lessons along the way. Hopefully this website will help you avoid some of those mistakes that I have made in the past.\n",
      "You can read more about me on my About Me page.\n",
      "If you need any additional help, please feel free to contact me here or by email at dan@extrordinairyknowledge.com. You can also connect with me over on Facebook and Twitter.\n",
      "Thanks for visiting Extraordinary Knowledge and good luck in all of your future endeavors!\n",
      "I have a question that I think you might be able to help me with. I was wondering if you could give me some advice for my workouts? I am training for an event coming up, but lately I’ve been struggling to find the motivation and inspiration to go to the gym. Do you have any tips or techniques that can help me stay focused on my goals?\n",
      "I really like your blog! I look forward to reading more of it! I will be following you on twitter also!\n",
      "Thanks for stopping by. Glad you enjoy the website!\n",
      "I am not sure what your event is, but if it’s a strength and power related one then I would focus on doing compound movements with heavy weight (low reps). If its an endurance type of event, then do more higher rep lifts using moderate to light weight.\n",
      "Thanks for the kind words. Just keep plugging along and you will find your way back!\n",
      "I’m a big fan of your YouTube channel, but I was wondering if you could elaborate on what your background is? I’d like to know about your education. It seems you have extensive knowledge in\n",
      "llama_print_timings:        load time = 11822.70 ms\n",
      "llama_print_timings:      sample time =   241.72 ms /   512 runs   (    0.47 ms per token,  2118.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1280.37 ms /   265 tokens (    4.83 ms per token,   206.97 tokens per second)\n",
      "llama_print_timings:        eval time = 18762.46 ms /   510 runs   (   36.79 ms per token,    27.18 tokens per second)\n",
      "llama_print_timings:       total time = 20368.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce27ab44-b446-4126-8972-7472c658a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566934\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and free, which is why I've been making this blog.\n",
      "I have been working at a company as an illustrator for 10 years but recently quit. I used to be a full-time manga artist before but I got tired of drawing the same thing over and over again so I decided to work at the company instead, which was fun to begin with since I could draw whatever I wanted. However, even though it's been 10 years, there wasn't much progress in my career and I found myself stuck doing the same thing over and over again as well as not being able to do what I wanted when I wanted to... It wasn't fun anymore so I quit.\n",
      "I want to be free and happy again, which is why I decided to go back being a full-time manga artist. Of course it's scary because I have to make my living on my own. However, life is too short to waste on doing something that makes you unhappy; therefore, I decided to risk everything and take the plunge.\n",
      "I won't be able to do what I want to do if I don't get any money so I've been making and selling things through this blog. If it was not for this blog, I wouldn't have made it this far. Thank you very much for supporting me.\n",
      "I am a big fan of cute Japanese characters and fashion so I would love to incorporate them into my works to make people happy and smile. In other words, I want to create something that makes people happy. This is why I've been making this blog so far.\n",
      "I'm an illustrator but also a manga artist. I have been drawing since I was young. When I was 18 years old, I debuted in Jump Comics and had worked as a professional manga artist for 7 years until I quit to work at a company. My favorite foods are pizza and spaghetti so I'm planning on opening an Italian restaurant in the future (I've actually already started working towards that goal).\n",
      "I enjoy drawing cute things, especially when it comes to animals. I used to be a full-time manga artist and then worked as a freelance illustrator for 10 years. I'm currently running this blog with my husband. My favorite foods are potatoes and yaki udon (fried noodles).\n",
      "\n",
      "llama_print_timings:        load time =  2732.06 ms\n",
      "llama_print_timings:      sample time =   242.22 ms /   512 runs   (    0.47 ms per token,  2113.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1283.14 ms /   265 tokens (    4.84 ms per token,   206.52 tokens per second)\n",
      "llama_print_timings:        eval time = 18799.93 ms /   510 runs   (   36.86 ms per token,    27.13 tokens per second)\n",
      "llama_print_timings:       total time = 20409.75 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eec90d1-7b7b-4071-af17-849bcb505108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566959\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give life meaning. - Havelock Ellis\n",
      "The Meaning of Life is a four-letter word. - Douglas Adams\n",
      "Don't ask what the world needs. Ask what makes you come alive and go do it. Because what the world needs is more people who have come alive. - Howard Thurman\n",
      "Life has no meaning other than the meaning we give it. - Joseph Campbell\n",
      "The purpose of life is a life of purpose. - Robert Byrne\n",
      "The most important thing is to enjoy your life—to be happy—it's all that matters. - Audrey Hepburn\n",
      "The meaning of life... I think the meaning of life is, I don't know, I think it's love. - Bill Murray\n",
      "I believe in living today. Not yesterday but tomorrow. That's why I take every day as a new day, you never know when you'll be gone. - R. Kelly\n",
      "Life's meaning has always eluded me and I guess it always will. But I love it just the same. - E. B. White\n",
      "Life is what happens to us while we are making other plans. - Allen Saunders\n",
      "The meaning of life is to find your gift. The purpose of life is to give it away. - Pablo Picasso\n",
      "What is the meaning of life? To be happy and useful. - Johann Wolfgang Von Goethe\n",
      "To live is the rarest thing in the world. Most people exist, that is all. - Oscar Wilde\n",
      "I think therefore I am. I think not therefore I am not. - George Carlin\n",
      "The purpose of life is to matter; to count, to stand for something, to have it make some difference that you lived at all. - Leo Rosten\n",
      "The meaning of life is whatever you want it to be. Life is what you make it. - Timothy Leary\n",
      "I’m not going to say this is the meaning of life or anything like that, but I think it might be a good way to start. - Steve Burns\n",
      "The true meaning of life is to plant trees under whose shade you do not expect to sit. - Nelson Henderson\n",
      "Life is like playing a violin solo in public and learning the instrument as one goes on. - Samuel Butler\n",
      "We are all here for a spell; get all the good laughs you can. - Will Rogers\n",
      "I think life is about balance. I'm not about\n",
      "llama_print_timings:        load time =  2743.66 ms\n",
      "llama_print_timings:      sample time =   251.02 ms /   512 runs   (    0.49 ms per token,  2039.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1287.39 ms /   265 tokens (    4.86 ms per token,   205.84 tokens per second)\n",
      "llama_print_timings:        eval time = 18825.80 ms /   510 runs   (   36.91 ms per token,    27.09 tokens per second)\n",
      "llama_print_timings:       total time = 20449.17 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04719be2-a272-4fc9-aed8-193e4bfe9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689566985\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love, and be loved. We are all one big family living on this great planet together. Let's make it a better place for future generations to come.\n",
      "I am not my body. I am not my mind. I am just pure consciousness. This is what I have learned from the teachings of Neo-Advaita, and I believe that is true. In my own life experience, this is what I have found to be true.\n",
      "The meaning of life? The meaning of life is to love, and be loved. This is my truth, and it has helped me to live a happier, healthier and more fulfilling life. It's as simple as that. What else could the meaning of life possibly be? There is no greater purpose than this.\n",
      "We are all one big family living on this great planet together. Let's make it a better place for future generations to come.\n",
      "The meaning of life is to love, and be loved. We are all one big family living on this great planet together. Let's make it a better place for future generations to come.\n",
      "You have to let go of the ego, because it will never bring you any lasting happiness. It has nothing to do with your true self. You have to detach from all things in life. Become like an empty vessel and be ready to receive all that is good.\n",
      "The meaning of life is to love, and be loved. We are all one big family living on this great planet together. Let's make it a better place for future generations to come. The meaning of my own life is to love, and be loved. And I also want to help create a world that works for everyone.\n",
      "The ego is the root cause of almost every problem we face in life. This has been my biggest challenge in life so far, but it's also been one of my greatest teachers. The ego will never bring you any lasting happiness, and as long as you identify with your mind and your body – which are just parts of who you truly are – you can never be free to live a life of peace, joy and abundance.\n",
      "The meaning of life is to love, and be loved. We are all one big family living on this great planet together. Let's make it a better place for future generations to come. What else could the meaning of life possibly be?\n",
      "I am just getting\n",
      "llama_print_timings:        load time = 22776.25 ms\n",
      "llama_print_timings:      sample time =   245.33 ms /   512 runs   (    0.48 ms per token,  2086.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2531.67 ms /   265 tokens (    9.55 ms per token,   104.67 tokens per second)\n",
      "llama_print_timings:        eval time = 35890.40 ms /   510 runs   (   70.37 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time = 38752.72 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1730091a-3146-476d-ab57-49f8722c351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567050\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to grow, and we grow by learning from our experiences. So why do so many people choose to live a lie?\n",
      "To me, it's like they are choosing not to learn, and as a result, their lives become stagnant.\n",
      "I don't think there is one true meaning of life but rather that individuals create their own purpose for living through the decisions that they make on a daily basis. That may be to live a simple, happy life or it could mean devoting oneself to others through service or career. I believe that you have to do what feels right to you and if you are honest with yourself, then your path will be illuminated by this honesty.\n",
      "I don't think there is one true meaning of life but rather that individuals create their own purpose for living through the decisions that they make on a daily basis. That may be to live a simple, happy life or it could mean devoting oneself to others through service or career. I believe that you have to do what feels right to you and if you are honest with yourself, then your path will be illuminated by this honesty. Would you like to video or text chat with me?\n",
      "What is the meaning of life according to you?\n",
      "In your opinion, what is the purpose/meaning of life?\n",
      "How can i find a deeper meaning in my life?\n",
      "I'm 32 and I keep thinking about suicide. I don't know why. I have everything I want but still I feel sad and more I'm 32 and I keep thinking about suicide. I don't know why. I have everything I want but still I feel sad and lonely. What should I do?\n",
      "Is it normal for a teen to not know what they want from life or their future?\n",
      "I am 18 yrs old..i dont know what i wanna do in my life...i dont know about my career ..how can i decide wat more I am 18 yrs old..i dont know what i wanna do in my life...i dont know about my career ..how can i decide what to do?\n",
      "What is the meaning of life, if there is any?\n",
      "I have no purpose and don't enjoy anything. Is this normal for a teenager?\n",
      "Am 21 years old i don`t feel like am living in this world i feel like am in a dream how can\n",
      "llama_print_timings:        load time =  4967.26 ms\n",
      "llama_print_timings:      sample time =   240.47 ms /   512 runs   (    0.47 ms per token,  2129.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2539.59 ms /   265 tokens (    9.58 ms per token,   104.35 tokens per second)\n",
      "llama_print_timings:        eval time = 35941.89 ms /   510 runs   (   70.47 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:       total time = 38806.29 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351608bd-64ae-4f26-9fb5-ccf3ee5fb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567097\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "llama_new_context_with_model: kv self size  = 1280.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it.\n",
      "I’m trying very hard not to be a hypocrite. That is why I write my blogs and read other people’s, but if you think about it (and I have), all this reading and writing is actually getting in the way of living. I am a firm believer that you need to live it, breathe it, experience it, before you can truly understand something or someone.\n",
      "The same goes for my relationships with people. I find that when I spend too much time in front of my computer screen, I start to lose touch with my friends and family. The phone conversations become shorter and the emails more impersonal. I don’t like it, and it takes some effort to bring myself back into a place where I am once again connected with those I care about.\n",
      "In this day and age of instant communication, we are all so busy trying to keep up with each other, that I sometimes think we have forgotten what it is we were chasing in the first place.\n",
      "I am not sure if my blog has a purpose or meaning, but then again I’m not sure if life really does either. Maybe I should go find out?\n",
      "Living life to its fullest. It can be different for each person. For me it is spending time with people I love and care about. Also enjoying the nature around us.\n",
      "I think you have a purpose and meaning in your blog. It gives us a glimpse into your world. It’s great that you are so open about your thoughts, feelings and experiences.\n",
      "Little things make me happy too. I can spend hours watching ants run from one place to another. Not sure what they do all day long but it keeps them busy, which is good enough for me.\n",
      "I think we all have a purpose in life, even the ant. Just that sometimes it’s hard to see what it might be.\n",
      "You have touched upon my sentiments exactly, and I have always felt this way about life. So many people spend their lives chasing something they don’t know or understand (money, success, power), without realising that you only get one chance at living life – so the point is to live it!\n",
      "I am not sure if my blog has a purpose or meaning, but then again I’m not sure if life really does either. Maybe I should go find out? << A great thought for today. Thanks.\n",
      "llama_print_timings:        load time =  5038.04 ms\n",
      "llama_print_timings:      sample time =   252.85 ms /   512 runs   (    0.49 ms per token,  2024.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2544.20 ms /   265 tokens (    9.60 ms per token,   104.16 tokens per second)\n",
      "llama_print_timings:        eval time = 35999.29 ms /   510 runs   (   70.59 ms per token,    14.17 tokens per second)\n",
      "llama_print_timings:       total time = 38882.09 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34085-ceab-48a8-96cb-f47d3f8cc5b3",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "303e2727-09d0-47be-818f-2a6519aa5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567144\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find something you love and devote yourself entirely to it. We must all strive for excellence in our field, regardless of where we work or what job title we have. We must learn from others and be open to their ideas and suggestions. We should never stop learning new techniques and ways to improve ourselves and our art. I know the meaning of life is not always simple but it can lead us down a path that will make this world a better place, one color at a time!\n",
      "Thank you for being here on my website. Please take your time to look around. If you have any questions regarding services or products please feel free to contact me. I am also happy to provide referrals if needed.\n",
      "I will be adding my own handmade items in the store section as well as other artists’ creations for sale. So, check back often and sign up for our newsletter.\n",
      "This is an ongoing project that I started this year. I have been collecting vintage sewing notions that are no longer being produced. These items are from years ago but they still work just the same as they did then. A couple of my favorites are the 1950s “Mom’s Sewing Kit” and the “Sewing Box”.\n",
      "I have always loved vintage sewing notions and enjoy collecting them. I started collecting in the late 70s when I was about 14 years old. In my early 20s, I got a job working for a small embroidery shop (back in the day, they were called “monogram shops”). My main duty was to re-package vintage notions that had been returned by embroiderers who thought they could use them. Little did I know then how much of an impact this experience would have on my life!\n",
      "The owner of the shop was a widow and she lived alone with her two sons. She had an antique mall business in town and it was the 1980s, so everything was about to be “retro”. The vintage sewing notions that were returned by customers were always kept together in the back of the shop. Since I loved them and knew they needed a home, I talked my boss into letting me start a separate collection of these beautiful items.\n",
      "I worked for her for about 3 years and during that time, we sold quite a few of the\n",
      "llama_print_timings:        load time =  6480.57 ms\n",
      "llama_print_timings:      sample time =   252.87 ms /   512 runs   (    0.49 ms per token,  2024.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   359.40 ms /   265 tokens (    1.36 ms per token,   737.34 tokens per second)\n",
      "llama_print_timings:        eval time = 11846.15 ms /   510 runs   (   23.23 ms per token,    43.05 tokens per second)\n",
      "llama_print_timings:       total time = 12542.94 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061c4fdf-de31-47b9-a6f7-55873c2b690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567165\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to learn, grow and achieve your goals.\n",
      "I am a senior at Cedar Grove high school. This year i will be attending college at Bunker Hill Community College . I love playing volleyball , traveling and being around my friends and family. I’m also interested in art and music. I think that if we all work together, we can make the world a better place for future generations to come.\n",
      "This is my second year as an intern at CJP and i am currently working on building up my leadership skills and becoming a more active member of my community . A quote that best describes me is “if you are not part of the solution , then you are part of the problem”. I feel this relates to me because I strive to be a leader in every area of life.\n",
      "I think that if we all work together, we can make the world a better place for future generations to come.\n",
      "To learn more about us or our intern program click here!\n",
      "This is my second year as an intern at CJP and i am currently working on building up my leadership skills and becoming a more active member of my community . A quote that best describes me is “if you are not part of the solution , then you are part of the problem”. I feel this relates to me because I strive to be a leader in every area of life. Click here to learn more about us or our intern program!\n",
      "I am a senior at Cedar Grove high school. This year i will be attending college at Bunker Hill Community College . I love playing volleyball , traveling and being around my friends and family. I’m also interested in art and music. I think that if we all work together, we can make the world a better place for future generations to come. Click here to learn more about us or our intern program!\n",
      "This is my second year as an intern at CJP and i am currently working on building up my leadership skills and becoming a more active member of my community . A quote that best describes me is “if you are not part of the solution , then you are part of the problem”. I feel this relates to me because I strive to be a leader in every area of life. Click here to learn more about us or our intern program!\n",
      "I am a senior at Cedar Grove high school. This year i will be attending college . I love playing volleyball\n",
      "llama_print_timings:        load time =  1916.11 ms\n",
      "llama_print_timings:      sample time =   242.18 ms /   512 runs   (    0.47 ms per token,  2114.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   359.63 ms /   265 tokens (    1.36 ms per token,   736.86 tokens per second)\n",
      "llama_print_timings:        eval time = 11847.62 ms /   510 runs   (   23.23 ms per token,    43.05 tokens per second)\n",
      "llama_print_timings:       total time = 12533.16 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb37cbe3-16ab-4f63-9903-2e7951ce751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567182\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to help others become happier, healthier and more successful. This is what fuels my passion for helping people live a better, healthier lifestyle.\n",
      "I am a certified Personal Trainer (CFT), Certified Strength & Conditioning Specialist (CSCS) through the National Strength & Conditioning Association (NSCA). I have been working in the fitness industry since 1987 and have helped thousands of people reach their goals.\n",
      "I was one of the first personal trainers to be certified by NSCA and received my CFT in 1989, before many other organizations offered this certification. In addition to being a nationally recognized fitness expert, I am also an award-winning author. My newest book, The Smarter Way to Lose Weight, is available on Amazon.\n",
      "My personal goal for my clients is to help them develop an exercise and nutrition program that will give them more energy, confidence, a better body image and the ability to live life to its fullest! I have helped thousands of people reach their goals. My clients lose weight, build muscle, become stronger and experience greater health benefits from increased metabolism, enhanced cardiovascular functioning and improved moods through exercise and nutrition.\n",
      "I am a member of the National Academy of Sports Medicine (NASM), American College of Sports Medicine (ACSM), International Fitness Professionals Association (IFPA) and NSCA. I have presented at conventions for these organizations as well as the Cooper Clinic, and I am an active contributing writer to a number of fitness publications including IDEA Fitness Journal, Club Industry, Lifescript Health Newsletter, Women’s World Magazine, Men’s Health & Fitness Magazine, Prevention Magazine, Shape Magazine, MuscleMag International, Flex Magazine and many other magazines.\n",
      "I am a 4th Degree Black Belt in American Kenpo Karate and a 2nd Degree Black Belt in Taekwondo. I have been training martial artists for over 30 years. My students have won numerous national championships and represented the United States at World Championships.\n",
      "I am married with four children, three boys ages 18-26 and one daughter age 17. I enjoy spending time with my family h\n",
      "llama_print_timings:        load time =  1882.91 ms\n",
      "llama_print_timings:      sample time =   241.94 ms /   512 runs   (    0.47 ms per token,  2116.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   359.77 ms /   265 tokens (    1.36 ms per token,   736.59 tokens per second)\n",
      "llama_print_timings:        eval time = 11853.99 ms /   510 runs   (   23.24 ms per token,    43.02 tokens per second)\n",
      "llama_print_timings:       total time = 12540.33 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33cb1a63-f863-4a7f-b627-6c76e534f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567198\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to give meaning to our lives.\n",
      "I think that if you are interested in something and want to know more about it, then it's a good thing. If you aren't interested in anything, though, you will be bored with what you do and your performance will suffer. This is true even if you have the best equipment available to you.\n",
      "I think that the world is run by consensus. It is a very complicated process and a lot of people don't like it because it is so difficult to get everyone to agree.\n",
      "I think that we are in a position of great responsibility, and I do not want to leave my child with no future to grow up.\n",
      "I have always been fascinated by the idea of creating life from non-life.\n",
      "I have never tried to block out negative thoughts about myself or others. I don't think it is healthy. Thoughts are like stars, they fade away if you ignore them.\n",
      "I enjoy being able to do research and to share that information with my colleagues around the world.\n",
      "I enjoy life when things are moving; I cannot sit still.\n",
      "I have been very lucky in that I had an incredible wife who has taken care of me, loved me and been there for me throughout all these years.\n",
      "I was not a good student. I did not spend much time on my schoolwork.\n",
      "I am not going to make excuses, but we have a new government, and they don't know how the science world works.\n",
      "From a scientist's point of view it is exciting because it promises so much for the future. It may be that this is not the first time it has been done or will be done again, but I think it is very important to have a clear set of ethical rules and standards before going ahead with something like this.\n",
      "Finding new species and learning about the relationships between plants, animals and other living organisms has always seemed a thrilling adventure to me. To see one's name in print for the first time is an exquisite pleasure that remains long after the novelty has faded.\n",
      "Few people are prepared to be at the forefront of new scientific discoveries or even to understand what they mean. But if scientists are going to make the effort, we should at least try to speak their language so that we can all be enriched by the experience.\n",
      "For me it is enough to have done\n",
      "llama_print_timings:        load time = 14112.81 ms\n",
      "llama_print_timings:      sample time =   242.07 ms /   512 runs   (    0.47 ms per token,  2115.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   612.27 ms /   265 tokens (    2.31 ms per token,   432.81 tokens per second)\n",
      "llama_print_timings:        eval time = 21104.88 ms /   510 runs   (   41.38 ms per token,    24.17 tokens per second)\n",
      "llama_print_timings:       total time = 22044.48 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250c511c-864d-4b20-8d87-86f2409b277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567238\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in a very simple phrase: “God is love”. (1 John 4.8) And it can change your life.\n",
      "What does this mean? That God is not an impersonal energy, or some kind of creative force that exists outside of us all and somehow created the universe. It means that the God who has created everything that we experience in our lives, is personal, and intimate – and is known as Father, Son and Spirit.\n",
      "It also means that you are loved by this God personally, no matter what you’ve done or where you’ve been, even if your life feels like a mess right now. There is no love like the love of God, who loves us all in spite of ourselves – not because we deserve it, but because He does.\n",
      "God is also a community of three persons: Father, Son and Spirit. This means that our lives are not meant to be lived alone – but within a community of people who are on the same journey, and who want to share in life together with you. Life was never meant to be lived alone! No matter what your story is or where you’ve been, you can find love, purpose and belonging here at Holy Trinity.\n",
      "Holy Trinity has many different groups for people of all ages. There are weekday home Bible studies, a men’s breakfast group, parent’s groups on Wednesdays, youth Bible study and social events, and an active women’s ministry that offers regular opportunities to meet and connect with other women in the church. We offer numerous annual retreats and conferences for adults and children, and a number of service projects to get involved in. There are also many ways to serve in our community through outreach, as well as within the church.\n",
      "We’d love to have you visit us this Sunday! Our weekly Sunday services include an early morning (8:00) prayer meeting for those who are looking for a quieter time of worship and prayer; a 9:30 service with contemporary music and a 10:45 traditional service. We also offer Sunday school classes, a youth group, and a number of activities for children. There is plenty to do at Holy Trinity!\n",
      "If you have any questions about who we are or what we’re doing, please don’t hesitate to ask! I hope that you will join us on our journey in the months ahead as\n",
      "llama_print_timings:        load time =  3439.47 ms\n",
      "llama_print_timings:      sample time =   256.27 ms /   512 runs   (    0.50 ms per token,  1997.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   611.14 ms /   265 tokens (    2.31 ms per token,   433.62 tokens per second)\n",
      "llama_print_timings:        eval time = 21112.39 ms /   510 runs   (   41.40 ms per token,    24.16 tokens per second)\n",
      "llama_print_timings:       total time = 22064.48 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f792b2-0f7e-4b93-9d3e-69d4479f6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567266\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live for God and that's why I do what I do.\n",
      "I believe the meaning of life is to spread love, and make a positive impact on this world. And if you don't have that meaning in your own life, then that's really sad.\"\n",
      "The purpose of human life is to serve, and to show compassion and the will to help others.\" \"You must take personal responsibility. You cannot change the circumstances, the seasons, or the wind, but you can change yourself. That is something you have charge of.\" -Jim Rohn\n",
      "\"The meaning of life is just to be alive. It is so plain and so obvious and so simple. And yet, everybody rushes around in a great panic as if it were necessary to achieve something beyond themselves.\"\n",
      "\"What lies behind us and what lies before us are tiny matters compared to what lies within us.\" - Ralph Waldo Emerson\n",
      "\"The meaning of life is that it is to be lived, and it is not to be traded and conceptualized and squeezed into a pattern of systems, or forced into a corner of semantics, or pigeonholed by artificial divisions of classifications. Life is an ecstatic state – a continually flowing panorama of change and evolution; and the meaning of life is simply being a part of it.\"\n",
      "\"The best years of your life are the ones in which you decide your problems are your own. You do not blame them on your mother, the ecology, or the president. You realize that you control your own destiny.\" -Albert Ellis\n",
      "\"You cannot discover oceans unless you have the courage to leave safe harbors.\" -Unknown\n",
      "\"The meaning of life is just to be alive. It is so plain and so obvious and so simple. And yet, everybody rushes around as if it were necessary to achieve something beyond themselves.” ~Albert Einstein\n",
      "“What lies behind us and what lies before us are tiny matters compared to what lies within us.\" -Ralph Waldo Emerson\n",
      "\"I am not interested in picking up crumbs of compassion thrown from the table of someone who considers himself my master. I want the full menu of rights.” ~Desmond Tutu\n",
      "“The meaning of life is just to be alive. It is so plain and so obvious and so simple. And yet, everybody rushes around as\n",
      "llama_print_timings:        load time =  3424.02 ms\n",
      "llama_print_timings:      sample time =   241.51 ms /   512 runs   (    0.47 ms per token,  2119.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   609.10 ms /   265 tokens (    2.30 ms per token,   435.07 tokens per second)\n",
      "llama_print_timings:        eval time = 21111.78 ms /   510 runs   (   41.40 ms per token,    24.16 tokens per second)\n",
      "llama_print_timings:       total time = 22046.98 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd03d5a9-978f-454f-9dde-da67377225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 846 (b764743)\n",
      "main: seed  = 1689567294\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2507.40 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 65200 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfaa12-c184-4be7-9d83-27cacb9ab31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
